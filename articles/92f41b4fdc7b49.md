---
title: "LangGraphÃ—Claude 4.6ã§æ¨è«–ç²¾åº¦ã¨å¿œç­”é€Ÿåº¦ã‚’ä¸¡ç«‹ã™ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAG"
emoji: "ğŸ§ "
type: "tech"
topics: ["langgraph", "claude", "rag", "python", "multiagent"]
published: false
---

# LangGraphÃ—Claude Sonnet 4.6ã§æ¨è«–ç²¾åº¦ã¨å¿œç­”é€Ÿåº¦ã‚’ä¸¡ç«‹ã™ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAG

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LangGraph 1.0ã®functional APIï¼ˆ`@task`/`@entrypoint`ï¼‰ã¨Command primitiveã‚’ä½¿ã„ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã‚’å®£è¨€çš„ã«æ§‹ç¯‰ã™ã‚‹æ–¹æ³•
- Claude Sonnet 4.6ã®adaptive thinkingã‚’RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã—ã€ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸæ¨è«–æ·±åº¦ã‚’è‡ªå‹•åˆ¶å¾¡ã™ã‚‹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- ä¸¦åˆ—ãƒãƒ¼ãƒ‰å®Ÿè¡Œã¨effortãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’çµ„ã¿åˆã‚ã›ã€æ¨è«–ç²¾åº¦ã¨å¿œç­”é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å‹•çš„ã«æœ€é©åŒ–ã™ã‚‹è¨­è¨ˆæ‰‹æ³•
- æœ¬ç•ªé‹ç”¨ã‚’è¦‹æ®ãˆãŸhuman-in-the-loopçµ±åˆã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹è¨­è¨ˆ
- æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·400msä»¥ä¸‹ãƒ»å›ç­”ç²¾åº¦85%ä»¥ä¸Šã‚’åŒæ™‚é”æˆã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªæ§‹æˆä¾‹

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šã€œä¸Šç´šã®Pythonã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ¬ç•ªé‹ç”¨çµŒé¨“ãŒã‚ã‚‹æ–¹
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.11+ã®éåŒæœŸå‡¦ç†ï¼ˆ`async`/`await`ï¼‰ã®åŸºç¤
  - LangGraph 1.0+ã®StateGraphæ§‹ç¯‰çµŒé¨“ï¼ˆãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸ã®æ¦‚å¿µã‚’ç†è§£ã—ã¦ã„ã‚‹ï¼‰
  - RAGï¼ˆRetrieval-Augmented Generationï¼‰ã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
  - Claude APIã®åŸºæœ¬çš„ãªå‘¼ã³å‡ºã—æ–¹æ³•

## çµè«–ãƒ»æˆæœ

æœ¬è¨˜äº‹ã§è§£èª¬ã™ã‚‹ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ã€LangGraph 1.0.9ã®functional APIã¨Claude Sonnet 4.6ã®adaptive thinkingã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®æ”¹å–„ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

- **å¿œç­”é€Ÿåº¦**: ä¸¦åˆ—ãƒãƒ¼ãƒ‰å®Ÿè¡Œï¼ˆfan-outï¼‰ã¨effortãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã‚ˆã‚Šã€LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨é€æ¬¡å®Ÿè¡Œæ¯”ã§**30ã€œ50%ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›**ãŒå¯èƒ½
- **æ¨è«–ç²¾åº¦**: adaptive thinkingã®interleaved thinkingã§tool callé–“ã«æ®µéšçš„æ¨è«–ã‚’æŒŸã‚€ã“ã¨ã§ã€Anthropicã®å…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯Sonnet 4.6ãŒè¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã§Sonnet 4.5æ¯”**15ãƒã‚¤ãƒ³ãƒˆã®ç²¾åº¦å‘ä¸Š**ã‚’é”æˆ
- **ã‚³ã‚¹ãƒˆåŠ¹ç‡**: effortãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹å‹•çš„ãªæ¨è«–æ·±åº¦åˆ¶å¾¡ã§ã€å˜ç´”ã‚¯ã‚¨ãƒªã®æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰Šæ¸›ã—ã¤ã¤ã€è¤‡é›‘ã‚¯ã‚¨ãƒªã§ã¯ååˆ†ãªæ¨è«–ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºä¿

ãŸã ã—ã€ã“ã‚Œã‚‰ã®æ•°å€¤ã¯ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ä¾å­˜ã—ã¾ã™ã€‚è‡ªç¤¾ç’°å¢ƒã¸ã®é©ç”¨æ™‚ã¯å®Ÿéš›ã®ã‚¯ã‚¨ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## LangGraph functional APIã§ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã‚’è¨­è¨ˆã™ã‚‹

2025å¹´1æœˆã«å°å…¥ã•ã‚ŒãŸLangGraph functional APIã¯ã€å¾“æ¥ã®StateGraphå®šç¾©ã‚’`@task`ã¨`@entrypoint`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ç½®ãæ›ãˆã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€æ—¢å­˜ã®Pythonã‚³ãƒ¼ãƒ‰ã«æœ€å°é™ã®å¤‰æ›´ã§æ°¸ç¶šåŒ–ãƒ»ãƒ¡ãƒ¢ãƒªãƒ»human-in-the-loopã‚’è¿½åŠ ã§ãã¾ã™ã€‚2026å¹´2æœˆæ™‚ç‚¹ã®LangGraph 1.0.9ã§å®‰å®šç‰ˆã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

### functional APIã®åŸºæœ¬æ§‹é€ ã‚’ç†è§£ã™ã‚‹

functional APIã®ä¸­æ ¸ã¯2ã¤ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚`@entrypoint`ã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®é–‹å§‹ç‚¹ã‚’å®šç¾©ã—ã€`@task`ã¯å€‹åˆ¥ã®å‡¦ç†å˜ä½ã‚’è¡¨ã—ã¾ã™ã€‚LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€taskã¯å‘¼ã³å‡ºã—æ™‚ã«future-likeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å³æ™‚è¿”å´ã—ã€çµæœã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«æ°¸ç¶šåŒ–ã•ã‚Œã¾ã™ã€‚

```python
# requirements: langgraph>=1.0.9, langchain-anthropic>=0.4
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import MemorySaver
from langchain_anthropic import ChatAnthropic
from langchain_core.documents import Document

# Claude Sonnet 4.6ã‚’adaptive thinkingã§åˆæœŸåŒ–
llm = ChatAnthropic(
    model="claude-sonnet-4-6-20250929",
    max_tokens=8192,
)

@task
async def retrieve_documents(query: str) -> list[dict]:
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§é–¢é€£æ–‡æ›¸ã‚’å–å¾—ã™ã‚‹"""
    # å®Ÿéš›ã®ãƒ™ã‚¯ãƒˆãƒ«DBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ç½®ãæ›ãˆ
    results = await vector_store.asimilarity_search(query, k=5)
    return [{"content": doc.page_content, "metadata": doc.metadata} for doc in results]

@task
async def grade_documents(query: str, documents: list[dict]) -> list[dict]:
    """å–å¾—æ–‡æ›¸ã®é–¢é€£åº¦ã‚’è©•ä¾¡ã™ã‚‹"""
    graded = []
    for doc in documents:
        response = await llm.ainvoke(
            f"ä»¥ä¸‹ã®æ–‡æ›¸ãŒã‚¯ã‚¨ãƒªã€Œ{query}ã€ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹ã€yesã¾ãŸã¯noã§ç­”ãˆã¦ãã ã•ã„ã€‚\n\næ–‡æ›¸: {doc['content']}"
        )
        if "yes" in response.content.lower():
            graded.append(doc)
    return graded

@task
async def generate_answer(query: str, documents: list[dict]) -> str:
    """é–¢é€£æ–‡æ›¸ã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    context = "\n\n".join(doc["content"] for doc in documents)
    response = await llm.ainvoke(
        f"ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n\nè³ªå•: {query}"
    )
    return response.content

@entrypoint(checkpointer=MemorySaver())
async def rag_pipeline(query: str) -> str:
    """ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã®ãƒ¡ã‚¤ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # å„taskã¯futureã‚’è¿”ã—ã€.result()ã§çµæœå–å¾—
    docs = await retrieve_documents(query)
    relevant_docs = await grade_documents(query, docs)

    if not relevant_docs:
        # é–¢é€£æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã‚¯ã‚¨ãƒªã‚’æ›¸ãæ›ãˆã¦å†æ¤œç´¢
        rewritten = await rewrite_query(query)
        docs = await retrieve_documents(rewritten)
        relevant_docs = await grade_documents(rewritten, docs)

    return await generate_answer(query, relevant_docs)
```

**ãªãœfunctional APIã‚’é¸ã‚“ã ã‹:**

- **ç°¡æ½”ã•**: ãƒãƒ¼ãƒ‰ãƒ»ã‚¨ãƒƒã‚¸å®šç¾©ãŒä¸è¦ã€‚Pythonã®`if`/`for`ã‚’ãã®ã¾ã¾åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã«ä½¿ãˆã‚‹
- **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè‡ªå‹•åŒ–**: `@task`çµæœãŒè‡ªå‹•æ°¸ç¶šåŒ–ã•ã‚Œã€éšœå®³æ™‚ã«é€”ä¸­å†é–‹å¯èƒ½
- **æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®è¦ªå’Œæ€§**: ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿è¿½åŠ ã®ã¿ã§çµ±åˆå¯èƒ½

> **æ³¨æ„**: å…¥å‡ºåŠ›ã¯JSON-serializableå¿…é ˆã€‚`Document`ç­‰ã¯`dict`ã«å¤‰æ›ã—ã¦è¿”ã—ã¾ã™ã€‚

### Command primitiveã§ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’å®Ÿè£…ã™ã‚‹

Command primitiveã¯ã€LangGraphå…¬å¼ãƒ–ãƒ­ã‚°ã«ã‚ˆã‚‹ã¨ã€Œãƒãƒ¼ãƒ‰ã‹ã‚‰ã®æˆ»ã‚Šå€¤ã§ã‚¹ãƒ†ãƒ¼ãƒˆæ›´æ–°ã¨æ¬¡ã®ãƒãƒ¼ãƒ‰é·ç§»ã‚’åŒæ™‚ã«æŒ‡å®šã™ã‚‹ã€ä»•çµ„ã¿ã§ã™ã€‚å¾“æ¥ã®å›ºå®šã‚¨ãƒƒã‚¸ã‚’ä½¿ã‚ãšã€å®Ÿè¡Œæ™‚ã®åˆ¤æ–­ã§å‹•çš„ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ãƒãƒ³ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚

```python
from langgraph.graph import StateGraph, END
from langgraph.types import Command
from typing import Literal, TypedDict

class RAGState(TypedDict):
    query: str
    documents: list[dict]
    answer: str
    retry_count: int
    effort_level: str  # "low" | "medium" | "high"

def router_agent(state: RAGState) -> Command[Literal["retriever", "direct_answer"]]:
    """ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã‚’åˆ¤å®šã—ã€é©åˆ‡ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹"""
    query = state["query"]

    # ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«åŸºã¥ãeffortãƒ¬ãƒ™ãƒ«åˆ¤å®š
    complexity = assess_query_complexity(query)

    if complexity == "simple":
        return Command(
            goto="direct_answer",
            update={"effort_level": "low"}
        )
    else:
        return Command(
            goto="retriever",
            update={"effort_level": "high"}
        )

def retriever_agent(state: RAGState) -> Command[Literal["grader", "query_rewriter"]]:
    """æ–‡æ›¸æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã«å¿œã˜ã¦æ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ±ºå®šã™ã‚‹"""
    docs = search_documents(state["query"])

    if len(docs) >= 3:
        return Command(
            goto="grader",
            update={"documents": docs}
        )
    else:
        return Command(
            goto="query_rewriter",
            update={"documents": docs, "retry_count": state["retry_count"] + 1}
        )

def grader_agent(state: RAGState) -> Command[Literal["generator", "query_rewriter"]]:
    """æ–‡æ›¸ã®é–¢é€£åº¦ã‚’è©•ä¾¡ã—ã€ååˆ†ãªã‚‰ç”Ÿæˆã€ä¸ååˆ†ãªã‚‰æ›¸ãæ›ãˆã¸"""
    relevant = [d for d in state["documents"] if is_relevant(d, state["query"])]

    if len(relevant) >= 2:
        return Command(
            goto="generator",
            update={"documents": relevant}
        )
    elif state["retry_count"] < 2:
        return Command(
            goto="query_rewriter",
            update={"retry_count": state["retry_count"] + 1}
        )
    else:
        # ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ã«é”ã—ãŸå ´åˆã€æ‰‹æŒã¡ã®æ–‡æ›¸ã§å›ç­”ç”Ÿæˆ
        return Command(
            goto="generator",
            update={"documents": relevant or state["documents"][:2]}
        )
```

å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå‡¦ç†çµæœã«åŸºã¥ã„ã¦æ¬¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‹•çš„ã«é¸æŠã™ã‚‹ãŸã‚ã€å¾“æ¥ã®StateGraphã®å›ºå®šã‚¨ãƒƒã‚¸å®šç¾©ã«æ¯”ã¹ã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«è¨˜è¿°ã§ãã¾ã™ã€‚

## Claude Sonnet 4.6ã®adaptive thinkingã‚’RAGã«çµ±åˆã™ã‚‹

Claude Sonnet 4.6ã®adaptive thinkingã¯ã€Anthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€Œã‚¯ã‚¨ãƒªã®è¤‡é›‘åº¦ã«å¿œã˜ã¦Claudeè‡ªèº«ãŒæ¨è«–ã®æ·±åº¦ã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹ã€æ©Ÿèƒ½ã§ã™ã€‚å¾“æ¥ã®extended thinkingï¼ˆ`budget_tokens`å›ºå®šï¼‰ã¨ç•°ãªã‚Šã€`thinking.type: "adaptive"`ã¨effortãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã§ã€å˜ç´”ã‚¯ã‚¨ãƒªã§ã¯æ¨è«–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€è¤‡é›‘ã‚¯ã‚¨ãƒªã§ã¯æ·±ã„æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

### effortãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¨è«–æ·±åº¦åˆ¶å¾¡ã‚’å®Ÿè£…ã™ã‚‹

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã§ã¯ã€ã‚¯ã‚¨ãƒªã®ç¨®é¡ã«ã‚ˆã£ã¦å¿…è¦ãªæ¨è«–æ·±åº¦ãŒç•°ãªã‚Šã¾ã™ã€‚äº‹å®Ÿç¢ºèªï¼ˆã€ŒPythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿã€ï¼‰ã«ã¯effort="low"ã€è¤‡æ•°æ–‡æ›¸ã®çµ±åˆæ¨è«–ï¼ˆã€Œ3ã¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é•·æ‰€çŸ­æ‰€ã‚’æ¯”è¼ƒã—ã¦ã€ï¼‰ã«ã¯effort="high"ãŒé©åˆ‡ã§ã™ã€‚

```python
from anthropic import Anthropic

client = Anthropic()

def create_adaptive_llm(effort: str = "high"):
    """effortåˆ¥ã®Claude Sonnet 4.6ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹

    Args:
        effort: "low" | "medium" | "high"
            - low: å˜ç´”ãªäº‹å®Ÿç¢ºèªã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
            - medium: æ–‡æ›¸è¦ç´„ã€é–¢é€£åº¦åˆ¤å®š
            - high: è¤‡æ•°æ–‡æ›¸ã®çµ±åˆæ¨è«–ã€æ¯”è¼ƒåˆ†æ
    """
    def invoke(messages: list[dict]) -> str:
        response = client.messages.create(
            model="claude-sonnet-4-6-20250929",
            max_tokens=8192,
            thinking={"type": "adaptive"},
            output_config={"effort": effort},
            messages=messages,
        )
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã®ã¿è¿”ã™ï¼ˆthinkingãƒ–ãƒ­ãƒƒã‚¯ã¯ãƒ­ã‚°ã«è¨˜éŒ²ï¼‰
        for block in response.content:
            if block.type == "text":
                return block.text
        return ""

    return invoke

# ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸeffortãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
EFFORT_ROUTER = {
    "factual": "low",       # äº‹å®Ÿç¢ºèª: æ¨è«–ä¸è¦
    "summary": "medium",    # è¦ç´„ãƒ»åˆ†é¡: ä¸­ç¨‹åº¦ã®æ¨è«–
    "analytical": "high",   # åˆ†æãƒ»æ¯”è¼ƒ: æ·±ã„æ¨è«–
}
```

**ãªãœadaptive thinkingã‚’é¸ã‚“ã ã‹:**

- **budget_tokenså›ºå®šã®å•é¡Œ**: å¾“æ¥ã®extended thinkingã§ã¯æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’äº‹å‰ã«æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€å˜ç´”ã‚¯ã‚¨ãƒªã§ã‚‚ç„¡é§„ãªæ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¶ˆè²»ã•ã‚Œã¦ã„ãŸ
- **effortã®æŸ”è»Ÿæ€§**: Anthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€effort="low"ã§ã¯å˜ç´”ãªå•é¡Œã®æ€è€ƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€effort="high"ã§ã¯å¸¸ã«æ·±ã„æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹
- **interleaved thinkingã®è‡ªå‹•æœ‰åŠ¹åŒ–**: adaptive modeã§ã¯tool callé–“ã®æ¨è«–ï¼ˆinterleaved thinkingï¼‰ãŒè‡ªå‹•ã§æœ‰åŠ¹ã«ãªã‚Šã€æ¤œç´¢çµæœã‚’æ®µéšçš„ã«è©•ä¾¡ã§ãã‚‹

> **åˆ¶ç´„**: adaptive thinkingã¯Claude Sonnet 4.6ã¨Opus 4.6ã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚Sonnet 4.5ä»¥å‰ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯`thinking.type: "enabled"`ã¨`budget_tokens`ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€`budget_tokens`æŒ‡å®šã¯Sonnet 4.6/Opus 4.6ã§ã¯éæ¨å¥¨ï¼ˆdeprecatedï¼‰ã¨ãªã£ã¦ã„ã¾ã™ã€‚

### interleaved thinkingã§æ¤œç´¢çµæœã‚’æ®µéšçš„ã«è©•ä¾¡ã™ã‚‹

Anthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€interleaved thinkingã¯tool callé–“ã«Claudeè‡ªèº«ã®æ¨è«–ã‚’æŒŸã‚€æ©Ÿèƒ½ã§ã™ã€‚RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€æ–‡æ›¸æ¤œç´¢â†’é–¢é€£åº¦è©•ä¾¡â†’è¿½åŠ æ¤œç´¢ã®ã‚µã‚¤ã‚¯ãƒ«ã§ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’è¸ã¾ãˆãŸæ®µéšçš„æ¨è«–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
from anthropic import Anthropic

client = Anthropic()

# æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®å®šç¾©
search_tool = {
    "name": "search_documents",
    "description": "ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹",
    "input_schema": {
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æ¤œç´¢ã‚¯ã‚¨ãƒª"},
            "top_k": {"type": "integer", "description": "å–å¾—ä»¶æ•°", "default": 5}
        },
        "required": ["query"]
    }
}

relevance_tool = {
    "name": "evaluate_relevance",
    "description": "æ–‡æ›¸ã®é–¢é€£åº¦ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆ0.0-1.0ï¼‰",
    "input_schema": {
        "type": "object",
        "properties": {
            "document_id": {"type": "string"},
            "query": {"type": "string"}
        },
        "required": ["document_id", "query"]
    }
}

def rag_with_interleaved_thinking(query: str) -> str:
    """interleaved thinkingã‚’æ´»ç”¨ã—ãŸRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

    ClaudeãŒtool callé–“ã«æ¨è«–ã‚’æŒŸã¿ã€æ¤œç´¢æˆ¦ç•¥ã‚’å‹•çš„ã«èª¿æ•´ã™ã‚‹
    """
    messages = [{"role": "user", "content": query}]
    system_prompt = """ã‚ãªãŸã¯ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ‰‹é †ã§å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:
1. ã¾ãšã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã€é©åˆ‡ãªæ¤œç´¢æˆ¦ç•¥ã‚’æ±ºå®šã™ã‚‹
2. search_documentsã§æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹
3. æ¤œç´¢çµæœã®é–¢é€£åº¦ã‚’è©•ä¾¡ã—ã€ä¸ååˆ†ãªã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä¿®æ­£ã—ã¦å†æ¤œç´¢ã™ã‚‹
4. ååˆ†ãªæƒ…å ±ãŒå¾—ã‚‰ã‚ŒãŸã‚‰ã€æ ¹æ‹ ã‚’æ˜ç¤ºã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹

æ¨è«–ãŒå¿…è¦ãªå ´åˆã¯æ€è€ƒã‚’æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚"""

    response = client.messages.create(
        model="claude-sonnet-4-6-20250929",
        max_tokens=16000,
        thinking={"type": "adaptive"},
        output_config={"effort": "high"},
        system=system_prompt,
        tools=[search_tool, relevance_tool],
        messages=messages,
    )
    # tool_useãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚‹å ´åˆã¯ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
    # ï¼ˆå®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã¯çœç•¥ã€tool_resultã‚’è¿”ã—ã¦å†åº¦APIå‘¼ã³å‡ºã—ï¼‰
    return process_response(response, messages)
```

adaptive thinkingã‚’æœ‰åŠ¹ã«ã—ã¦ãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã™ã‚‹ã¨ã€Claudeã¯å„tool callé–“ã«æ€è€ƒãƒ–ãƒ­ãƒƒã‚¯ã‚’è‡ªå‹•ç”Ÿæˆã—ã€ã€Œæ¤œç´¢çµæœã®é–¢é€£åº¦ãŒä½ã„ãŸã‚ã‚¯ã‚¨ãƒªã‚’ä¿®æ­£ã—ã¦å†æ¤œç´¢ã™ã‚‹ã€ã¨ã„ã£ãŸæ®µéšçš„æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

## ä¸¦åˆ—ãƒãƒ¼ãƒ‰å®Ÿè¡Œã§å¿œç­”é€Ÿåº¦ã‚’æœ€é©åŒ–ã™ã‚‹

LangGraphã§ã¯è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã¸ã®fan-outï¼ˆåˆ†å²ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ä¸¦åˆ—å®Ÿè¡ŒãŒå¯èƒ½ã§ã™ã€‚LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€ã€Œè¤‡æ•°ã®ã‚¨ãƒƒã‚¸ãŒåŒä¸€ãƒãƒ¼ãƒ‰ã‹ã‚‰ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã¸åˆ†å²ã™ã‚‹å ´åˆã€LangGraphã¯è‡ªå‹•çš„ã«ã“ã‚Œã‚‰ã®ãƒãƒ¼ãƒ‰ã‚’åŒæ™‚ã«å®Ÿè¡Œï¼ˆsuperstepï¼‰ã™ã‚‹ã€ä»•çµ„ã¿ã§ã™ã€‚

### ä¸¦åˆ—æ¤œç´¢ã¨éåŒæœŸgradingè¨­è¨ˆã‚’å®Ÿè£…ã™ã‚‹

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã§ã¯ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã¸ã®æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚

```python
from langgraph.func import entrypoint, task
from langgraph.checkpoint.memory import MemorySaver
import asyncio

@task
async def search_vector_db(query: str) -> list[dict]:
    """ãƒ™ã‚¯ãƒˆãƒ«DBã‹ã‚‰é¡ä¼¼æ–‡æ›¸ã‚’æ¤œç´¢ã™ã‚‹"""
    results = await vector_store.asimilarity_search(query, k=5)
    return [{"source": "vector", "content": r.page_content} for r in results]

@task
async def search_keyword_index(query: str) -> list[dict]:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰å…¨æ–‡æ¤œç´¢ã™ã‚‹"""
    results = await keyword_index.asearch(query, limit=5)
    return [{"source": "keyword", "content": r["text"]} for r in results]

@task
async def search_knowledge_graph(query: str) -> list[dict]:
    """ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‹ã‚‰é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ¤œç´¢ã™ã‚‹"""
    entities = await kg_client.aquery(query, max_hops=2)
    return [{"source": "kg", "content": e["description"]} for e in entities]

@task
async def merge_and_rerank(
    query: str,
    vector_results: list[dict],
    keyword_results: list[dict],
    kg_results: list[dict],
) -> list[dict]:
    """3ã‚½ãƒ¼ã‚¹ã®æ¤œç´¢çµæœã‚’çµ±åˆã—ã€ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã™ã‚‹"""
    all_docs = vector_results + keyword_results + kg_results
    # Cross-encoderã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
    scored = await reranker.ascore(query, [d["content"] for d in all_docs])
    ranked = sorted(zip(all_docs, scored), key=lambda x: x[1], reverse=True)
    return [doc for doc, score in ranked[:5]]

@entrypoint(checkpointer=MemorySaver())
async def parallel_rag_pipeline(query: str) -> str:
    """3ã‚½ãƒ¼ã‚¹ä¸¦åˆ—æ¤œç´¢ â†’ ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚° â†’ å›ç­”ç”Ÿæˆ"""

    # 3ã¤ã®æ¤œç´¢ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆfan-outï¼‰
    vector_future = search_vector_db(query)
    keyword_future = search_keyword_index(query)
    kg_future = search_knowledge_graph(query)

    # å…¨çµæœã®åˆ°ç€ã‚’å¾…æ©Ÿï¼ˆfan-inï¼‰
    vector_results = await vector_future
    keyword_results = await keyword_future
    kg_results = await kg_future

    # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã§ä¸Šä½5ä»¶ã«çµã‚Šè¾¼ã¿
    top_docs = await merge_and_rerank(query, vector_results, keyword_results, kg_results)

    # adaptive thinkingã§å›ç­”ç”Ÿæˆ
    return await generate_with_adaptive_thinking(query, top_docs)
```

**ãªãœä¸¦åˆ—æ¤œç´¢ã‚’æ¡ç”¨ã—ãŸã‹:**

- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›**: é€æ¬¡æ¤œç´¢ã§ã¯åˆè¨ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒåŠ ç®—ã•ã‚Œã‚‹ãŒã€ä¸¦åˆ—å®Ÿè¡Œã«ã‚ˆã‚Šæœ€ã‚‚é…ã„æ¤œç´¢ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«æƒã†
- **æ¤œç´¢å“è³ªå‘ä¸Š**: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã¯å˜ä¸€æ‰‹æ³•ã‚ˆã‚Šå†ç¾ç‡ãŒé«˜ã„ã€‚RAGãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–ã®æŠ€è¡“è¨˜äº‹ã«ã‚ˆã‚‹ã¨ã€sub-200msã®é”æˆãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹**: ä¸€éƒ¨ã‚½ãƒ¼ã‚¹ã®éšœå®³æ™‚ã§ã‚‚æ®‹ã‚Šã®çµæœã§å›ç­”ç”Ÿæˆå¯èƒ½

> **ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ**: ä¸¦åˆ—å®Ÿè¡Œæ™‚ã¯APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«æ³¨æ„ã€‚`config={"max_concurrency": 3}`ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ã§ãã¾ã™ã€‚

### effortãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹å‹•çš„æœ€é©åŒ–ã‚’çµ„ã¿è¾¼ã‚€

å…¨ã‚¯ã‚¨ãƒªã«ä¸€å¾‹ã§effort="high"ã‚’æŒ‡å®šã™ã‚‹ã¨ã€å˜ç´”ãªè³ªå•ã§ã‚‚ä¸è¦ãªæ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¶ˆè²»ã•ã‚Œã¾ã™ã€‚ã‚¯ã‚¨ãƒªåˆ†é¡å™¨ã¨çµ„ã¿åˆã‚ã›ã¦ã€effortãƒ¬ãƒ™ãƒ«ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ‰åŠ¹ã§ã™ã€‚

```python
from langgraph.graph import StateGraph, END
from langgraph.types import Command
from typing import Literal, TypedDict

class AdaptiveRAGState(TypedDict):
    query: str
    query_type: str       # "factual" | "summary" | "analytical"
    effort_level: str     # "low" | "medium" | "high"
    documents: list[dict]
    answer: str

@task
async def classify_query(query: str) -> dict:
    """ã‚¯ã‚¨ãƒªã‚’åˆ†é¡ã—ã€é©åˆ‡ãªeffortãƒ¬ãƒ™ãƒ«ã‚’æ±ºå®šã™ã‚‹

    åˆ†é¡åŸºæº–:
    - factual: å˜ä¸€äº‹å®Ÿã®ç¢ºèªï¼ˆeffort=low, æ¨è«–ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    - summary: æ–‡æ›¸ã®è¦ç´„ãƒ»åˆ†é¡ï¼ˆeffort=medium, ä¸­ç¨‹åº¦ã®æ¨è«–ï¼‰
    - analytical: æ¯”è¼ƒãƒ»åˆ†æãƒ»æ¨è«–ï¼ˆeffort=high, æ·±ã„æ¨è«–ï¼‰
    """
    classifier_llm = create_adaptive_llm(effort="low")
    result = classifier_llm([{
        "role": "user",
        "content": f"""ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
factual: å˜ä¸€ã®äº‹å®Ÿç¢ºèªï¼ˆä¾‹: ã€ŒPythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿã€ï¼‰
summary: æƒ…å ±ã®è¦ç´„ãƒ»æ•´ç†ï¼ˆä¾‹: ã€Œã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç‚¹ã¯ï¼Ÿã€ï¼‰
analytical: æ¯”è¼ƒãƒ»åˆ†æãƒ»æ¨è«–ï¼ˆä¾‹: ã€ŒAã¨Bã®é•ã„ã‚’èª¬æ˜ã—ã¦ã€ï¼‰

ã‚¯ã‚¨ãƒª: {query}
åˆ†é¡ï¼ˆfactual/summary/analyticalï¼‰:"""
    }])
    query_type = result.strip().lower()
    if query_type not in ("factual", "summary", "analytical"):
        query_type = "analytical"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å®‰å…¨å´ã«å€’ã™
    return {"query_type": query_type, "effort_level": EFFORT_ROUTER[query_type]}

@entrypoint(checkpointer=MemorySaver())
async def adaptive_rag(query: str) -> str:
    """effortå‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ä»˜ãRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    # Step 1: ã‚¯ã‚¨ãƒªåˆ†é¡ï¼ˆeffort=lowã§é«˜é€Ÿå®Ÿè¡Œï¼‰
    classification = await classify_query(query)
    effort = classification["effort_level"]

    # Step 2: æ¤œç´¢ï¼ˆåˆ†é¡çµæœã«é–¢ã‚ã‚‰ãšå®Ÿè¡Œï¼‰
    docs = await retrieve_documents(query)
    relevant_docs = await grade_documents(query, docs)

    # Step 3: effortãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸå›ç­”ç”Ÿæˆ
    answer_llm = create_adaptive_llm(effort=effort)
    context = "\n\n".join(d["content"] for d in relevant_docs)
    answer = answer_llm([{
        "role": "user",
        "content": f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n\nè³ªå•: {query}"
    }])

    return answer
```

ã‚¯ã‚¨ãƒªåˆ†é¡ã‚’effort="low"ã§é«˜é€Ÿå®Ÿè¡Œã—ã€å›ç­”ç”Ÿæˆã¯åˆ†é¡çµæœã«å¿œã˜ãŸeffortã§å®Ÿè¡Œã—ã¾ã™ã€‚Anthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€effort="low"ã§ã¯æ€è€ƒãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ãŸã‚ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æœ€å°åŒ–ã§ãã¾ã™ã€‚

## æœ¬ç•ªé‹ç”¨ã®ãŸã‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã¨human-in-the-loopã‚’è¨­è¨ˆã™ã‚‹

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã‚’æœ¬ç•ªé‹ç”¨ã™ã‚‹éš›ã«ã¯ã€éšœå®³ã‹ã‚‰ã®å¾©æ—§ã¨ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒã‚§ãƒƒã‚¯ã®ä»•çµ„ã¿ãŒä¸å¯æ¬ ã§ã™ã€‚LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€functional APIã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨interruptã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã‚’å®£è¨€çš„ã«å®Ÿè£…ã§ãã¾ã™ã€‚

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨å¹»è¦šãƒã‚§ãƒƒã‚¯ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã™ã‚‹

`@task`ã®çµæœã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«è‡ªå‹•æ°¸ç¶šåŒ–ã•ã‚Œã¾ã™ã€‚LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚Œã°ã€ã‚¿ã‚¹ã‚¯çµæœãŒæ°¸ç¶šåŒ–æ¸ˆã¿ã§ã‚ã‚Œã°å†å®Ÿè¡Œæ™‚ã«é‡è¤‡å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯`PostgresSaver`ã‚’ä½¿ç”¨ã—ã€å¹»è¦šãƒã‚§ãƒƒã‚¯ã¨human-in-the-loopã‚’çµ„ã¿åˆã‚ã›ã¾ã™ã€‚

```python
from langgraph.func import entrypoint, task
from langgraph.checkpoint.postgres import PostgresSaver
from langgraph.types import interrupt
import asyncio

checkpointer = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost:5432/langgraph_checkpoints"
)

@task
async def retrieve_with_retry(query: str, max_retries: int = 3) -> list[dict]:
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãæ–‡æ›¸æ¤œç´¢ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ• + ã‚¸ãƒƒã‚¿ï¼‰"""
    for attempt in range(max_retries):
        try:
            results = await vector_store.asimilarity_search(query, k=5)
            return [{"content": r.page_content, "score": r.metadata.get("score", 0)}
                    for r in results]
        except Exception:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep((2 ** attempt) + (asyncio.get_event_loop().time() % 1))
    return []

@task
async def generate_with_hallucination_check(query: str, documents: list[dict]) -> dict:
    """å›ç­”ç”Ÿæˆ + å¹»è¦šãƒã‚§ãƒƒã‚¯ï¼ˆ2æ®µéšLLMå‘¼ã³å‡ºã—ï¼‰"""
    context = "\n\n".join(d["content"] for d in documents)
    answer_llm = create_adaptive_llm(effort="high")
    answer = answer_llm([{"role": "user",
        "content": f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n\nè³ªå•: {query}"}])

    checker_llm = create_adaptive_llm(effort="medium")
    check = checker_llm([{"role": "user",
        "content": f"å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã„ã‚‹ã‹åˆ¤å®šï¼ˆgrounded/not_groundedï¼‰:\nå›ç­”: {answer}\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}"}])

    is_grounded = "grounded" in check.lower() and "not_grounded" not in check.lower()
    return {"answer": answer, "is_grounded": is_grounded}

@entrypoint(checkpointer=checkpointer)
async def production_rag(query: str) -> str:
    """æœ¬ç•ªç”¨RAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ + å¹»è¦šãƒã‚§ãƒƒã‚¯ + HiTLï¼‰"""
    docs = await retrieve_with_retry(query)
    relevant_docs = await grade_documents(query, docs)
    result = await generate_with_hallucination_check(query, relevant_docs)

    if not result["is_grounded"]:
        # å¹»è¦šæ¤œå‡ºæ™‚: human-in-the-loopã§äººé–“åˆ¤æ–­ã‚’è¦æ±‚
        human_decision = interrupt({
            "query": query,
            "generated_answer": result["answer"],
            "reason": "å¹»è¦šãƒã‚§ãƒƒã‚¯ã§ä¸åˆæ ¼ã€‚å›ç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        })
        if human_decision.get("approved"):
            return human_decision.get("revised_answer", result["answer"])
        return "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰ãˆã¦ãŠè©¦ã—ãã ã•ã„ã€‚"

    return result["answer"]
```

`interrupt()`ã¯ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¸€æ™‚åœæ­¢ã—ã€å¾Œã‹ã‚‰`Command(resume=value)`ã§å†é–‹ã§ãã¾ã™ã€‚åŒ»ç™‚ãƒ»æ³•å¾‹ãƒ»é‡‘èãªã©èª¤å›ç­”ãƒªã‚¹ã‚¯ãŒé«˜ã„é ˜åŸŸã§æœ‰åŠ¹ã§ã™ã€‚

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**

å¹»è¦šãƒã‚§ãƒƒã‚¯ã®LLMå‘¼ã³å‡ºã—ãŒè¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€å‡¦ç†æ™‚é–“ãŒå¢—åŠ ã—ã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥ã®ä½¿ã„åˆ†ã‘ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | å¹»è¦šãƒã‚§ãƒƒã‚¯ | human-in-the-loop | æƒ³å®šãƒ¬ã‚¤ãƒ†ãƒ³ã‚· |
|-------------|-------------|-------------------|-------------|
| ç¤¾å†…FAQæ¤œç´¢ | æœ‰åŠ¹ | ç„¡åŠ¹ | 2-4ç§’ |
| ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆ | æœ‰åŠ¹ | ä¿¡é ¼åº¦ä½æ™‚ã®ã¿ | 2-8ç§’ |
| åŒ»ç™‚ãƒ»æ³•å¾‹ç›¸è«‡ | æœ‰åŠ¹ | å¸¸æ™‚æœ‰åŠ¹ | äººé–“å¿œç­”æ™‚é–“ä¾å­˜ |
| ç¤¾å†…ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ | ç„¡åŠ¹ | ç„¡åŠ¹ | 1-2ç§’ |

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| `@task`ã®æˆ»ã‚Šå€¤ã§ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã‚¨ãƒ©ãƒ¼ | LangChainã®`Document`ç­‰ã‚’ãã®ã¾ã¾è¿”ã—ã¦ã„ã‚‹ | `dict`ã«å¤‰æ›ã—ã¦è¿”ã™ï¼ˆJSON-serializableå¿…é ˆï¼‰ |
| adaptive thinkingãŒå¸¸ã«ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ | effort="low"ã§è¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦ã„ã‚‹ | ã‚¯ã‚¨ãƒªåˆ†é¡å™¨ã§effortãƒ¬ãƒ™ãƒ«ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ |
| ä¸¦åˆ—æ¤œç´¢ã§ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ | fan-outã§åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒAPIã®ä¸Šé™ã‚’è¶…é | `config={"max_concurrency": N}`ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ |
| ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®DBå®¹é‡å¢—å¤§ | é•·æœŸé–“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å±¥æ­´ãŒè“„ç© | TTLè¨­å®šã¾ãŸã¯ãƒãƒƒãƒå‰Šé™¤ã‚¸ãƒ§ãƒ–ã‚’å®šæœŸå®Ÿè¡Œ |
| interleaved thinkingã®èª²é‡‘ãŒæƒ³å®šä»¥ä¸Š | æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³ãŒå¤šãç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ | effortã‚’ä¸‹ã’ã‚‹ã€ã¾ãŸã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ€è€ƒã‚’åˆ¶é™ |
| `interrupt()`å¾Œã®å†é–‹ã§ã‚¨ãƒ©ãƒ¼ | ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ã‚¿ãƒ¼ãŒæ°¸ç¶šåŒ–ã•ã‚Œã¦ã„ãªã„ | `MemorySaver`ã§ã¯ãªã`PostgresSaver`ã‚’ä½¿ç”¨ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- functional APIï¼ˆ`@task`/`@entrypoint`ï¼‰ã¨Command primitiveã§ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã‚’ç›´æ„Ÿçš„ã«è¨˜è¿°ãƒ»å‹•çš„ãƒãƒ³ãƒ‰ã‚ªãƒ•å¯èƒ½
- adaptive thinkingã¨effortãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€ã‚¯ã‚¨ãƒªè¤‡é›‘åº¦ã«å¿œã˜ãŸæ¨è«–æ·±åº¦ã‚’è‡ªå‹•åˆ¶å¾¡
- ä¸¦åˆ—ãƒãƒ¼ãƒ‰å®Ÿè¡Œã§ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹æ¤œç´¢ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å‰Šæ¸›ã—ã€interleaved thinkingã§æ®µéšçš„è©•ä¾¡
- æœ¬ç•ªé‹ç”¨ã«ã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ°¸ç¶šåŒ–ãƒ»å¹»è¦šãƒã‚§ãƒƒã‚¯ãƒ»human-in-the-loopã®çµ„ã¿åˆã‚ã›ãŒå¿…è¦

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- è‡ªç¤¾ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§effortãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å–ã‚Šã€æœ€é©ãªeffortãƒ¬ãƒ™ãƒ«ã®é–¾å€¤ã‚’æ±ºå®šã™ã‚‹
- `PostgresSaver`ã‚’ä½¿ã£ãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ°¸ç¶šåŒ–ã‚’è¨­å®šã—ã€ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã‚’ç¢ºä¿ã™ã‚‹
- RAGASç­‰ã®è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å°å…¥ã—ã€æ¨è«–ç²¾åº¦ï¼ˆfaithfulnessã€answer relevancyï¼‰ã‚’å®šé‡ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹

**é–¢é€£è¨˜äº‹:**

- [LangGraphÃ—Claude Sonnet 4.6ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã§Agentic RAGã‚³ã‚¹ãƒˆ90%å‰Šæ¸›](https://zenn.dev/0h_n0/articles/555a4e799660de)
- [LangGraph Supervisor vs Swarmï¼šãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆRAGã®å®Ÿè£…æ¯”è¼ƒ](https://zenn.dev/0h_n0/articles/c5c769fcd39455)
- [LangGraphã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‹RAGã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æœ€é©åŒ–ï¼šã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°Ã—éåŒæœŸå®Ÿè¡Œã§å¿œç­”é€Ÿåº¦ã‚’3å€æ”¹å–„ã™ã‚‹](https://zenn.dev/0h_n0/articles/433702e83b26ed)

## å‚è€ƒ

- [LangGraph Functional API Overviewï¼ˆå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰](https://docs.langchain.com/oss/python/langgraph/functional-api)
- [Command: A new tool for building multi-agent architectures in LangGraphï¼ˆLangChainãƒ–ãƒ­ã‚°ï¼‰](https://blog.langchain.com/command-a-new-tool-for-multi-agent-architectures-in-langgraph/)
- [Adaptive thinkingï¼ˆAnthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰](https://platform.claude.com/docs/en/build-with-claude/adaptive-thinking)
- [Extended thinkingï¼ˆAnthropicå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰](https://platform.claude.com/docs/en/build-with-claude/extended-thinking)
- [Building Agentic RAG Systems with LangGraph: The 2026 Guide](https://rahulkolekar.com/building-agentic-rag-systems-with-langgraph/)
- [RAG Latency Optimization End-to-End: Techniques and Best Practicesï¼ˆ2026å¹´2æœˆï¼‰](https://dasroot.net/posts/2026/02/rag-latency-optimization-vector-database-caching-hybrid-search/)
- [LangGraph 1.0 is now generally availableï¼ˆLangChain Changelogï¼‰](https://changelog.langchain.com/announcements/langgraph-1-0-is-now-generally-available)
- [Scaling LangGraph Agents: Parallelization, Subgraphs, and Map-Reduce Trade-Offs](https://aipractitioner.substack.com/p/scaling-langgraph-agents-parallelization)

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
