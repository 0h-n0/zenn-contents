---
title: "ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…ã‚¬ã‚¤ãƒ‰ï¼šLLM APIã‚³ã‚¹ãƒˆ73%å‰Šæ¸›ã¨å¿œç­”97%é«˜é€ŸåŒ–"
emoji: "ğŸ§ "
type: "tech"
topics: ["llm", "redis", "cache", "python", "ai"]
published: false
---

# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…ã‚¬ã‚¤ãƒ‰ï¼šLLM APIã‚³ã‚¹ãƒˆ73%å‰Šæ¸›ã¨å¿œç­”97%é«˜é€ŸåŒ–

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä»•çµ„ã¿ã¨ã€å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã®ä½¿ã„åˆ†ã‘
- RedisVL `SemanticCache` ã¨ LangChain `RedisSemanticCache` ã«ã‚ˆã‚‹å®Ÿè£…æ‰‹æ³•
- é¡ä¼¼åº¦é–¾å€¤ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã¨å½é™½æ€§ã‚’3%ä»¥ä¸‹ã«æŠ‘ãˆã‚‹æ–¹æ³•
- AWS MemoryDB / Redis LangCache ã‚’ä½¿ã£ãŸæœ¬ç•ªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨­è¨ˆ
- TTLæˆ¦ç•¥ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ãƒ»ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œã®é‹ç”¨ãƒã‚¦ãƒã‚¦

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ¬ç•ªé‹ç”¨ã—ã¦ã„ã‚‹ä¸­ç´šè€…ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.10+ ã®åŸºç¤æ–‡æ³•
  - OpenAI API / Anthropic Claude API ã®åŸºæœ¬çš„ãªåˆ©ç”¨çµŒé¨“
  - Redis ã®åŸºæœ¬æ¦‚å¿µï¼ˆGET/SETã€TTLï¼‰
  - ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingï¼‰ã®æ¦‚å¿µç†è§£

## çµè«–ãƒ»æˆæœ

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é©åˆ‡ã«å°å…¥ã™ã‚‹ã“ã¨ã§ã€**LLM APIã‚³ã‚¹ãƒˆã‚’50-80%å‰Šæ¸›ã—ã€å¿œç­”æ™‚é–“ã‚’ç§’å˜ä½ã‹ã‚‰ãƒŸãƒªç§’å˜ä½ã«çŸ­ç¸®**ã§ãã¾ã™ã€‚VentureBeatã®å ±å‘Šã«ã‚ˆã‚‹ã¨ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å°å…¥ã§**ã‚³ã‚¹ãƒˆã‚’73%å‰Šæ¸›**ã—ãŸäº‹ä¾‹ãŒã‚ã‚Šã¾ã™ã€‚AWS MemoryDBã‚’ç”¨ã„ãŸæ¤œè¨¼ã§ã¯**ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®97.6%çŸ­ç¸®**ï¼ˆ5ç§’â†’120msï¼‰ãŒç¢ºèªã•ã‚Œã¦ã„ã¾ã™ã€‚

ãŸã ã—ã€é–¾å€¤è¨­å®šã‚’èª¤ã‚‹ã¨å½é™½æ€§ï¼ˆæ„å‘³ãŒç•°ãªã‚‹ã®ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã™ã‚‹ï¼‰ãŒç™ºç”Ÿã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã‚’æãªã†ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€å®Ÿè£…ã‹ã‚‰é–¾å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€æœ¬ç•ªé‹ç”¨ã¾ã§ã‚’æ®µéšçš„ã«è§£èª¬ã—ã¾ã™ã€‚

> **é–¢é€£è¨˜äº‹**: LLMã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å…¨èˆ¬ã®3å±¤æˆ¦ç•¥ï¼ˆå®Œå…¨ä¸€è‡´ãƒ»ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ï¼‰ã«ã¤ã„ã¦ã¯ã€[LLMå‡ºåŠ›ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥](https://zenn.dev/0h_n0/articles/d32f933fec9176)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚æœ¬è¨˜äº‹ã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…ã«ç‰¹åŒ–ã—ãŸæ·±æ˜ã‚Šè¨˜äº‹ã§ã™ã€‚

## ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹

å¾“æ¥ã®å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§ã¯ã€`"Pythonã®ãƒªã‚¹ãƒˆæ“ä½œ"` ã¨ `"Pythonã§ãƒªã‚¹ãƒˆã‚’æ“ä½œã™ã‚‹æ–¹æ³•"` ã¯**åˆ¥ã®ã‚¯ã‚¨ãƒª**ã¨ã—ã¦æ‰±ã‚ã‚Œã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ã¾ã›ã‚“ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ã€ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›ã—ã€**æ„å‘³ã®é¡ä¼¼åº¦**ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¸€è‡´ã‚’åˆ¤å®šã—ã¾ã™ã€‚

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†ãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒª] --> B[Embeddingãƒ¢ãƒ‡ãƒ«ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–]
    B --> C{ãƒ™ã‚¯ãƒˆãƒ«DBã§é¡ä¼¼æ¤œç´¢}
    C -->|é¡ä¼¼åº¦ â‰¥ é–¾å€¤| D[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: ä¿å­˜æ¸ˆã¿å¿œç­”ã‚’è¿”å´]
    C -->|é¡ä¼¼åº¦ < é–¾å€¤| E[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: LLM APIã‚’å‘¼ã³å‡ºã—]
    E --> F[å¿œç­”ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨å…±ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜]
    F --> G[å¿œç­”ã‚’è¿”å´]
    D --> G
```

### 3ã¤ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®æ¯”è¼ƒ

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®ä¸­ã§**ã‚³ã‚¹ãƒˆã¨ãƒ’ãƒƒãƒˆç‡ã®ãƒãƒ©ãƒ³ã‚¹ã«å„ªã‚ŒãŸé¸æŠè‚¢**ã§ã™ã€‚

| æˆ¦ç•¥ | ä»•çµ„ã¿ | ãƒ’ãƒƒãƒˆç‡ | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸› | å®Ÿè£…ã‚³ã‚¹ãƒˆ |
|------|--------|---------|--------------|-----------|
| **å®Œå…¨ä¸€è‡´** | æ–‡å­—åˆ—ãƒãƒƒã‚·ãƒ¥ä¸€è‡´ | 15-30% | 99%+ | ä½ |
| **ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯** | ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ | 25-45% | 95-97% | ä¸­ |
| **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°** | APIå´ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹å†åˆ©ç”¨ | 50-70% | 50-90% | ä½ï¼ˆAPIä¾å­˜ï¼‰ |

**ãªãœã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’é¸ã¶ã‹:**
- å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã ã‘ã§ã¯ãƒ’ãƒƒãƒˆç‡ãŒä½ã™ãã‚‹ï¼ˆè‡ªç„¶è¨€èªã®è¡¨ç¾æºã‚Œã«å¯¾å¿œã§ããªã„ï¼‰
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã¯APIæä¾›å…ƒã«ä¾å­˜ã—ã€åˆ¶å¾¡ã—ãã‚Œãªã„
- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯**è‡ªå‰ã§åˆ¶å¾¡å¯èƒ½**ã§ã€é–¾å€¤ã‚„TTLã‚’ç”¨é€”ã«å¿œã˜ã¦èª¿æ•´ã§ãã‚‹

**æ³¨æ„ç‚¹:**
> ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯Embeddingè¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã‚ã‚‹ãŸã‚ã€**å…¨ã‚¯ã‚¨ãƒªã®å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¯å®Œå…¨ä¸€è‡´ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ˆã‚Š5-20mså¢—åŠ **ã—ã¾ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è€ƒæ…®ã—ã¦å°å…¥åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚

## RedisVL SemanticCacheã§å®Ÿè£…ã™ã‚‹

RedisVLã®`SemanticCache`ã¯ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ§‹ç¯‰ã«å¿…è¦ãªæ©Ÿèƒ½ã‚’çµ±åˆçš„ã«æä¾›ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚Redis Stackä¸Šã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è‡ªå‹•ä½œæˆã—ã€é¡ä¼¼åº¦æ¤œç´¢ãƒ»TTLç®¡ç†ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

### ç’°å¢ƒæ§‹ç¯‰

```bash
# Redis Stackã®èµ·å‹•ï¼ˆãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å«ã‚€ï¼‰
docker run -d --name redis-stack -p 6379:6379 redis/redis-stack:latest

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install redisvl openai
```

### åŸºæœ¬å®Ÿè£…

```python
# semantic_cache.py
from redisvl.extensions.cache.llm import SemanticCache
from redisvl.utils.vectorize import HFTextVectorizer

# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åˆæœŸåŒ–
cache = SemanticCache(
    name="llm_cache",
    redis_url="redis://localhost:6379",
    distance_threshold=0.1,  # ã‚³ã‚µã‚¤ãƒ³è·é›¢ã®é–¾å€¤ï¼ˆä½ã„ã»ã©å³å¯†ï¼‰
    vectorizer=HFTextVectorizer("redis/langcache-embed-v1"),
)


def ask_with_cache(prompt: str, llm_call) -> str:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãLLMå‘¼ã³å‡ºã—"""
    # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¤œç´¢
    results = cache.check(
        prompt=prompt,
        return_fields=["prompt", "response", "metadata"],
    )

    if results:
        print(f"[CACHE HIT] é¡ä¼¼ã‚¯ã‚¨ãƒª: {results[0]['prompt']}")
        return results[0]["response"]

    # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ â†’ LLMå‘¼ã³å‡ºã—
    print("[CACHE MISS] LLM APIã‚’å‘¼ã³å‡ºã—ã¾ã™")
    response = llm_call(prompt)

    # 3. å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    cache.store(
        prompt=prompt,
        response=response,
        metadata={"source": "openai", "model": "gpt-4o"},
    )
    return response
```

`distance_threshold=0.1`ã¯ã‚³ã‚µã‚¤ãƒ³è·é›¢ã®é–¾å€¤ã§ã€**å€¤ãŒå°ã•ã„ã»ã©å³å¯†ãªãƒãƒƒãƒãƒ³ã‚°**ã«ãªã‚Šã¾ã™ã€‚ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«æ›ç®—ã™ã‚‹ã¨`1 - 0.1 = 0.9`ï¼ˆ90%ä»¥ä¸Šã®é¡ä¼¼åº¦ã§ãƒ’ãƒƒãƒˆï¼‰ã«ç›¸å½“ã—ã¾ã™ã€‚

### LangChainçµ±åˆã«ã‚ˆã‚‹å®Ÿè£…

LangChainã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€`RedisSemanticCache`ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦è¨­å®šã§ãã¾ã™ã€‚

```python
# langchain_semantic_cache.py
from langchain_redis import RedisSemanticCache
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.globals import set_llm_cache

# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
set_llm_cache(
    RedisSemanticCache(
        redis_url="redis://localhost:6379",
        embeddings=OpenAIEmbeddings(model="text-embedding-3-small"),
        distance_threshold=0.1,
    )
)

# é€šå¸¸é€šã‚ŠLLMã‚’å‘¼ã³å‡ºã™ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯è‡ªå‹•é©ç”¨ï¼‰
llm = ChatOpenAI(model="gpt-4o", temperature=0)

# 1å›ç›®: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼ˆLLM APIå‘¼ã³å‡ºã—ï¼‰
response1 = llm.invoke("Pythonã§ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ")

# 2å›ç›®: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼ˆæ„å‘³çš„ã«é¡ä¼¼ â†’ ãƒŸãƒªç§’ã§å¿œç­”ï¼‰
response2 = llm.invoke("Pythonã®ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã„ã„ã§ã™ã‹ï¼Ÿ")
```

**ãªãœLangChainçµ±åˆã‚’é¸ã¶ã‹:**
- æ—¢å­˜ã®LangChainã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«**ã‚³ãƒ¼ãƒ‰å¤‰æ›´æœ€å°é™**ã§å°å…¥å¯èƒ½
- `set_llm_cache`ã®1è¡Œã§å…¨LLMå‘¼ã³å‡ºã—ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒé©ç”¨ã•ã‚Œã‚‹

**æ³¨æ„ç‚¹:**
> LangChainçµ±åˆã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç²’åº¦ãŒã€ŒLLMãƒ¢ãƒ‡ãƒ« + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å˜ä½ã«ãªã‚Šã¾ã™ã€‚åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚`temperature`ã‚„ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç•°ãªã‚‹å ´åˆã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ã¦ã—ã¾ã†å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå›ºå®šã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã®åˆ©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## é¡ä¼¼åº¦é–¾å€¤ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ€§èƒ½ã¯**é–¾å€¤è¨­å®š**ã§æ±ºã¾ã‚Šã¾ã™ã€‚é–¾å€¤ãŒç·©ã™ãã‚‹ã¨å½é™½æ€§ï¼ˆé–“é•ã£ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰ãŒå¢—ãˆã€å³ã—ã™ãã‚‹ã¨ãƒ’ãƒƒãƒˆç‡ãŒä½ä¸‹ã—ã¾ã™ã€‚

### ç”¨é€”åˆ¥ã®æ¨å¥¨é–¾å€¤

InfoQã®éŠ€è¡Œæ¥­å‹™ã§ã®ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã«ã‚ˆã‚‹ã¨ã€ã‚¯ã‚¨ãƒªã‚«ãƒ†ã‚´ãƒªã”ã¨ã«é–¾å€¤ã‚’åˆ†ã‘ã‚‹ã“ã¨ã§å½é™½æ€§ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã§ãã¾ã™ã€‚

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | æ¨å¥¨é–¾å€¤ï¼ˆã‚³ã‚µã‚¤ãƒ³è·é›¢ï¼‰ | ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦æ›ç®— | ç†ç”± |
|------------|----------------------|------------------|------|
| **FAQãƒ»ãƒ˜ãƒ«ãƒ—ãƒ‡ã‚¹ã‚¯** | 0.10-0.15 | 0.85-0.90 | è¡¨ç¾æºã‚ŒãŒå¤§ããã€å¯›å®¹ãªè¨­å®šãŒæœ‰åŠ¹ |
| **ãƒãƒªã‚·ãƒ¼ãƒ»è¦ç´„** | 0.06-0.10 | 0.90-0.94 | å›ç­”ã®æ­£ç¢ºæ€§ãŒé‡è¦ |
| **æ³¨æ–‡ãƒ»ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°** | 0.02-0.05 | 0.95-0.98 | å›ºæœ‰IDãƒ»æ•°å€¤ã‚’å«ã‚€ãŸã‚å³å¯†ã« |
| **ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ** | 0.05-0.08 | 0.92-0.95 | æ–‡è„ˆä¾å­˜åº¦ãŒé«˜ã„ |

### é–¾å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè£…

```python
# threshold_tuner.py
from dataclasses import dataclass
from redisvl.extensions.cache.llm import SemanticCache
from redisvl.utils.vectorize import HFTextVectorizer


@dataclass
class ThresholdTestResult:
    threshold: float
    hit_rate: float
    false_positive_rate: float
    avg_latency_ms: float


def evaluate_threshold(
    cache: SemanticCache,
    test_pairs: list[tuple[str, str, bool]],
    threshold: float,
) -> ThresholdTestResult:
    """é–¾å€¤ã‚’è©•ä¾¡ã™ã‚‹

    Args:
        cache: SemanticCacheã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        test_pairs: (query, expected_match_query, should_hit) ã®ãƒªã‚¹ãƒˆ
        threshold: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®é–¾å€¤
    """
    cache.set_threshold(threshold)
    hits = 0
    false_positives = 0
    total = len(test_pairs)

    for query, _, should_hit in test_pairs:
        results = cache.check(prompt=query)
        is_hit = len(results) > 0

        if is_hit:
            hits += 1
        if is_hit and not should_hit:
            false_positives += 1

    return ThresholdTestResult(
        threshold=threshold,
        hit_rate=hits / total,
        false_positive_rate=false_positives / total,
        avg_latency_ms=0.0,  # åˆ¥é€”è¨ˆæ¸¬
    )


# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¾‹
test_pairs = [
    ("Pythonã§ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹æ–¹æ³•", "Pythonã®ãƒªã‚¹ãƒˆã‚½ãƒ¼ãƒˆæ–¹æ³•", True),
    ("Pythonã®è¾æ›¸ã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹æ–¹æ³•", "Pythonã®ãƒªã‚¹ãƒˆã‚½ãƒ¼ãƒˆæ–¹æ³•", False),
    ("AWSã®S3ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹", "S3ãƒã‚±ãƒƒãƒˆã®ä½œã‚Šæ–¹", True),
    ("AWSã®EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’èµ·å‹•ã™ã‚‹", "S3ãƒã‚±ãƒƒãƒˆã®ä½œã‚Šæ–¹", False),
]

# è¤‡æ•°ã®é–¾å€¤ã§è©•ä¾¡
cache = SemanticCache(
    name="threshold_test",
    redis_url="redis://localhost:6379",
    distance_threshold=0.1,
    vectorizer=HFTextVectorizer("redis/langcache-embed-v1"),
)

# ã¾ãšãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€Œæ­£è§£ã€å´ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ ¼ç´
for _, expected_query, _ in test_pairs:
    cache.store(prompt=expected_query, response=f"Response for: {expected_query}")

for threshold in [0.05, 0.10, 0.15, 0.20, 0.30]:
    result = evaluate_threshold(cache, test_pairs, threshold)
    print(
        f"é–¾å€¤={result.threshold:.2f}: "
        f"ãƒ’ãƒƒãƒˆç‡={result.hit_rate:.1%}, "
        f"å½é™½æ€§ç‡={result.false_positive_rate:.1%}"
    )
```

### å½é™½æ€§ãŒ3-5%ã‚’è¶…ãˆãŸå ´åˆã®å¯¾ç­–

é–¾å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã ã‘ã§å½é™½æ€§ã‚’æŠ‘ãˆã‚‰ã‚Œãªã„å ´åˆã€ä»¥ä¸‹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ãŒæœ‰åŠ¹ã§ã™ã€‚

1. **ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–Embeddingãƒ¢ãƒ‡ãƒ«**: æ±ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆ`text-embedding-3-small`ï¼‰ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€é¡ä¼¼åº¦ã®ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹
2. **ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã‚ˆã‚‹ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆå€™è£œã«å¯¾ã—ã¦ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã€é–¾å€¤ä»¥ä¸‹ã‚’é™¤å¤–ã™ã‚‹
3. **ã‚¯ã‚¨ãƒªå‰å‡¦ç†**: ã‚¿ã‚¤ãƒä¿®æ­£ãƒ»ã‚¹ãƒ©ãƒ³ã‚°æ­£è¦åŒ–ã§å…¥åŠ›ã‚¯ã‚¨ãƒªã®å“è³ªã‚’çµ±ä¸€ã™ã‚‹

```python
# cross_encoder_reranking.py
from sentence_transformers import CrossEncoder

reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")


def check_with_reranking(
    cache: SemanticCache,
    query: str,
    rerank_threshold: float = 0.7,
) -> list[dict] | None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆå€™è£œã‚’ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§å†æ¤œè¨¼"""
    candidates = cache.check(prompt=query, num_results=3)
    if not candidates:
        return None

    # ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§é–¢é€£åº¦ã‚’å†ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    pairs = [(query, c["prompt"]) for c in candidates]
    scores = reranker.predict(pairs)

    # é–¾å€¤ä»¥ä¸Šã®ã¿æ¡ç”¨
    valid = [
        c for c, score in zip(candidates, scores) if score >= rerank_threshold
    ]
    return valid if valid else None
```

## æœ¬ç•ªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¨­è¨ˆã™ã‚‹

ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ¬ç•ªé‹ç”¨ã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«DBãƒ»Embeddingãƒ¢ãƒ‡ãƒ«ãƒ»TTLæˆ¦ç•¥ãƒ»ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œã‚’çµ„ã¿åˆã‚ã›ãŸè¨­è¨ˆãŒå¿…è¦ã§ã™ã€‚

### AWS MemoryDBã‚’ä½¿ã£ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

AWSç’°å¢ƒã§ã¯ã€MemoryDBã®ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæ§‹ç¯‰ã§ãã¾ã™ã€‚DevelopersIOã®æ¤œè¨¼ã§ã¯ã€**ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚’97.6%çŸ­ç¸®**ï¼ˆ5,010msâ†’188msï¼‰ã™ã‚‹çµæœãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

```mermaid
flowchart LR
    A[ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ] --> B[API Gateway]
    B --> C[Lambda]
    C --> D{MemoryDB<br/>ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢}
    D -->|HIT| E[ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”]
    D -->|MISS| F[Bedrock<br/>Claude/Titan]
    F --> G[å¿œç­”ã‚’MemoryDBã«ä¿å­˜]
    G --> E
    C --> H[Titan Embeddings V2<br/>1024æ¬¡å…ƒ]
```

### TTLæˆ¦ç•¥ã®è¨­è¨ˆ

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é®®åº¦ã‚’ä¿ã¤ãŸã‚ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¤‰åŒ–é »åº¦ã«å¿œã˜ãŸTTLè¨­è¨ˆãŒé‡è¦ã§ã™ã€‚

```python
# ttl_strategy.py
from enum import IntEnum


class CacheTTL(IntEnum):
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®åˆ¥ã”ã¨ã®TTLï¼ˆç§’ï¼‰"""
    REALTIME = 300        # 5åˆ†: ä¾¡æ ¼ãƒ»åœ¨åº«ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿
    FREQUENT = 3_600      # 1æ™‚é–“: ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±
    MODERATE = 14_400     # 4æ™‚é–“: è£½å“èª¬æ˜ãƒ»ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    STABLE = 86_400       # 24æ™‚é–“: FAQãƒ»ãƒãƒªã‚·ãƒ¼ãƒ»ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«
    PERMANENT = 604_800   # 7æ—¥é–“: æ•°å­¦çš„äº‹å®Ÿãƒ»ä¸å¤‰ã®çŸ¥è­˜


def get_ttl_for_query(query: str, category: str) -> int:
    """ã‚¯ã‚¨ãƒªã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ã„ã¦TTLã‚’è¿”ã™"""
    ttl_map = {
        "pricing": CacheTTL.REALTIME,
        "news": CacheTTL.FREQUENT,
        "documentation": CacheTTL.MODERATE,
        "faq": CacheTTL.STABLE,
        "knowledge": CacheTTL.PERMANENT,
    }
    return ttl_map.get(category, CacheTTL.MODERATE)
```

**ã‚ˆãã‚ã‚‹é–“é•ã„:**

æœ€åˆã¯TTLã‚’ä¸€å¾‹24æ™‚é–“ã«è¨­å®šã—ãŸããªã‚Šã¾ã™ãŒã€ä¾¡æ ¼æƒ…å ±ã‚„ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã«é•·ã„TTLã‚’é©ç”¨ã™ã‚‹ã¨**å¤ã„æƒ…å ±ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ã¦èª¤ã£ãŸå›ç­”ã‚’è¿”ã™**å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¤‰åŒ–é »åº¦ã«å¿œã˜ãŸTTLåˆ†é¡ã¯ã€é‹ç”¨åˆæœŸã‹ã‚‰è¨­è¨ˆã«å«ã‚ã‚‹ã¹ãã§ã™ã€‚

### ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆå¯¾å¿œ

è¤‡æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ‰±ã†å ´åˆã€`filterable_fields`ã§ãƒ†ãƒŠãƒ³ãƒˆåˆ†é›¢ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

```python
# multi_tenant_cache.py
from redisvl.extensions.cache.llm import SemanticCache
from redisvl.query.filter import Tag
from redisvl.utils.vectorize import HFTextVectorizer

tenant_cache = SemanticCache(
    name="tenant_cache",
    redis_url="redis://localhost:6379",
    distance_threshold=0.1,
    vectorizer=HFTextVectorizer("redis/langcache-embed-v1"),
    filterable_fields=[
        {"name": "tenant_id", "type": "tag"},
        {"name": "content_type", "type": "tag"},
    ],
)


def store_for_tenant(
    tenant_id: str, prompt: str, response: str, content_type: str
) -> None:
    """ãƒ†ãƒŠãƒ³ãƒˆåˆ¥ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜"""
    tenant_cache.store(
        prompt=prompt,
        response=response,
        filters={"tenant_id": tenant_id, "content_type": content_type},
    )


def check_for_tenant(tenant_id: str, prompt: str) -> str | None:
    """ãƒ†ãƒŠãƒ³ãƒˆåˆ¥ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ï¼ˆä»–ãƒ†ãƒŠãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã¯è¿”ã•ãªã„ï¼‰"""
    tenant_filter = Tag("tenant_id") == tenant_id
    results = tenant_cache.check(
        prompt=prompt,
        filter_expression=tenant_filter,
        num_results=1,
    )
    return results[0]["response"] if results else None
```

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç›£è¦–ã¨é‹ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨­è¨ˆã™ã‚‹

æœ¬ç•ªç’°å¢ƒã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ’ãƒƒãƒˆç‡ãƒ»å½é™½æ€§ç‡ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ç¶™ç¶šçš„ã«ç›£è¦–ã™ã‚‹ä»•çµ„ã¿ãŒå¿…è¦ã§ã™ã€‚

### ç›£è¦–ã™ã¹ããƒ¡ãƒˆãƒªã‚¯ã‚¹

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç›®æ¨™å€¤ | ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤ | è¨ˆæ¸¬æ–¹æ³• |
|-----------|--------|------------|---------|
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡** | 30-50% | 20%ä»¥ä¸‹ | `hits / total_queries` |
| **å½é™½æ€§ç‡** | 1-3% | 5%ä»¥ä¸Š | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ / ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¤œè¨¼ |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | 5-50ms | 100msä»¥ä¸Š | p50/p95/p99 |
| **Embeddingè¨ˆç®—æ™‚é–“** | 10-30ms | 50msä»¥ä¸Š | ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã®è¨ˆæ¸¬ |
| **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡** | - | 80%ä»¥ä¸Š | Redis INFO memory |

### Prometheusé€£æºã®å®Ÿè£…ä¾‹

```python
# cache_metrics.py
import time
from prometheus_client import Counter, Histogram, Gauge

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
CACHE_HITS = Counter(
    "semantic_cache_hits_total",
    "Total cache hits",
    ["cache_name", "content_type"],
)
CACHE_MISSES = Counter(
    "semantic_cache_misses_total",
    "Total cache misses",
    ["cache_name"],
)
CACHE_LATENCY = Histogram(
    "semantic_cache_latency_seconds",
    "Cache lookup latency",
    ["operation"],  # "check" or "store"
    buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5],
)
CACHE_HIT_RATE = Gauge(
    "semantic_cache_hit_rate",
    "Rolling cache hit rate",
    ["cache_name"],
)


def cached_llm_call(cache, prompt: str, llm_call, cache_name: str = "default"):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä»˜ãã‚­ãƒ£ãƒƒã‚·ãƒ¥LLMå‘¼ã³å‡ºã—"""
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¤œç´¢ã®è¨ˆæ¸¬
    start = time.perf_counter()
    results = cache.check(prompt=prompt)
    elapsed = time.perf_counter() - start
    CACHE_LATENCY.labels(operation="check").observe(elapsed)

    if results:
        CACHE_HITS.labels(
            cache_name=cache_name,
            content_type=results[0].get("metadata", {}).get("content_type", "unknown"),
        ).inc()
        return results[0]["response"]

    CACHE_MISSES.labels(cache_name=cache_name).inc()

    # LLMå‘¼ã³å‡ºã—
    response = llm_call(prompt)

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã®è¨ˆæ¸¬
    start = time.perf_counter()
    cache.store(prompt=prompt, response=response)
    elapsed = time.perf_counter() - start
    CACHE_LATENCY.labels(operation="store").observe(elapsed)

    return response
```

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãŒ10%ä»¥ä¸‹ | é–¾å€¤ãŒå³ã—ã™ãã‚‹ / ã‚¯ã‚¨ãƒªã®å¤šæ§˜æ€§ãŒé«˜ã„ | é–¾å€¤ã‚’æ®µéšçš„ã«ç·©å’Œï¼ˆ0.05â†’0.10â†’0.15ï¼‰ |
| èª¤ã£ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”ãŒè¿”ã‚‹ | é–¾å€¤ãŒç·©ã™ãã‚‹ / æ±ç”¨Embeddingã®ç²¾åº¦ä¸è¶³ | é–¾å€¤ã‚’å³æ ¼åŒ– + ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨ |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¿œç­”ãŒé…ã„ï¼ˆ100ms+ï¼‰ | Redisãƒ¡ãƒ¢ãƒªä¸è¶³ / ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚µã‚¤ã‚ºè¶…é | Redis Stackã®FLATã‹ã‚‰HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›´ |
| å¤ã„æƒ…å ±ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ | TTLæœªè¨­å®š / ä¸€å¾‹TTL | ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç¨®åˆ¥ã”ã¨ã®TTLæˆ¦ç•¥ã‚’é©ç”¨ |
| ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ç’°å¢ƒã§ãƒ‡ãƒ¼ã‚¿æ¼æ´© | ãƒ†ãƒŠãƒ³ãƒˆãƒ•ã‚£ãƒ«ã‚¿æœªè¨­å®š | `filterable_fields`ã§ãƒ†ãƒŠãƒ³ãƒˆåˆ†é›¢ |
| Embeddingè¨ˆç®—ã®ã‚³ã‚¹ãƒˆãŒé«˜ã„ | é«˜æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆ1536æ¬¡å…ƒï¼‰ã‚’ä½¿ç”¨ | è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆ384æ¬¡å…ƒï¼‰ã¸ã®å¤‰æ›´ã‚’æ¤œè¨ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ã€ã‚¯ã‚¨ãƒªã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦**æ„å‘³ã®é¡ä¼¼åº¦ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’åˆ¤å®š**ã™ã‚‹æ‰‹æ³•ã§ã€è‡ªç„¶è¨€èªã®è¡¨ç¾æºã‚Œã«å¯¾å¿œã§ãã‚‹
- RedisVL `SemanticCache` ã¾ãŸã¯ LangChain `RedisSemanticCache` ã‚’ä½¿ãˆã°ã€**æ•°åè¡Œã®ã‚³ãƒ¼ãƒ‰ã§å°å…¥å¯èƒ½**
- é–¾å€¤ã¯ç”¨é€”åˆ¥ã«è¨­å®šã—ï¼ˆFAQ: 0.10-0.15ã€æ³¨æ–‡è¿½è·¡: 0.02-0.05ï¼‰ã€**å½é™½æ€§ç‡3%ä»¥ä¸‹ã‚’ç¶­æŒ**ã™ã‚‹
- TTLæˆ¦ç•¥ã¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å¤‰åŒ–é »åº¦ã«å¿œã˜ã¦5æ®µéšã§è¨­è¨ˆã—ã€å¤ã„æƒ…å ±ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’é˜²ã
- æœ¬ç•ªç’°å¢ƒã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãƒ»å½é™½æ€§ç‡ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®3æŒ‡æ¨™ã‚’ç¶™ç¶šç›£è¦–ã™ã‚‹

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- ã¾ãšRedis Stackã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§èµ·å‹•ã—ã€RedisVLã®`SemanticCache`ã§åŸºæœ¬å‹•ä½œã‚’ç¢ºèªã™ã‚‹
- è‡ªèº«ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åˆã£ãŸé–¾å€¤ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã™ã‚‹
- AWS MemoryDB / Redis Cloudãªã©ã€æœ¬ç•ªå‘ã‘ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®æ¤œè¨¼ã‚’é–‹å§‹ã™ã‚‹

## å‚è€ƒ

- [Redis: What is semantic caching?](https://redis.io/blog/what-is-semantic-caching/) - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŸºæœ¬æ¦‚å¿µã¨è¨­è¨ˆæŒ‡é‡
- [Redis: Semantic Caching for LLMsï¼ˆRedisVLå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰](https://redis.io/docs/latest/develop/ai/redisvl/user_guide/llmcache/) - RedisVL SemanticCacheã®å®Ÿè£…ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹
- [Redis: LangCacheç´¹ä»‹](https://redis.io/blog/spring-release-2025/) - 2025å¹´ãƒªãƒªãƒ¼ã‚¹ã®ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- [AWS: MemoryDB Semantic Cacheæ¤œè¨¼ï¼ˆDevelopersIOï¼‰](https://dev.classmethod.jp/articles/aws-memorydb-semantic-cache-llm-performance/) - AWSç’°å¢ƒã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
- [AWS: Improve speed and reduce cost with MemoryDB](https://aws.amazon.com/blogs/database/improve-speed-and-reduce-cost-for-generative-ai-workloads-with-a-persistent-semantic-cache-in-amazon-memorydb/) - AWSå…¬å¼ãƒ–ãƒ­ã‚°
- [VentureBeat: Why your LLM bill is exploding](https://venturebeat.com/orchestration/why-your-llm-bill-is-exploding-and-how-semantic-caching-can-cut-it-by-73) - ã‚³ã‚¹ãƒˆ73%å‰Šæ¸›ã®äº‹ä¾‹åˆ†æ
- [InfoQ: Reducing False Positives in RAG Semantic Caching](https://www.infoq.com/articles/reducing-false-positives-retrieval-augmented-generation/) - éŠ€è¡Œæ¥­å‹™ã§ã®å½é™½æ€§å‰Šæ¸›ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£
- [GPTCacheï¼ˆGitHubï¼‰](https://github.com/zilliztech/GPTCache) - LangChain/LlamaIndexçµ±åˆã®OSSã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥
- [LangChain RedisSemanticCache API Reference](https://python.langchain.com/api_reference/redis/cache/langchain_redis.cache.RedisSemanticCache.html) - LangChainå…¬å¼Redisçµ±åˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
