---
title: "Qwen3.5×RTX 3090でバイブコーディング環境を構築する実践ガイド"
emoji: "🖥️"
type: "tech"
topics: ["llm", "qwen", "ollama", "vscode", "gpu"]
published: false
---

# Qwen3.5×RTX 3090でバイブコーディング環境を構築する実践ガイド

## この記事でわかること

- RTX 3090（24GB VRAM）でQwen3.5系モデルを動かすための量子化戦略とモデル選定
- llama.cppとOllamaを使ったローカル推論サーバーの構築手順
- Aider・Continue.dev・Qwen Code CLIを組み合わせたバイブコーディング環境の設定方法
- ローカルLLMとClaude Sonnet 4.5のコーディング性能比較と使い分け戦略
- VRAM使用量・推論速度の実測ベンチマークとチューニング手法

## 対象読者

- **想定読者**: ローカルLLMに興味がある中級者〜上級者のエンジニア
- **必要な前提知識**:
  - Linux（Ubuntu 22.04以降）の基本操作
  - NVIDIAドライバ・CUDAの導入経験
  - ターミナルベースの開発経験（Git、エディタ操作）
  - バイブコーディング（自然言語でコードを生成するワークフロー）の基本概念

## 結論・成果

RTX 3090単体でQwen3.5-35B-A3B（MoEモデル）をQ4量子化で動かすと、**101 tok/s**のトークン生成速度が得られます。これはバイブコーディングに十分な応答速度で、Aiderとの連携で**1つの機能実装を5〜15分**で完了できます。SWE-bench Verifiedスコアでは、Qwen3.5フルモデル（397B）が76.4%を記録しており、Claude Sonnet 4.5の77.2%に迫る水準です。ただし、RTX 3090に収まる35B-A3Bモデルでは性能が下がるため、**「Sonnet 4.5相当」を1枚のGPUで完全に再現するのは現時点では困難**です。実用上は、定型的なコード生成やリファクタリングでは十分な品質が得られ、複雑なアーキテクチャ設計にはAPIモデルとの併用が有効です。

> **関連記事**: Qwen3.5の全体像については「[Qwen3.5徹底解説：397B MoEモデルをvLLMでデプロイする実践ガイド](https://zenn.dev/0h_n0/articles/657d35a2bbf71d)」で詳しく解説しています。

## RTX 3090に載るQwen3.5モデルを選定する

バイブコーディング環境を構築するには、まずRTX 3090の24GB VRAMに収まるモデルを選ぶ必要があります。Qwen3.5ファミリーには複数のモデルサイズがあり、RTX 3090での実用性は大きく異なります。

### Qwen3.5モデルの選択肢を比較する

RTX 3090で動作する候補モデルは以下の3つです。

| モデル | 総パラメータ | アクティブパラメータ | Q4 VRAM（32kコンテキスト） | 生成速度 | 用途 |
|--------|------------|-------------------|--------------------------|---------|------|
| Qwen3.5-35B-A3B | 35B | 3B（MoE） | 約19GB | 約101 tok/s | バイブコーディングに推奨 |
| Qwen3.5-27B | 27B | 27B（Dense） | 約16GB | 約31 tok/s | 高精度だが低速 |
| Qwen3-Coder-Next | 80B | 3B（MoE） | 約46GB（GPU+RAM分散） | 約25-40 tok/s | コーディング特化だがRAM必要 |

**なぜQwen3.5-35B-A3Bを推奨するか:**

- MoEアーキテクチャにより、トークン生成ごとに3Bパラメータのみ活性化するため、**27B Denseモデルの約3倍の速度**で生成できます
- Q4量子化で19GB程度のVRAMに収まり、KVキャッシュ用の余裕が残ります
- HuggingFaceのベンチマークによると、35B-A3Bは前世代のQwen3-235B-A22Bをコーディングタスクで上回る性能を達成しています

**注意点:**
> Qwen3-Coder-Nextは80Bパラメータのコーディング特化モデルですが、Q4量子化でも約46GBのメモリを必要とします。RTX 3090単体では全レイヤーをGPUに載せられないため、CPU/RAMへの部分オフロードが必要になり、推論速度が低下します。64GB以上のシステムRAMがある場合は選択肢に入りますが、速度面では35B-A3Bが有利です。

### コンテキスト長とVRAM使用量の関係を把握する

コンテキスト長を増やすとKVキャッシュがVRAMを消費します。RTX 3090の24GBでは、モデルとKVキャッシュの合計を管理する必要があります。

| コンテキスト長 | 35B-A3B Q4 VRAM | 27B Q4 VRAM | バイブコーディングでの実用性 |
|--------------|-----------------|-------------|------------------------|
| 4K〜32K | 約19GB | 約16GB | 通常のコード生成に十分 |
| 65K | 約20GB | 約20GB | 大規模ファイルの読み取りに対応 |
| 131K | 約22GB | 約24GB | リポジトリ全体の理解が可能 |
| 262K | 約25GB | 約33GB | 3090ではオーバーフロー |

バイブコーディングでは通常32K〜65Kのコンテキストで十分です。リポジトリ全体をコンテキストに入れたい場合は131Kが必要ですが、RTX 3090では35B-A3Bモデルのみ対応可能です。

## ローカル推論環境を構築する

RTX 3090でQwen3.5モデルを動かすための推論バックエンドを構築します。llama.cppとOllamaの2つの方法を紹介し、それぞれの利点を説明します。

### llama.cppでGGUF推論サーバーを立てる

llama.cppはC/C++で実装された軽量な推論エンジンで、GGUFフォーマットの量子化モデルを高速に実行できます。

**ステップ1: llama.cppのビルド**

```bash
# 依存パッケージのインストール
sudo apt-get update && sudo apt-get install -y build-essential cmake curl pciutils

# llama.cppのクローンとCUDAビルド
git clone https://github.com/ggml-org/llama.cpp
cmake llama.cpp -B llama.cpp/build \
  -DGGML_CUDA=ON \
  -DCMAKE_CUDA_ARCHITECTURES=86  # RTX 3090のCompute Capability
cmake --build llama.cpp/build --config Release -j$(nproc) \
  --target llama-cli llama-server
```

**ステップ2: GGUFモデルのダウンロード**

```bash
# Unslothが提供するDynamic GGUF（品質が高い）を使用
pip install huggingface_hub

# Qwen3.5-35B-A3BのQ4_K_M量子化版をダウンロード
huggingface-cli download unsloth/Qwen3.5-35B-A3B-GGUF \
  --local-dir ./models/qwen35-35b \
  --include "*Q4_K_M*"
```

**ステップ3: 推論サーバーの起動**

```bash
# OpenAI互換APIサーバーとして起動
./llama.cpp/build/bin/llama-server \
  --model ./models/qwen35-35b/Qwen3.5-35B-A3B-Q4_K_M.gguf \
  --port 8080 \
  --ctx-size 32768 \
  --n-gpu-layers 99 \
  --temp 0.7 \
  --top-p 0.8 \
  --top-k 20 \
  --repeat-penalty 1.05 \
  --flash-attn
```

各フラグの意味は以下の通りです。

| フラグ | 値 | 説明 |
|--------|-----|------|
| `--ctx-size` | 32768 | コンテキスト長。バイブコーディングでは32Kで十分 |
| `--n-gpu-layers` | 99 | 全レイヤーをGPUにオフロード |
| `--flash-attn` | - | Flash Attention有効化で高速化 |
| `--temp` | 0.7 | Qwen公式推奨の温度パラメータ |
| `--top-p` | 0.8 | Nucleus samplingの確率閾値 |

**動作確認:**

```bash
# curlでAPIエンドポイントをテスト
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3.5-35b-a3b",
    "messages": [
      {"role": "user", "content": "Pythonでフィボナッチ数列を生成する関数を書いてください"}
    ]
  }'
```

### Ollamaで手軽にセットアップする

Ollamaはモデルのダウンロードから推論までを1コマンドで完結できるツールです。セットアップの手軽さを重視する場合はこちらを推奨します。

```bash
# Ollamaのインストール
curl -fsSL https://ollama.com/install.sh | sh

# Qwen3.5-35B-A3Bモデルの取得と起動
ollama pull qwen3.5:35b-a3b-q4_K_M
ollama run qwen3.5:35b-a3b-q4_K_M
```

OllamaはデフォルトでOpenAI互換APIを`http://localhost:11434/v1`で提供します。llama.cppと比較すると設定の柔軟性は劣りますが、**モデルの切り替えやアップデートが容易**です。

**llama.cppとOllamaの使い分け:**

| 観点 | llama.cpp | Ollama |
|------|-----------|--------|
| セットアップ難易度 | 中（ビルド必要） | 低（1コマンド） |
| パラメータ調整 | 細かく制御可能 | 制限あり |
| 推論速度 | やや速い | 同等 |
| モデル管理 | 手動 | 自動（pullで管理） |
| 推奨ユースケース | 本番運用・チューニング | プロトタイピング・検証 |

### よくある起動時の問題を解決する

最初の起動時につまずきやすいポイントと対策をまとめます。

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| `CUDA out of memory` | VRAMオーバーフロー | `--ctx-size`を16384に下げる、または`--n-gpu-layers`を減らしてCPUオフロード |
| 推論が10 tok/s以下 | GPUへのレイヤーオフロード不足 | `nvidia-smi`でVRAM使用率を確認し、`--n-gpu-layers`を増やす |
| モデルファイルが見つからない | パス指定ミス | 絶対パスを使用する |
| Tool callingが動かない | llama.cppのバージョンが古い | 2026年2月以降のビルドに更新する（ツール呼び出しバグ修正済み） |

## バイブコーディングツールと連携する

ローカル推論サーバーが起動したら、バイブコーディングツールと接続して実際のコーディングワークフローを構築します。2026年時点で、ローカルLLMと相性の良いツールは**Aider**、**Continue.dev**、**Qwen Code CLI**の3つです。

### AiderでGit統合バイブコーディングを実現する

Aiderはターミナルベースのオープンソースコーディングアシスタントで、Git統合が組み込まれています。ローカルLLMとの連携が特に優れています。

**インストールと設定:**

```bash
# Aiderのインストール
pip install aider-chat

# ローカルLLMを指定して起動
aider --model openai/qwen3.5-35b-a3b \
      --openai-api-base http://localhost:8080/v1 \
      --openai-api-key not-needed
```

**設定ファイルで永続化（`~/.aider.conf.yml`）:**

```yaml
# ~/.aider.conf.yml
model: openai/qwen3.5-35b-a3b
openai-api-base: http://localhost:8080/v1
openai-api-key: not-needed
auto-commits: true
edit-format: diff
```

**実際のバイブコーディング例:**

```bash
$ aider

# 自然言語で指示するだけ
> FastAPIでユーザー認証付きのTODO APIを作ってください。
> JWTトークンとSQLiteを使ってください。

# Aiderが自動でファイル作成・編集・Gitコミットを実行
# 生成されたコードを確認して、フィードバックを追加
> テストも追加してください。pytestを使ってください。
```

Aiderはファイルの追加・編集を自動でGitコミットするため、各ステップの変更を追跡できます。これはバイブコーディングにおける「やり直し」を容易にします。

**よくある間違い**: 最初はAiderに大きなリポジトリ全体を読み込ませようとしがちですが、32Kコンテキストでは数ファイルが限界です。`/add`コマンドで関連ファイルだけを明示的に追加するのが効果的です。

### Continue.devでVSCodeからローカルLLMを使う

Continue.devはVSCode拡張として動作するオープンソースのAIコーディングアシスタントです。GUIでの操作を好む場合はこちらが適しています。

**VSCodeへのインストール:**

```bash
# VSCode拡張マーケットプレイスからインストール
code --install-extension continue.continue
```

**設定ファイル（`~/.continue/config.json`）:**

```json
{
  "models": [
    {
      "title": "Qwen3.5-35B-A3B (Local)",
      "provider": "openai",
      "model": "qwen3.5-35b-a3b",
      "apiBase": "http://localhost:8080/v1",
      "apiKey": "not-needed",
      "contextLength": 32768
    }
  ],
  "tabAutocompleteModel": {
    "title": "Qwen3.5 Autocomplete",
    "provider": "openai",
    "model": "qwen3.5-35b-a3b",
    "apiBase": "http://localhost:8080/v1",
    "apiKey": "not-needed"
  }
}
```

Continue.devではチャットパネルからの対話的なコード生成に加え、**タブ補完**（インラインのコード提案）もローカルLLMで動作します。Cursorの代替としてプライバシーを確保しながらバイブコーディングが可能です。

### Qwen Code CLIでエージェント型コーディングを試す

Qwen Code CLIは、Qwenチームが公式にリリースしたターミナルベースのコーディングエージェントです。Claude Codeに似た体験をローカルLLMで実現します。

**インストール:**

```bash
# npmでグローバルインストール（Node.js 20以降が必要）
npm install -g @qwen-code/qwen-code@latest

# または Homebrew
brew install qwen-code
```

**ローカルLLMとの接続設定（`~/.qwen/settings.json`）:**

```json
{
  "modelProviders": {
    "openai": [
      {
        "id": "qwen3.5-35b-local",
        "name": "Qwen3.5-35B-A3B (Local)",
        "baseUrl": "http://localhost:8080/v1",
        "envKey": "LOCAL_API_KEY"
      }
    ]
  }
}
```

```bash
# 環境変数を設定（ダミー値でOK）
export LOCAL_API_KEY="not-needed"

# プロジェクトディレクトリで起動
cd your-project/
qwen
```

Qwen Code CLIはSubAgentやSkillsなどのエージェント機能を備えており、複数ファイルにまたがる変更を自律的に実行できます。ただし、これらのエージェント機能はQwen3-Coder以上のモデルで最適化されているため、35B-A3Bでは**単純なコード生成・編集**に限定して使うのが実用的です。

### 3ツールの使い分け

| 用途 | 推奨ツール | 理由 |
|------|-----------|------|
| 新規プロジェクトの骨格生成 | Aider | Git自動コミットで変更追跡が容易 |
| 既存コードの編集・リファクタリング | Continue.dev | VSCode統合でコンテキスト把握が楽 |
| マルチファイル変更・調査 | Qwen Code CLI | エージェント型で自律的に作業 |
| タブ補完・インライン提案 | Continue.dev | リアルタイム補完に対応 |

## ローカルLLMとAPIモデルの性能を比較する

バイブコーディングの品質は、使用するLLMの性能に直結します。RTX 3090で動くローカルモデルとClaude Sonnet 4.5を複数の観点で比較します。

### ベンチマークで定量比較する

主要なコーディングベンチマークでの各モデルの性能を示します。

| ベンチマーク | Qwen3.5-35B-A3B | Qwen3.5-397B（フル） | Claude Sonnet 4.5 | 備考 |
|------------|-----------------|---------------------|-------------------|------|
| SWE-bench Verified | 非公開（推定50-60%） | 76.4% | 77.2% | 実際のGitHub Issue修正タスク |
| HumanEval | 公式データなし | 高水準 | 高水準 | 基本的なコード生成 |
| 推論速度 | 101 tok/s（ローカル） | API依存 | API依存 | RTX 3090 Q4量子化時 |
| コスト | 電気代のみ（約$0.05/h） | API課金 | API課金 | 長期利用で有利 |

SWE-bench Verifiedのスコアでは、Qwen3.5フルモデル（397B）がClaude Sonnet 4.5と1ポイント差の76.4%を記録しています。ただし、RTX 3090で動く35B-A3Bモデルはフルモデルより性能が低く、**複雑なバグ修正やアーキテクチャレベルの設計では差が出ます**。

### タスク別の実用性を評価する

実際のバイブコーディングでは、タスクの種類によってローカルモデルの有用性が大きく変わります。

**ローカルLLM（35B-A3B）が得意なタスク:**

- 定型的なCRUD API生成
- テストコードの自動生成
- コードの言語間変換
- ドキュメント・コメントの生成
- リファクタリング（変数名変更、関数分割）

**APIモデル（Claude Sonnet 4.5等）を使うべきタスク:**

- 複雑なアーキテクチャ設計
- 大規模なリポジトリ全体を理解した上での修正
- 高度なデバッグ（複数ファイルにまたがるバグ）
- セキュリティレビュー

**トレードオフ**: ローカルLLMはプライバシーとコストで有利ですが、コンテキスト長と推論品質ではAPIモデルに劣ります。実務では**ローカルで日常的なコーディングを行い、複雑なタスクにはAPIモデルを使う**ハイブリッド戦略が有効です。

### コスト比較で長期運用を検討する

| 項目 | ローカル（RTX 3090） | Claude Sonnet 4.5 API |
|------|---------------------|----------------------|
| 初期費用 | 約$800（中古3090） | $0 |
| 月額運用費（8h/日利用） | 電気代約$12/月 | 約$50-200/月（使用量依存） |
| 1年間の総コスト | 約$944 | 約$600-2,400 |
| プライバシー | コード外部送信なし | APIにコード送信 |
| 応答速度 | 安定（ネットワーク不要） | ネットワーク遅延あり |

月間のAPI利用量が多い開発者（$100/月以上）であれば、約8ヶ月でRTX 3090の初期投資を回収できます。一方、APIの利用頻度が低い場合は、クラウドAPIのほうがコスト効率が高くなります。

## 推論パフォーマンスをチューニングする

RTX 3090での推論速度を最適化するためのチューニング手法を紹介します。

### 量子化レベルの選び方

量子化レベルはモデルの品質と速度のトレードオフを決定します。

| 量子化 | モデルサイズ | VRAM使用量 | 生成速度 | 品質への影響 |
|--------|-----------|-----------|---------|------------|
| Q2_K | 約12GB | 約14GB | 最速 | 顕著な品質低下 |
| Q4_K_M | 約18GB | 約19GB | 速い | 軽微な品質低下（推奨） |
| Q5_K_M | 約22GB | 約23GB | 普通 | ほぼ無視できる |
| Q6_K | 約26GB | VRAM不足 | - | - |

**Q4_K_Mを推奨する理由**: Unslothが提供するDynamic GGUF（UD）フォーマットは、重要なレイヤーに高ビット量子化を割り当てるため、標準的なQ4量子化よりも品質劣化が少ないと報告されています。

### KVキャッシュの最適化

長いコンテキストを扱う際のVRAM消費を抑える設定です。

```bash
# KVキャッシュの量子化を有効にする
./llama.cpp/build/bin/llama-server \
  --model ./models/qwen35-35b/Qwen3.5-35B-A3B-Q4_K_M.gguf \
  --port 8080 \
  --ctx-size 65536 \
  --n-gpu-layers 99 \
  --cache-type-k q8_0 \
  --cache-type-v q8_0 \
  --flash-attn
```

`--cache-type-k q8_0`と`--cache-type-v q8_0`により、KVキャッシュのメモリ使用量を半減できます。65Kコンテキストでも24GBのVRAM内に収まるようになります。

### nvidia-smiで使用状況をモニタリングする

推論中のGPU使用状況をリアルタイムで確認することで、ボトルネックを特定できます。

```bash
# 1秒間隔でGPU使用状況を監視
watch -n 1 nvidia-smi

# または、より詳細な情報を取得
nvidia-smi --query-gpu=utilization.gpu,utilization.memory,memory.used,memory.total,temperature.gpu \
  --format=csv -l 1
```

**チューニングの目安:**

- **GPU使用率が50%以下**: `--n-gpu-layers`を増やしてGPUオフロードを強化
- **VRAMが上限に近い**: `--ctx-size`を下げるか、KVキャッシュ量子化を有効化
- **温度が85℃以上**: ファンカーブを調整するか、ケースのエアフローを改善

**ハマりポイント**: RTX 3090はTDP 350Wの高発熱GPUです。長時間の推論ではサーマルスロットリングで速度が低下する場合があります。ケース内のエアフローが不十分な環境では、`--n-gpu-layers`を減らしてCPUに一部を分散させることで温度を下げられます。

## 実践：バイブコーディングでWebアプリを構築する

実際のバイブコーディングワークフローを、FastAPI + React のTODOアプリ構築を例に紹介します。

### プロジェクトの初期化からAPI生成まで

```bash
# プロジェクトディレクトリの作成
mkdir todo-app && cd todo-app
git init

# Aiderでバイブコーディング開始
aider --model openai/qwen3.5-35b-a3b \
      --openai-api-base http://localhost:8080/v1 \
      --openai-api-key not-needed
```

Aiderに以下のように指示します。

```
> FastAPIでTODO管理APIを作ってください。
> 要件:
> - SQLiteデータベースを使用
> - CRUD操作（作成・読取・更新・削除）
> - Pydanticモデルでバリデーション
> - uvicornで起動可能
```

Aiderは`main.py`、`models.py`、`database.py`などのファイルを自動生成し、各ステップをGitコミットします。生成されたコードに問題があれば、続けて修正を指示できます。

```
> テストを追加してください。pytestとhttpclientを使ってください。
> 認証機能も追加してください。JWT + bcryptを使ってください。
```

### 生成コードの品質チェックを組み込む

バイブコーディングで生成されたコードは、必ず検証ステップを挟むことが重要です。

```bash
# 型チェック
pip install mypy
mypy main.py --ignore-missing-imports

# リンター
pip install ruff
ruff check .

# テスト実行
pip install pytest httpx
pytest -v
```

**制約条件**: ローカルLLM（35B-A3B）で生成されたコードは、APIモデルと比較して型ヒントの精度や例外処理の網羅性が低い傾向があります。`mypy`と`ruff`を組み合わせることで、生成コードの品質を補完できます。

## まとめと次のステップ

**まとめ:**

- RTX 3090でQwen3.5-35B-A3B（MoE、Q4量子化）を動かすと、**101 tok/sの生成速度**でバイブコーディングが実用的に行えます
- Aider・Continue.dev・Qwen Code CLIの3ツールを使い分けることで、ターミナル・VSCode・エージェント型の多様なワークフローに対応できます
- SWE-benchスコアではQwen3.5フルモデルがClaude Sonnet 4.5に迫る水準（76.4% vs 77.2%）ですが、RTX 3090に収まる35B-A3Bモデルでは性能差が広がります
- 定型的なコード生成にはローカルLLMで十分な品質が得られ、複雑なタスクにはAPIモデルとのハイブリッド運用が有効です
- 月間API費用が$100を超える開発者は、約8ヶ月でRTX 3090の投資回収が見込めます

**次にやるべきこと:**

- Ollama + Qwen3.5-35B-A3Bをインストールして、30分で動作確認する
- Aiderのconfig設定を行い、実際のプロジェクトでバイブコーディングを試す
- `nvidia-smi`で推論中のVRAM使用量をモニタリングし、`--ctx-size`を調整する

## 参考

- [Unsloth: Qwen3-Coder-Next How to Run Locally](https://unsloth.ai/docs/models/qwen3-coder-next)
- [Hardware Corner: Qwen3.5 27B vs 35B-A3B Hardware Requirements](https://www.hardware-corner.net/qwen35-27b-35b-hardware-requirements/)
- [DEV Community: Qwen3-Coder-Next Complete 2026 Guide](https://dev.to/sienna/qwen3-coder-next-the-complete-2026-guide-to-running-powerful-ai-coding-agents-locally-1k95)
- [HuggingFace: Qwen3.5-35B-A3B](https://huggingface.co/Qwen/Qwen3.5-35B-A3B)
- [Composio: Qwen3 Coder vs Claude Sonnet Coding Comparison](https://composio.dev/blog/qwen-3-coder-vs-kimi-k2-vs-claude-4-sonnet-coding-comparison)
- [GitHub: QwenLM/qwen-code](https://github.com/QwenLM/qwen-code)
- [16x Engineer: Qwen3 Coder Evaluation Results](https://eval.16x.engineer/blog/qwen3-coder-evaluation-results)

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
