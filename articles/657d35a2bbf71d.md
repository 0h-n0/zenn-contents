---
title: "Qwen3.5徹底解説：397B MoEモデルをvLLMでデプロイする実践ガイド"
emoji: "🐪"
type: "tech"
topics: ["qwen", "llm", "vllm", "moe", "multimodal"]
published: false
---

# Qwen3.5徹底解説：397B MoEモデルをvLLMでデプロイする実践ガイド

## この記事でわかること

- Qwen3.5-397B-A17Bのアーキテクチャと従来モデルからの進化ポイント
- vLLM・SGLangを使った本番デプロイの具体的な手順
- Thinking Mode（思考モード）の制御方法と使い分け
- ベンチマーク（自社報告）から見るGPT-5.2・Claude Opus 4.5との比較
- Visual Agenticによるデスクトップ・モバイルアプリ自律操作の仕組み

## 対象読者

- **想定読者**: 中級〜上級のLLMエンジニア・MLOpsエンジニア
- **必要な前提知識**:
  - Python 3.10以上の実務経験
  - LLM推論サーバー（vLLM/SGLang）の基本概念
  - Mixture of Experts（MoE）アーキテクチャの基礎理解
  - GPU環境（H100/A100）の運用経験があると望ましい

## 結論・成果

Qwen3.5-397B-A17Bは**397B中17BのみアクティベートするスパースMoE設計**で、前世代比**コスト約60%削減**・**256Kで19倍高速化**を実現（公称値）。Apache 2.0のオープンウェイトモデルとして、自社ベンチマークではGPT-5.2やClaude Opus 4.5に匹敵するスコアを記録しています（独立検証は進行中）。

## Qwen3.5のアーキテクチャを理解する

2026年2月16日にAlibaba Cloudが公開したQwen3.5は、**テキスト・画像・動画・音声を早期融合で学習した統合アーキテクチャ**を持つ新世代モデルです。

### Gated Delta Networks：線形アテンションの革新

Qwen3.5はFull AttentionとGated Delta Networksの**ハイブリッドアテンション機構**を採用。60層のうち4層に1層がFull Attention、残りがGated DeltaNetです。

```
アーキテクチャ構成（60層）:
15 × (3 × Gated DeltaNet-MoE + 1 × Gated Attention-MoE)

Gated DeltaNet:
├── Linear Attention Heads: 64 (V), 16 (QK)
├── Head Dimension: 128
└── 特徴: シーケンス長に対して線形計算量

Gated Attention:
├── Attention Heads: 32 (Q), 2 (KV)
├── Head Dimension: 256
└── 特徴: 従来のFull Attention（高精度）
```

Gated DeltaNetはシーケンス長に対して**線形計算量**で、256Kコンテキストで**19倍の高速化**を達成しました。4層に1層のFull Attentionで精度も維持します。

> **注意**: Gated DeltaNetは既存のFlashAttention最適化がそのまま適用できません。vLLM nightly版（v0.16+）またはSGLangのmainブランチが必要です（2026年2月時点）。

### MoEの設計：512エキスパートから11を選択

```
Mixture of Experts:
├── 総エキスパート数: 512
├── アクティブ: 10（ルーティング） + 1（共有）
├── エキスパート中間次元: 1024
└── 総パラメータ: 397B → アクティブ: 17B
```

入力トークンごとに512個から10個をルーティング、1個の共有エキスパートが常時稼働します。**約4.3%の計算コスト**で397B相当の知識にアクセスできます。

## vLLMで本番デプロイする

実際に8xH100環境でのデプロイ手順を見ていきましょう。

### 基本デプロイ

```bash
# vLLMのインストール（Qwen3.5対応にはnightly版が必要、2026年2月時点）
uv pip install vllm --torch-backend=auto \
  --extra-index-url https://wheels.vllm.ai/nightly

# 基本起動（262Kコンテキスト）
vllm serve Qwen/Qwen3.5-397B-A17B \
  --port 8000 \
  --tensor-parallel-size 8 \
  --max-model-len 262144 \
  --reasoning-parser qwen3
```

エージェント用途ではツール呼び出しオプションを追加し、MTP（Multi-Token Prediction）でスループットを向上できます。

```bash
# ツール呼び出し + MTP高速化を同時に有効化
vllm serve Qwen/Qwen3.5-397B-A17B \
  --port 8000 \
  --tensor-parallel-size 8 \
  --max-model-len 262144 \
  --reasoning-parser qwen3 \
  --enable-auto-tool-choice \
  --tool-call-parser qwen3_coder \
  --speculative-config \
    '{"method":"qwen3_next_mtp","num_speculative_tokens":2}'
```

**ハマりポイント:** `--reasoning-parser`には`qwen3`を指定します（[Model Card](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)推奨）。[vLLM公式レシピ](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3.5.html)では`deepseek_r1`の例もあり両方動作しますが、`qwen3`が推奨です。

### Pythonクライアントからの利用

OpenAI互換APIで、既存コードをほぼ変更なく利用可能です。

```python
from openai import OpenAI

client = OpenAI(api_key="EMPTY", base_url="http://localhost:8000/v1")

# Thinking Mode ON（デフォルト）: 複雑な推論向け
response = client.chat.completions.create(
    model="Qwen/Qwen3.5-397B-A17B",
    messages=[{"role": "user", "content": "二分探索の計算量を証明してください"}],
    max_tokens=81920,
    temperature=0.6,
    top_p=0.95,
    extra_body={"top_k": 20},
)

# Thinking Mode OFF: 高速レスポンス向け（チャットボット等）
response = client.chat.completions.create(
    model="Qwen/Qwen3.5-397B-A17B",
    messages=[{"role": "user", "content": "東京の天気を教えて"}],
    max_tokens=32768,
    temperature=0.7,
    top_p=0.8,
    presence_penalty=1.5,
    extra_body={"top_k": 20, "chat_template_kwargs": {"enable_thinking": False}},
)
```

> **トレードオフ**: Thinking Mode OFFで応答速度は向上しますが、数学・コーディングの精度が低下します。用途に応じて切り替えましょう。

## ベンチマークで見る実力

公開ベンチマークを確認しましょう（**自社報告、独立検証未完了**）。

| カテゴリ | ベンチマーク | スコア |
|---|---|---|
| 数学 | AIME26 | 91.3 |
| 数学 | HMMT Feb 25 | 94.8 |
| コーディング | LiveCodeBench v6 | 83.6 |
| コーディング | SWE-bench Verified | 76.4 |
| 知識 | MMLU-Pro | 87.8 |
| マルチモーダル | MMMU | 85.0 |
| ドキュメント | OmniDocBench1.5 | 90.8 |
| 動画理解 | VideoMME(w sub.) | 87.5 |

**よくある誤解:** 「397Bモデルだから推論が遅い」と思われがちですが、MoE設計により実質17Bの計算量です。公称値では8xH100で**約45 tokens/sec**とされています（実環境では設定により変動します）。

## Visual Agenticで自律操作を実現する

**Visual Agentic**はスクリーンショットからUIの要素を検出し、自律的にアプリを操作する機能です。モバイルではアプリ間横断タスク、デスクトップではオフィス業務の自動化が可能です。

### Qwen-Agentでのエージェント構築

```python
import os
from qwen_agent.agents import Assistant

llm_cfg = {
    "model": "Qwen3.5-397B-A17B",
    "model_type": "qwenvl_oai",
    "model_server": "https://dashscope.aliyuncs.com/compatible-mode/v1",
    "api_key": os.getenv("DASHSCOPE_API_KEY"),
    "generate_cfg": {"use_raw_api": True, "extra_body": {"enable_thinking": True}},
}

# MCPサーバー経由でファイルシステムにアクセス
tools = [{"mcpServers": {"filesystem": {
    "command": "npx",
    "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp/workspace"]
}}}]

bot = Assistant(llm=llm_cfg, function_list=tools)
messages = [{"role": "user", "content": "ワークスペースのCSVファイルをサマリーしてください"}]
for responses in bot.run(messages=messages):
    pass
```

**制約条件:** フル機能はQwen3.5-Plus（ホステッド版）のみ。オープンウェイト版ではUI分析は可能ですがリアルタイム操作にはAPI連携が必要です。

## よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|---|---|---|
| OOMエラー | 262Kコンテキスト確保 | `--max-model-len 131072`に削減 |
| 思考出力が長い | Thinking Modeデフォルト | `enable_thinking: False`で無効化 |
| ロード失敗 | 古いvLLM | vLLM nightlyまたは最新安定版（v0.16+）にアップグレード |
| YaRN品質低下 | factor不適切 | コンテキスト長に合わせて調整 |

## まとめと次のステップ

**まとめ:**
- **17Bアクティブ/397B総パラメータ**の効率的MoEモデル（Apache 2.0）
- Gated Delta Networksで**256Kコンテキスト19倍高速化**
- **ネイティブマルチモーダル**で画像・動画・テキスト統合処理
- **Visual Agentic**でアプリ自律操作に対応

**次にやるべきこと:**
- [Qwen Chat](https://chat.qwen.ai)でまず性能を体感
- vLLMデプロイを試し、自社ベンチマークを取得
- [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)でプロトタイプ構築

## 参考

- [Qwen3.5 GitHub Repository](https://github.com/QwenLM/Qwen3.5)
- [Qwen3.5-397B-A17B Hugging Face Model Card](https://huggingface.co/Qwen/Qwen3.5-397B-A17B)
- [vLLM Qwen3.5 Usage Guide](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3.5.html)
- [Qwen3.5 Benchmarks & Complete Guide](https://www.digitalapplied.com/blog/qwen-3-5-agentic-ai-benchmarks-guide)
- [GIGAZINE - GPT-5.2と同等クラスの中華AI「Qwen3.5-397B-A17B」](https://gigazine.net/news/20260217-qwen3-5-397b-a17b/)

詳細なリサーチ内容は [Issue #143](https://github.com/0h-n0/zen-auto-create-article/issues/143) を参照してください。

## 関連する深掘り記事

この記事で取り上げた技術の1次情報を深掘りした記事です。

1. **[Gated Delta Networks — 線形時間シーケンスモデリングの統合理論とQwen3.5への実装](https://0h-n0.github.io/blog/2026/02/18/paper-gated-deltanet-2412-06464.html)** (arXiv 2412.06464, ICLR 2025)
   - Qwen3.5のメインアテンション層（60層中45層）の基盤論文
2. **[Gated Attention — Post-SDPAシグモイドゲートの理論とQwen3.5 Full Attention層への応用](https://0h-n0.github.io/blog/2026/02/18/paper-gated-attention-2505-06708.html)** (arXiv 2505.06708, NeurIPS 2025 Best Paper)
   - Qwen3.5のFull Attention層（60層中15層）に採用されたゲーティング機構
3. **[Qwen3 Technical Report — 4段階学習パイプラインとMoEアーキテクチャの全体像](https://0h-n0.github.io/blog/2026/02/18/paper-qwen3-2504-05737.html)** (arXiv 2504.05737)
   - Qwen3.5の直接的前身。MoEアーキテクチャと4段階ポストトレーニングの詳細
4. **[NVIDIA Dynamo — MoE推論のための分散サービングフレームワーク](https://0h-n0.github.io/blog/2026/02/18/techblog-nvidia-dynamo.html)** (NVIDIA Developer Blog)
   - MoEモデルのPrefill/Decode分離サービング。vLLM/SGLangと統合可能
5. **[Transformers are SSMs — Mamba-2とStructured State Space Dualityの理論](https://0h-n0.github.io/blog/2026/02/18/paper-mamba2-ssd-2406-06484.html)** (arXiv 2406.06484)
   - SSMとAttentionの等価性を証明。Qwen3.5 Gated DeltaNetの理論基盤

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
