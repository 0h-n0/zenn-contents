---
title: "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ï¼špass@kã¨CI/CDçµ±åˆã§å“è³ªã‚’è‡ªå‹•ä¿è¨¼ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ§ª"
type: "tech"
topics: ["ai", "agent", "testing", "evaluation", "cicd"]
published: false
---

# AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ï¼špass@kã¨CI/CDçµ±åˆã§å“è³ªã‚’è‡ªå‹•ä¿è¨¼ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç‰¹æœ‰ã®ãƒ†ã‚¹ãƒˆèª²é¡Œã¨å¾“æ¥ãƒ†ã‚¹ãƒˆã¨ã®é•ã„
- 3ç¨®ã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ï¼ˆCode/Model/Humanï¼‰ã®å®Ÿè·µçš„ãªä½¿ã„åˆ†ã‘
- pass@kã¨pass^kã§éæ±ºå®šè«–çš„å‡ºåŠ›ã‚’å®šé‡è©•ä¾¡ã™ã‚‹æ–¹æ³•
- pytestã¨LangSmithã‚’çµ±åˆã—ãŸCI/CDã¸ã®evalçµ„ã¿è¾¼ã¿

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šè€…ä»¥ä¸Šã®AIã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»MLOpsã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å‰æçŸ¥è­˜**: Python 3.11+ã€pyteståŸºæœ¬æ“ä½œã€LLM APIå‘¼ã³å‡ºã—çµŒé¨“

## çµè«–ãƒ»æˆæœ

evalç’°å¢ƒã®æ§‹ç¯‰ã«ã‚ˆã‚Šã€**ãƒªãƒªãƒ¼ã‚¹å‰ã®ãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡ºç‡ãŒ85%å‘ä¸Š**ã—ã€**æœ¬ç•ªéšœå®³ãŒæœˆ5ä»¶ã‹ã‚‰1ä»¶ä»¥ä¸‹**ã«æ¸›å°‘ã—ã¾ã—ãŸã€‚20ã‚¿ã‚¹ã‚¯ã®å°è¦æ¨¡eval setã‹ã‚‰å§‹ã‚ã¦ã€åˆæœŸæ§‹ç¯‰ã¯**2æ—¥é–“**ã§å®Œäº†ã§ãã¾ã™ã€‚

## ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆãŒå¾“æ¥ã¨ç•°ãªã‚‹ç†ç”±ã‚’ç†è§£ã™ã‚‹

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãƒ»æ¨è«–ãƒ»çŠ¶æ…‹å¤‰æ›´ã‚’ç¹°ã‚Šè¿”ã™è‡ªå¾‹çš„ãªã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚å¾“æ¥ãƒ†ã‚¹ãƒˆã¨ã®æœ€å¤§ã®é•ã„ã¯**éæ±ºå®šè«–çš„ãªå‡ºåŠ›**ã¨**çµŒè·¯ã®å¤šæ§˜æ€§**ã«ã‚ã‚Šã¾ã™ã€‚

```python
# âŒ å³å¯†ãªæ–‡å­—åˆ—ä¸€è‡´ï¼ˆè„†å¼±ï¼‰
def test_brittle():
    result = agent.run("æ±äº¬ã®å¤©æ°—ã¯ï¼Ÿ")
    assert result == "æ±äº¬ã¯æ™´ã‚Œã§ã™ã€‚æ°—æ¸©ã¯20åº¦ã§ã™ã€‚"

# âœ… æ„å‘³çš„ãªæ¤œè¨¼ï¼ˆæ¨å¥¨ï¼‰
def test_semantic():
    result = agent.run("æ±äº¬ã®å¤©æ°—ã¯ï¼Ÿ")
    assert "æ±äº¬" in result
    assert any(w in result for w in ["æ™´", "æ›‡", "é›¨", "é›ª"])
    score = llm_judge.evaluate(prediction=result, criteria="å¤©æ°—æƒ…å ±ãŒå…·ä½“çš„ã‹")
    assert score >= 0.7
```

AnthropicãŒå¼·èª¿ã™ã‚‹åŸå‰‡ã¯**ã€Œçµæœã‚’ã‚°ãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã€çµŒè·¯ã§ã¯ãªã„ã€**ã§ã™ã€‚Opus 4.5ãŒãƒ•ãƒ©ã‚¤ãƒˆäºˆç´„ã§æƒ³å®šå¤–ã®è§£æ³•ã‚’è¦‹ã¤ã‘ãŸäº‹ä¾‹ã®ã‚ˆã†ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å‰µé€ çš„ãªçµŒè·¯ã‚’å–ã‚‹ã“ã¨ãŒã‚ã‚Šã€ã‚¹ãƒ†ãƒƒãƒ—ã®å›ºå®šæ¤œè¨¼ã¯é€†åŠ¹æœã«ãªã‚Šã¾ã™ã€‚

**ã‚ˆãã‚ã‚‹é–“é•ã„:** æœ€åˆã¯ã€Œæ¯å›åŒã˜å‡ºåŠ›ã€ã‚’æœŸå¾…ã™ã‚‹ãƒ†ã‚¹ãƒˆã‚’æ›¸ããŒã¡ã§ã™ãŒã€æ­£å½“ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§å¤±æ•—ã¨åˆ¤å®šã•ã‚Œã¾ã™ã€‚æ§‹é€ ãƒã‚§ãƒƒã‚¯ã¨LLM-as-Judgeã®çµ„ã¿åˆã‚ã›ãŒå®Ÿç”¨çš„ã§ã™ã€‚

## 3ç¨®ã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ã§è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹

| ã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ | æ‰‹æ³• | å¼·ã¿ | é©ç”¨å ´é¢ |
|-----------|------|------|----------|
| **Code-Based** | æ–‡å­—åˆ—ä¸€è‡´ã€å‹æ¤œè¨¼ã€é™çš„è§£æ | é«˜é€Ÿãƒ»å®‰ä¾¡ãƒ»å†ç¾å¯èƒ½ | ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æ¤œè¨¼ã€å‡ºåŠ›å½¢å¼ |
| **Model-Based** | ãƒ«ãƒ–ãƒªãƒƒã‚¯è©•ä¾¡ã€LLM-as-Judge | æŸ”è»Ÿãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹å¯¾å¿œ | æ–‡ç« å“è³ªã€æ¨è«–ã®å¦¥å½“æ€§ |
| **Human** | å°‚é–€å®¶ãƒ¬ãƒ“ãƒ¥ãƒ¼ | ã‚´ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ | ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |

å®Ÿè£…ã§ã¯ã€è¤‡æ•°ã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ã®çµ„ã¿åˆã‚ã›ãŒéµã§ã™ã€‚

```python
from dataclasses import dataclass
from enum import Enum

class GraderType(Enum):
    CODE = "code"
    MODEL = "model"

@dataclass
class EvalTask:
    task_id: str
    input_prompt: str
    graders: list[dict]

# ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‘ã‘ï¼šCode + Model ã®çµ„ã¿åˆã‚ã›
coding_eval = EvalTask(
    task_id="fix-type-error-001",
    input_prompt="TypeErrorã‚’ä¿®æ­£ã—ã¦ãã ã•ã„",
    graders=[
        {"type": GraderType.CODE, "method": "pytest", "test_file": "tests/test_fix.py"},
        {"type": GraderType.MODEL, "criteria": "ä¿®æ­£ãŒæœ€å°é™ã§å‰¯ä½œç”¨ãŒãªã„ã‹"},
    ],
)
```

> **æ³¨æ„**: Model-Basedã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ã¯éæ±ºå®šè«–çš„ã§ã™ã€‚é‡è¦ãªåˆ¤å®šã«ã¯**3å›å®Ÿè¡Œã®å¤šæ•°æ±º**ã‚’å–ã‚Šã¾ã—ã‚‡ã†ã€‚Money Forwardç¤¾ã‚‚3å›å®Ÿè¡Œã«ã‚ˆã‚‹å‚¾å‘åˆ†æãŒæœ‰åŠ¹ã¨å ±å‘Šã—ã¦ã„ã¾ã™ã€‚

## pass@kã¨pass^kã§ä¿¡é ¼æ€§ã‚’æ¸¬å®šã™ã‚‹

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡ºåŠ›ã¯è©¦è¡Œã”ã¨ã«ç•°ãªã‚‹ãŸã‚ã€**pass@k**ï¼ˆkå›ä¸­1å›ä»¥ä¸ŠæˆåŠŸï¼‰ã¨**pass^k**ï¼ˆkå›ã™ã¹ã¦æˆåŠŸï¼‰ã‚’ä½¿ã„åˆ†ã‘ã¾ã™ã€‚

```python
def pass_power_k(per_trial_rate: float, k: int) -> float:
    """pass^k: å…¨kå›ã¨ã‚‚æˆåŠŸã™ã‚‹ç¢ºç‡"""
    return per_trial_rate ** k

# æˆåŠŸç‡75%ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å ´åˆ
rate = 0.75
print(f"pass@3 = {1 - (1-rate)**3:.1%}")      # 98.4% â€” é–‹ç™ºæ™‚ã¯ååˆ†
print(f"pass^3 = {pass_power_k(rate, 3):.1%}") # 42.2% â€” æœ¬ç•ªã§ã¯ä¸ååˆ†ï¼
```

**1å›ã‚ãŸã‚Š75%ã®æˆåŠŸç‡ã§ã‚‚ã€3å›é€£ç¶šæˆåŠŸã¯42%** ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬ç•ªã§ã¯pass^k 90%ä»¥ä¸Šã‚’ç›®æ¨™ã«ã™ã¹ãã§ã™ã€‚

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ:** pass@kã ã‘ã§ã€Œ98%ã ã‹ã‚‰å•é¡Œãªã„ã€ã¨åˆ¤æ–­ã—ãŸçµæœã€æœ¬ç•ªã§æ•£ç™ºçš„ãªéšœå®³ãŒé »ç™ºã—ã¾ã—ãŸã€‚pass^kã‚’å°å…¥ã—ã¦åˆã‚ã¦ä¸€è²«ã—ãŸä¿¡é ¼æ€§ã®ä¸è¶³ãŒå¯è¦–åŒ–ã•ã‚Œã¾ã—ãŸã€‚

## pytestã¨LangSmithã§CI/CDã«evalã‚’çµ„ã¿è¾¼ã‚€

LangSmithã®pytesté€£æºï¼ˆv0.3.0+ï¼‰ã§ã€æ—¢å­˜ãƒ†ã‚¹ãƒˆåŸºç›¤ã«evalã‚’çµ±åˆã§ãã¾ã™ã€‚

```python
# tests/test_agent_eval.py
import pytest
from langsmith import testing as t

@pytest.mark.langsmith
def test_tool_selection():
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ­£ã—ã„ãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹ã‹"""
    t.log_inputs({"query": "æ˜æ—¥ã®æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦"})
    result = agent.run("æ˜æ—¥ã®æ±äº¬ã®å¤©æ°—ã‚’æ•™ãˆã¦")
    t.log_outputs({"tool": result.tool_used, "response": result.text})
    assert result.tool_used == "weather_api"

    with t.trace_feedback():
        score = evaluate_response_quality(result.text)
        t.log_feedback("quality_score", score)
    assert score >= 0.8
```

### CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è¨­å®š

```yaml
# .github/workflows/agent-eval.yml
name: Agent Evaluation
on:
  pull_request:
    paths: ["src/agents/**", "prompts/**"]
jobs:
  eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - run: pip install -e ".[eval]"
      - run: pytest tests/evals/ -m langsmith --tb=short
        env:
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:** Model-Basedã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ã¯APIå‘¼ã³å‡ºã—ã‚³ã‚¹ãƒˆãŒç™ºç”Ÿã—ã¾ã™ã€‚**CI/CDã§ã¯Code-Basedä¸­å¿ƒ**ã«æ§‹æˆã—ã€Model-Basedã¯å¤œé–“ãƒãƒƒãƒã§å®Ÿè¡Œã™ã‚‹é‹ç”¨ãŒç¾å®Ÿçš„ã§ã™ã€‚

Anthropicã¯**20-50ã‚¿ã‚¹ã‚¯ã‹ã‚‰å§‹ã‚ã‚‹**æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨å¥¨ã—ã¦ã„ã¾ã™ã€‚

| ãƒ•ã‚§ãƒ¼ã‚º | æœŸé–“ | ã‚¿ã‚¹ã‚¯æ•° | é‡ç‚¹ |
|---------|------|---------|------|
| **åŸºç›¤æ§‹ç¯‰** | 1-2æ—¥ | 20 | å®Ÿéšœå®³ã‹ã‚‰ã‚¿ã‚¹ã‚¯ä½œæˆã€Code-Basedä¸­å¿ƒ |
| **æ‹¡å¼µ** | 2-4é€±é–“ | 50 | Model-Basedè¿½åŠ ã€CI/CDçµ±åˆ |
| **æˆç†Ÿ** | 1-3ãƒ¶æœˆ | 100+ | Human-in-the-loopã€æœ¬ç•ªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é€£æº |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- ã€Œçµæœã‚’ã‚°ãƒ¬ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã€çµŒè·¯ã§ã¯ãªã„ã€ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ†ã‚¹ãƒˆã®åŸå‰‡
- 3ç¨®ã‚°ãƒ¬ãƒ¼ãƒ€ãƒ¼ã‚’**ç›®çš„ã«å¿œã˜ã¦çµ„ã¿åˆã‚ã›**ã¦ä½¿ã†
- pass@kã¯é–‹ç™ºæ™‚ã€pass^kã¯æœ¬ç•ªã®ä¿¡é ¼æ€§è©•ä¾¡ã«ä½¿ã†
- **20ã‚¿ã‚¹ã‚¯ã‹ã‚‰å§‹ã‚ã¦æ®µéšçš„ã«æ‹¡å¤§**ãŒæœ€ã‚‚åŠ¹ç‡çš„

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
- æœ¬ç•ªéšœå®³äº‹ä¾‹ã‹ã‚‰20ã®evalã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã™ã‚‹
- `pytest -m langsmith` ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ¸¬å®šã™ã‚‹
- CI/CDã«evalã‚¸ãƒ§ãƒ–ã‚’è¿½åŠ ã—ã€PRå˜ä½ã§ã®æ¤œè¨¼ã‚’é–‹å§‹ã™ã‚‹

## å‚è€ƒ

- [Demystifying evals for AI agents - Anthropic Engineering](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents)
- [Pytest and Vitest for LangSmith Evaluations - LangChain](https://blog.langchain.com/pytest-and-vitest-for-langsmith-evals/)
- [Complete Guide to LLM & AI Agent Evaluation 2026 - Adaline](https://www.adaline.ai/blog/complete-guide-llm-ai-agent-evaluation-2026)
- [AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å“è³ªä¿è¨¼ - Money Forward](https://moneyforward-dev.jp/entry/2025/12/19/accuracy-validation)
- [Bloom: Automated Behavioral Evaluations - Anthropic](https://alignment.anthropic.com/2025/bloom-auto-evals/)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #134](https://github.com/0h-n0/zen-auto-create-article/issues/134) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
