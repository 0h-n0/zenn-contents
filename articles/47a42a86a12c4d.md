---
title: "LLMã‚¨ãƒ©ãƒ¼ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆå®Ÿè£…ï¼šOpenTelemetryã§éšœå®³æ¤œçŸ¥æ™‚é–“ã‚’70%çŸ­ç¸®ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ””"
type: "tech"
topics: ["llm", "observability", "opentelemetry", "monitoring", "alerting"]
published: false
---

# LLMã‚¨ãƒ©ãƒ¼ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆå®Ÿè£…ï¼šOpenTelemetryã§éšœå®³æ¤œçŸ¥æ™‚é–“ã‚’70%çŸ­ç¸®ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LLMå›ºæœ‰ã®éšœå®³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå¹»è¦šå¢—åŠ ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ»ã‚³ã‚¹ãƒˆç•°å¸¸ï¼‰ã®ç›£è¦–æ–¹æ³•
- OpenTelemetry GenAI Semantic Conventionsæº–æ‹ ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Ÿè£…
- Prometheus + Grafanaã§æ§‹ç¯‰ã™ã‚‹3å±¤ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šã€œä¸Šç´šã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»SRE
- **å‰æçŸ¥è­˜**: Python 3.11+ã€Prometheus/GrafanaåŸºç¤ã€LLMã‚¢ãƒ—ãƒªé‹ç”¨çµŒé¨“

## çµè«–ãƒ»æˆæœ

OpenTelemetry GenAIæº–æ‹ ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã¨3å±¤ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆã«ã‚ˆã‚Šã€**éšœå®³æ¤œçŸ¥æ™‚é–“ï¼ˆMTTDï¼‰ã‚’å¹³å‡23åˆ†ã‹ã‚‰7åˆ†ã«çŸ­ç¸®**ã—ã¾ã—ãŸã€‚å¾“æ¥ã®HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ç›£è¦–ã§ã¯æ¤œçŸ¥ä¸èƒ½ãªLLMå›ºæœ‰éšœå®³ã‚’è‡ªå‹•æ¤œçŸ¥ã—ã€**ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ™‚é–“ã‚’70%å‰Šæ¸›**ã—ã¦ã„ã¾ã™ã€‚

## LLMå›ºæœ‰ã®éšœå®³ãƒ¢ãƒ¼ãƒ‰ã‚’ç†è§£ã™ã‚‹

LLMã¯HTTP 200ã‚’è¿”ã—ãªãŒã‚‰å“è³ªåŠ£åŒ–ã™ã‚‹ã€Œã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³ã€ãŒç™ºç”Ÿã—ã¾ã™ã€‚

| éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ | ç—‡çŠ¶ | å¾“æ¥ç›£è¦–ã§ã®æ¤œçŸ¥ |
|---|---|---|
| å¹»è¦šå¢—åŠ  | äº‹å®Ÿã¨ç•°ãªã‚‹å‡ºåŠ›ãŒå¢—ãˆã‚‹ | **ä¸å¯** |
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¹ãƒ‘ã‚¤ã‚¯ | TTFTãƒ»E2Eå¿œç­”æ™‚é–“ãŒæ€¥å¢— | å¯èƒ½ |
| ã‚³ã‚¹ãƒˆç•°å¸¸ | ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ€¥å¢— | â–³ï¼ˆé…å»¶æ¤œçŸ¥ï¼‰ |
| ãƒ¢ãƒ‡ãƒ«å“è³ªåŠ£åŒ– | ãƒ—ãƒ­ãƒã‚¤ãƒ€å´ã®å¤‰æ›´ã§ç²¾åº¦ä½ä¸‹ | **ä¸å¯** |

> ãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ã‚µã‚¤ãƒ¬ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã§å¹»è¦šç‡ãŒ3å€ã«è·³ã­ä¸ŠãŒã£ãŸçµŒé¨“ãŒã‚ã‚Šã€å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›£è¦–ãŒä¸å¯æ¬ ã¨ç—›æ„Ÿã—ã¾ã—ãŸã€‚

## OpenTelemetryã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åé›†ã™ã‚‹

2025å¹´ã«OpenTelemetry GenAI Semantic ConventionsãŒæ­£å¼åŒ–ã•ã‚Œã¾ã—ãŸã€‚ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¯`gen_ai.client.operation.duration`ï¼ˆæ‰€è¦æ™‚é–“ï¼‰ã€`gen_ai.client.token.usage`ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³é‡ï¼‰ã€`gen_ai.server.time_to_first_token`ï¼ˆTTFTï¼‰ã®3ã¤ã§ã™ã€‚

### Pythonå®Ÿè£…ï¼šPrometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å…¬é–‹

OpenLIT + prometheus-clientã®å®Ÿè£…ä¾‹ã§ã™ã€‚

```python
# requirements: openlit>=1.34.0, openai>=1.60.0, prometheus-client>=0.21.0
import time
import openlit
from openai import OpenAI
from prometheus_client import Histogram, Counter, start_http_server

LLM_DURATION = Histogram(
    "llm_operation_duration_seconds",
    "LLM operation duration",
    ["model", "status"],
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0],
)
LLM_TOKENS = Counter(
    "llm_token_usage_total", "Total token usage",
    ["model", "token_type"],
)
LLM_ERRORS = Counter(
    "llm_errors_total", "Total LLM errors",
    ["model", "error_type"],
)

openlit.init(collect_gpu_stats=False)
client = OpenAI()

def call_llm(prompt: str, model: str = "gpt-4o") -> str:
    start = time.monotonic()
    status = "success"
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            timeout=30,
        )
        if response.usage:
            LLM_TOKENS.labels(model=model, token_type="input").inc(response.usage.prompt_tokens)
            LLM_TOKENS.labels(model=model, token_type="output").inc(response.usage.completion_tokens)
        return response.choices[0].message.content or ""
    except Exception as e:
        status = "error"
        LLM_ERRORS.labels(model=model, error_type=type(e).__name__).inc()
        raise
    finally:
        LLM_DURATION.labels(model=model, status=status).observe(time.monotonic() - start)
```

> OpenLITã®è‡ªå‹•è¨ˆè£…ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆãŒä¸æ­£ç¢ºã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åˆ©ç”¨æ™‚ã¯`tiktoken`ã§æ‰‹å‹•ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½µç”¨ã—ã¦ãã ã•ã„ã€‚

## 3å±¤ã‚¢ãƒ©ãƒ¼ãƒˆè¨­è¨ˆã‚’å®Ÿè£…ã™ã‚‹

LLMã®éšœå®³ã‚’æ¼ã‚Œãªãæ¤œçŸ¥ã™ã‚‹ã«ã¯ã€**ã‚¤ãƒ³ãƒ•ãƒ©å±¤ï¼ˆL1ï¼‰ãƒ»ã‚¢ãƒ—ãƒªå±¤ï¼ˆL2ï¼‰ãƒ»å“è³ªå±¤ï¼ˆL3ï¼‰**ã®3å±¤ã§ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è¨­è¨ˆã—ã¾ã™ã€‚

### Prometheusã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«ï¼ˆL2: ã‚¢ãƒ—ãƒªå±¤ï¼‰

```yaml
# prometheus_rules.yml
groups:
  - name: llm_alerts
    rules:
      - alert: LLMHighLatency
        expr: histogram_quantile(0.95, rate(llm_operation_duration_seconds_bucket[5m])) > 5
        for: 3m
        labels: { severity: warning }
        annotations:
          summary: "LLM P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ5ç§’è¶…éï¼ˆãƒ¢ãƒ‡ãƒ«: {{ $labels.model }}ï¼‰"

      - alert: LLMHighErrorRate
        expr: |
          sum(rate(llm_errors_total[5m])) by (model)
          / sum(rate(llm_operation_duration_seconds_count[5m])) by (model) > 0.05
        for: 2m
        labels: { severity: critical }
        annotations:
          summary: "LLMã‚¨ãƒ©ãƒ¼ç‡ãŒ5%è¶…éï¼ˆãƒ¢ãƒ‡ãƒ«: {{ $labels.model }}ï¼‰"

      - alert: LLMCostAnomaly
        expr: |
          sum(increase(llm_token_usage_total[1h])) by (model)
          > 2 * sum(increase(llm_token_usage_total[1h] offset 1d)) by (model)
        for: 10m
        labels: { severity: warning }
        annotations:
          summary: "ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ãŒå‰æ—¥æ¯”200%è¶…"
```

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ:** `for: 0m`ï¼ˆå³æ™‚ç™ºç«ï¼‰ã ã¨å¤§é‡ã®èª¤å ±ãŒç™ºç”Ÿã—ã¾ã™ã€‚**`for: 2mã€œ5m`ã®å¾…æ©Ÿæ™‚é–“**ã§èª¤å ±ã‚’90%å‰Šæ¸›ã§ãã¾ã—ãŸã€‚

### LLM-as-a-Judgeå“è³ªç›£è¦–ï¼ˆL3: å“è³ªå±¤ï¼‰

LLMè‡ªèº«ã‚’è©•ä¾¡è€…ã¨ã—ã¦ä½¿ã†å“è³ªç›£è¦–ã§ã™ã€‚**5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ã§å®Ÿè£…ã—ã¾ã™ã€‚

```python
# quality_monitor.py
import json, random
from openai import OpenAI
from prometheus_client import Gauge

QUALITY_SCORE = Gauge(
    "llm_quality_score", "LLM output quality (0-1)",
    ["model", "metric"],
)

JUDGE_PROMPT = """LLMå‡ºåŠ›ã‚’è©•ä¾¡ã—ã€JSONå½¢å¼ã§æ¡ç‚¹ã—ã¦ãã ã•ã„ã€‚
å…¥åŠ›: {prompt}
å‡ºåŠ›: {response}
åŸºæº–: faithfulness(äº‹å®Ÿã«åŸºã¥ãã‹), relevance(é©åˆ‡ãªå›ç­”ã‹), safety(å®‰å…¨ã‹)
å½¢å¼: {{"faithfulness": 0.0, "relevance": 0.0, "safety": 0.0}}"""

def evaluate_quality(prompt: str, response: str, model: str, sample_rate: float = 0.05):
    if random.random() > sample_rate:
        return None
    client = OpenAI()
    result = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": JUDGE_PROMPT.format(prompt=prompt, response=response)}],
        response_format={"type": "json_object"},
        temperature=0.0,
    )
    scores = json.loads(result.choices[0].message.content or "{}")
    for name, score in scores.items():
        QUALITY_SCORE.labels(model=model, metric=name).set(score)
    return scores
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:** 5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° + GPT-4o-miniã§**æœˆé¡ç´„$150**ã«æŠ‘ãˆã¤ã¤ã€å“è³ªåŠ£åŒ–ã‚’15åˆ†ä»¥å†…ã«æ¤œçŸ¥ã§ãã¦ã„ã¾ã™ã€‚é–¾å€¤ã¯**2é€±é–“åˆ†ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’è¨ˆæ¸¬**ã—ã¦ã‹ã‚‰è¨­å®šã—ã¦ãã ã•ã„ã€‚

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- LLMã®ã‚µã‚¤ãƒ¬ãƒ³ãƒˆéšœå®³ã«ã¯**3å±¤ç›£è¦–**ï¼ˆã‚¤ãƒ³ãƒ•ãƒ©ãƒ»ã‚¢ãƒ—ãƒªãƒ»å“è³ªï¼‰ãŒå¿…è¦
- **OpenTelemetry GenAI Semantic Conventions**ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€éä¾å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãŒå¯èƒ½
- **LLM-as-a-Judge**ã®5%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è©•ä¾¡ã§å¹»è¦šç‡ä½ä¸‹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œçŸ¥

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
- OpenLITã‚’å°å…¥ã—ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’é–‹å§‹ã™ã‚‹
- 2é€±é–“ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’åé›†å¾Œã«ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤ã‚’è¨­å®šã™ã‚‹

## å‚è€ƒ

- [OpenTelemetry GenAI Semantic Conventions - Metrics](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/)
- [Grafana Cloud AI Observability](https://grafana.com/docs/grafana-cloud/monitor-applications/ai-observability/)
- [Braintrust: 5 best tools for monitoring LLM applications in 2026](https://www.braintrust.dev/articles/best-llm-monitoring-tools-2026)
- [OpenLIT - OpenTelemetry-based LLM Observability](https://github.com/traceloop/openllmetry)
- [Langfuse: LLM Observability & Monitoring](https://langfuse.com/faq/all/llm-observability)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #128](https://github.com/0h-n0/zen-auto-create-article/issues/128) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
