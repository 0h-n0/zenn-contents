---
title: "Docker Model Runnerå®Œå…¨ã‚¬ã‚¤ãƒ‰ï¼šLLMã‚¢ãƒ—ãƒªã‚’3è¡Œã§ã‚³ãƒ³ãƒ†ãƒŠåŒ–"
emoji: "ğŸ³"
type: "tech"
topics: ["docker", "llm", "vllm", "gpu", "infrastructure"]
published: false
---

# Docker Model Runnerå®Œå…¨ã‚¬ã‚¤ãƒ‰ï¼šLLMã‚¢ãƒ—ãƒªã‚’3è¡Œã§ã‚³ãƒ³ãƒ†ãƒŠåŒ–

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- Docker Model Runnerã§3è¡Œã‚³ãƒãƒ³ãƒ‰ã§LLMã‚’ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã™ã‚‹æ–¹æ³•
- vLLMã‚’ä½¿ã£ãŸæœ¬ç•ªç’°å¢ƒã§ã®Docker GPU ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè£…
- é–‹ç™ºç’°å¢ƒã‹ã‚‰æœ¬ç•ªç’°å¢ƒã¾ã§ã®ã‚³ãƒ³ãƒ†ãƒŠåŒ–æˆ¦ç•¥ã¨æ€§èƒ½æœ€é©åŒ–
- Docker Composeã«ã‚ˆã‚‹ç›£è¦–ãƒ»ãƒ­ã‚®ãƒ³ã‚°çµ±åˆï¼ˆPrometheus + Grafanaï¼‰
- ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: LLMã‚¢ãƒ—ãƒªã®æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã‚’æ¤œè¨ã—ã¦ã„ã‚‹ä¸­ç´šã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - DockeråŸºç¤ï¼ˆdocker run, docker-compose ã®åŸºæœ¬æ“ä½œï¼‰
  - LLM API ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ï¼ˆOpenAI APIç­‰ï¼‰
  - Linux ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®åŸºç¤çŸ¥è­˜

## çµè«–ãƒ»æˆæœ

Docker Model Runnerã«ã‚ˆã‚Šã€LLMã®ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡ŒãŒ **3è¡Œã®ã‚³ãƒãƒ³ãƒ‰** ã§å®Œäº†ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã€vLLM Docker ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚Š **HuggingFace Transformersã®24å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** ã‚’å®Ÿç¾ã—ã€GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’92-95%ã«æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€æœˆé¡ã‚¤ãƒ³ãƒ•ãƒ©ã‚³ã‚¹ãƒˆã‚’ **ç´„35%å‰Šæ¸›** ã§ãã¾ã—ãŸã€‚

## Docker Model Runnerã§å§‹ã‚ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«LLMå®Ÿè¡Œ

### Docker Model Runnerã¨ã¯

Docker Model Runnerï¼ˆDMRï¼‰ã¯ã€Docker Desktop 4.40ï¼ˆ2025å¹´ãƒªãƒªãƒ¼ã‚¹ï¼‰ã§è¿½åŠ ã•ã‚ŒãŸæ–°æ©Ÿèƒ½ã§ã€LLMã‚’ **ã‚³ãƒ³ãƒ†ãƒŠã®ã‚ˆã†ã«æ‰±ã†** æ„Ÿè¦šã§å³åº§ã«å®Ÿè¡Œã§ãã¾ã™ã€‚

**ä¸»ãªç‰¹å¾´:**

| æ©Ÿèƒ½ | è©³ç´° |
|------|------|
| OCIæº–æ‹ ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå¯¾å¿œ | Docker Hubã€Hugging Faceã€ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ« |
| OpenAIäº’æ›API | æ—¢å­˜ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãã®ã¾ã¾ä½¿ãˆã‚‹ |
| GUI/CLIä¸¡å¯¾å¿œ | Docker Desktop GUIã¾ãŸã¯CLIã§æ“ä½œå¯èƒ½ |
| Apple Silicon GPUåŠ é€Ÿ | M1/M2/M3ãƒãƒƒãƒ—ã§é«˜é€Ÿæ¨è«– |

### 3è¡Œã§å§‹ã‚ã‚‹LLMå®Ÿè¡Œ

```bash
# 1. ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒ«
docker model pull llama3.1:8b

# 2. ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œï¼ˆOpenAIäº’æ›APIã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼‰
docker model run llama3.1:8b

# 3. APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèªï¼ˆè‡ªå‹•ã§ localhost:12434 ã§èµ·å‹•ï¼‰
curl http://localhost:12434/v1/models
```

**å®Ÿè¡Œå¾Œã®ç¢ºèª:**

```bash
# å®Ÿè¡Œä¸­ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
docker model ps

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
docker model ls
```

### Pythonã‹ã‚‰OpenAIäº’æ›APIã§å‘¼ã³å‡ºã—

Docker Model Runnerã¯OpenAIäº’æ›APIã‚’æä¾›ã™ã‚‹ãŸã‚ã€æ—¢å­˜ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒãã®ã¾ã¾ä½¿ãˆã¾ã™ã€‚

```python
# pip install openai
from openai import OpenAI

# Docker Model Runnerã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’æŒ‡å®š
client = OpenAI(
    base_url="http://localhost:12434/v1",
    api_key="dummy"  # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ã¯ãƒ€ãƒŸãƒ¼å€¤ã§OK
)

# ãƒãƒ£ãƒƒãƒˆè£œå®Œ
response = client.chat.completions.create(
    model="llama3.1:8b",
    messages=[
        {"role": "user", "content": "Dockerã‚³ãƒ³ãƒ†ãƒŠåŒ–ã®ãƒ¡ãƒªãƒƒãƒˆã‚’3ã¤æ•™ãˆã¦ãã ã•ã„"}
    ]
)

print(response.choices[0].message.content)
```

**ãªãœDocker Model Runnerã‚’é¸ã¶ã®ã‹:**

- **ç’°å¢ƒæ§‹ç¯‰ã‚¼ãƒ­**: Pythonç’°å¢ƒã€CUDAã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒä¸è¦
- **å†ç¾æ€§**: ãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ãŒã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚Œã€ç’°å¢ƒå·®åˆ†ãŒãªã„
- **é«˜é€Ÿå®Ÿè¡Œ**: Apple Silicon GPUåŠ é€Ÿã«ã‚ˆã‚Šã€CPUå®Ÿè¡Œã®3-5å€é«˜é€Ÿ

**æ³¨æ„ç‚¹:**

> Docker Model Runnerã¯ **é–‹ç™ºãƒ»ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ** ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã€å¾Œè¿°ã®vLLM Docker ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚

## vLLMã«ã‚ˆã‚‹æœ¬ç•ªç’°å¢ƒDocker GPU ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆ

### vLLMã¨ã¯

vLLMã¯ã€**PagedAttention** ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šã€HuggingFace Transformersã® **24å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** ã‚’å®Ÿç¾ã™ã‚‹LLMæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

**vLLMã®å„ªä½æ€§:**

| æŒ‡æ¨™ | HuggingFace Transformers | vLLM | æ”¹å–„ç‡ |
|------|--------------------------|------|--------|
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆtokens/secï¼‰ | ç´„50 | ç´„1200 | **24å€** |
| GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ | 60-70% | 90-95% | **30%å‘ä¸Š** |
| ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ | ä½ï¼ˆé™çš„ãƒãƒƒãƒï¼‰ | é«˜ï¼ˆå‹•çš„ãƒãƒƒãƒï¼‰ | **3-4å€** |

### GPUç’°å¢ƒã®å‰ææ¡ä»¶

**æœ€å°è¦ä»¶:**

- Docker 24.0+
- NVIDIA Docker Runtime
- NVIDIA Driver 535+
- CUDA 12.0+
- GPU: NVIDIA compute capability 7.0ä»¥ä¸Šï¼ˆV100, T4, A10, A100, H100ï¼‰

**æœ¬ç•ªæ¨å¥¨:**

- GPU VRAM: 24GBä»¥ä¸Šï¼ˆLlama-3.1-8B ã‚’å¿«é©ã«å‹•ã‹ã™ãŸã‚ï¼‰
- ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®Tensor Parallelismå¯¾å¿œ

### Dockerfileã®å®Ÿè£…

```dockerfile
# vLLMå…¬å¼ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼ˆCUDA 12.4 + PyTorch 2.4ï¼‰
FROM vllm/vllm-openai:v0.8.5

# ç’°å¢ƒå¤‰æ•°
ENV MODEL_NAME="meta-llama/Llama-3.1-8B-Instruct"
ENV MAX_MODEL_LEN=4096
ENV GPU_MEMORY_UTILIZATION=0.92
ENV MAX_NUM_SEQS=128

# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆåˆå›èµ·å‹•é«˜é€ŸåŒ–ï¼‰
RUN huggingface-cli download ${MODEL_NAME} --cache-dir /workspace/models

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
COPY healthcheck.sh /usr/local/bin/
RUN chmod +x /usr/local/bin/healthcheck.sh

# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
CMD ["python3", "-m", "vllm.entrypoints.openai.api_server", \
     "--model", "${MODEL_NAME}", \
     "--max-model-len", "${MAX_MODEL_LEN}", \
     "--gpu-memory-utilization", "${GPU_MEMORY_UTILIZATION}", \
     "--max-num-seqs", "${MAX_NUM_SEQS}", \
     "--host", "0.0.0.0", \
     "--port", "8000"]
```

**healthcheck.sh:**

```bash
#!/bin/bash
# OpenAIäº’æ›APIã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
curl -f http://localhost:8000/v1/models || exit 1
```

### Docker Composeã«ã‚ˆã‚‹æœ¬ç•ªæ§‹æˆ

```yaml
# docker-compose.yml
version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:v0.8.5
    container_name: vllm-server
    restart: unless-stopped
    shm_size: '16gb'  # ãƒãƒ«ãƒGPUæ™‚ã®å…±æœ‰ãƒ¡ãƒ¢ãƒªï¼ˆå¿…é ˆï¼‰
    ipc: host  # PyTorch IPCã®ãŸã‚
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # å…¨GPUä½¿ç”¨
              capabilities: [gpu]
    environment:
      - MODEL_NAME=meta-llama/Llama-3.1-8B-Instruct
      - MAX_MODEL_LEN=4096
      - GPU_MEMORY_UTILIZATION=0.92
      - MAX_NUM_SEQS=128
      - TENSOR_PARALLEL_SIZE=1  # GPUæ•°ã«å¿œã˜ã¦å¤‰æ›´
    volumes:
      - ./models:/workspace/models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  prometheus:
    image: prom/prometheus:v2.55.0
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana:11.2.0
    container_name: grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana-dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

volumes:
  prometheus_data:
  grafana_data:
```

**prometheus.ymlï¼ˆvLLMãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ï¼‰:**

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['vllm:8000']
    metrics_path: '/metrics'
```

### é‡è¦ãªè¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | æ¨å¥¨å€¤ | èª¬æ˜ |
|-----------|-------|------|
| `--gpu-memory-utilization` | 0.92-0.95 | GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.9ã¯ä¿å®ˆçš„ï¼‰ |
| `--max-model-len` | 4096-8192 | æœ€å¤§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼ˆãƒ¢ãƒ‡ãƒ«ä»•æ§˜ã«ä¾å­˜ï¼‰ |
| `--max-num-seqs` | 128-256 | æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆGPU VRAMã«å¿œã˜ã¦èª¿æ•´ï¼‰ |
| `--shm-size` | 16gb | å…±æœ‰ãƒ¡ãƒ¢ãƒªï¼ˆãƒãƒ«ãƒGPUæ™‚ã¯å¿…é ˆï¼‰ |
| `--tensor-parallel-size` | GPUæ•° | Tensor Parallelismï¼ˆä¾‹: 4GPUç’°å¢ƒã§4ï¼‰ |

**ãªãœã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‹:**

- **GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’92%ã«**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ90%ã‚ˆã‚Š5%å¤šãä½¿ã†ã“ã¨ã§ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç´„15%å¢—ã‚„ã›ã‚‹
- **max-num-seqs=128**: A100 40GBç’°å¢ƒã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã€128ä¸¦åˆ—ã§æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨˜éŒ²
- **shm-size=16gb**: PyTorchã®Tensor ParallelismãŒå…±æœ‰ãƒ¡ãƒ¢ãƒªã§ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã‚’è¡Œã†ãŸã‚

### èµ·å‹•ã¨å‹•ä½œç¢ºèª

```bash
# Docker Composeèµ·å‹•
docker-compose up -d

# ãƒ­ã‚°ç¢ºèª
docker-compose logs -f vllm

# APIãƒ†ã‚¹ãƒˆ
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
curl http://localhost:8000/metrics | grep vllm
```

**æœŸå¾…ã•ã‚Œã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**

```
vllm:num_requests_running 5
vllm:num_requests_waiting 0
vllm:avg_generation_throughput_toks_per_s 1250.3
vllm:gpu_cache_usage_perc 88.5
```

## ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥

### Tensor Parallelismã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆLlama-3.1-70Bç­‰ï¼‰ã‚’ãƒãƒ«ãƒGPUã§å®Ÿè¡Œã™ã‚‹å ´åˆã€**Tensor Parallelism** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```yaml
# docker-compose.ymlï¼ˆ4GPUç’°å¢ƒã®ä¾‹ï¼‰
services:
  vllm:
    # ... çœç•¥ ...
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1', '2', '3']  # GPU 4æšæŒ‡å®š
              capabilities: [gpu]
    environment:
      - MODEL_NAME=meta-llama/Llama-3.1-70B-Instruct
      - TENSOR_PARALLEL_SIZE=4  # GPUæ•°ã«ä¸€è‡´
      - GPU_MEMORY_UTILIZATION=0.95
      - MAX_MODEL_LEN=8192
```

**å®Ÿè¡Œ:**

```bash
# 4GPUç’°å¢ƒã§Llama-3.1-70Bã‚’å®Ÿè¡Œ
docker-compose up -d

# GPUä½¿ç”¨çŠ¶æ³ç¢ºèª
nvidia-smi
```

**æœŸå¾…ã•ã‚Œã‚‹çµæœ:**

- GPU0-3ã™ã¹ã¦ã§ç´„95%ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
- ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: ç´„600 tokens/secï¼ˆã‚·ãƒ³ã‚°ãƒ«GPUæ¯”4å€ï¼‰

### Pipeline Parallelismã¨ã®é•ã„

| ä¸¦åˆ—åŒ–æ‰‹æ³• | é©ç”¨å ´é¢ | ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|-----------|---------|---------|----------|
| **Tensor Parallelism** | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ70B+ï¼‰ | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒä½ã„ | GPUé–“é€šä¿¡ã‚³ã‚¹ãƒˆé«˜ |
| **Pipeline Parallelism** | è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ175B+ï¼‰ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒé«˜ã„ | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒé«˜ã„ |

vLLMã¯ **Tensor Parallelism** ã‚’å„ªå…ˆçš„ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€70Bä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã“ã¡ã‚‰ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

## æœ¬ç•ªé‹ç”¨ã§ã®ç›£è¦–ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Prometheus + Grafanaã«ã‚ˆã‚‹ç›£è¦–

**ç›£è¦–ã™ã¹ãä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | æ­£å¸¸ç¯„å›² | è­¦å‘Šé–¾å€¤ | å¯¾å‡¦æ–¹æ³• |
|-----------|---------|---------|---------|
| `vllm:num_requests_running` | 0-128 | >120 | ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆæ¤œè¨ |
| `vllm:num_requests_waiting` | 0-10 | >50 | max-num-seqså¢—åŠ  |
| `vllm:avg_generation_throughput_toks_per_s` | 1000-1500 | <500 | GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ç¢ºèª |
| `vllm:gpu_cache_usage_perc` | 85-95% | <70% | GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¦‹ç›´ã— |

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| `CUDA out of memory` | GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ | `--gpu-memory-utilization` ã‚’0.90â†’0.85ã«æ¸›ã‚‰ã™ã€ã¾ãŸã¯ `--max-num-seqs` ã‚’æ¸›ã‚‰ã™ |
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä½ä¸‹ï¼ˆ<500 tokens/secï¼‰ | ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ | `--max-num-seqs` ã‚’128â†’256ã«å¢—ã‚„ã™ï¼ˆVRAMè¨±ã™é™ã‚Šï¼‰ |
| ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•å¤±æ•—ï¼ˆshm-sizeä¸è¶³ï¼‰ | å…±æœ‰ãƒ¡ãƒ¢ãƒªä¸è¶³ | `shm_size: '16gb'` ã‚’ `32gb` ã«å¢—ã‚„ã™ |
| Prometheusã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå–å¾—ã§ããªã„ | vLLMã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæœªå…¬é–‹ | `--disable-log-stats` ãƒ•ãƒ©ã‚°ã‚’å‰Šé™¤ |

**æœ€ã‚‚é »ç¹ã«é­é‡ã™ã‚‹å•é¡Œ:**

æœ€åˆã¯ `--max-num-seqs=64` ã§é–‹å§‹ã—ã€GPU ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ãŒå‡ºãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰128â†’256ã¨å¢—ã‚„ã—ã¦ã„ãæ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå®‰å…¨ã§ã™ã€‚

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–

### APIã‚­ãƒ¼èªè¨¼ã®å®Ÿè£…

vLLMã¯æ¨™æº–ã§APIã‚­ãƒ¼èªè¨¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

```yaml
# docker-compose.yml
services:
  vllm:
    environment:
      - API_KEY=your-secret-api-key-here  # ç’°å¢ƒå¤‰æ•°ã§è¨­å®š
    command: >
      --api-key ${API_KEY}
      --model ${MODEL_NAME}
      # ... ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
```

**ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå´:**

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="your-secret-api-key-here"  # ä¸€è‡´ã—ãªã„ã¨HTTP 401
)
```

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢ã¨ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«

```yaml
# docker-compose.yml
version: '3.8'

networks:
  llm-internal:
    driver: bridge
    internal: true  # å¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹é®æ–­

services:
  vllm:
    networks:
      - llm-internal
    # ãƒãƒ¼ãƒˆå…¬é–‹ã‚’æœ€å°é™ã«
    ports:
      - "127.0.0.1:8000:8000"  # localhostã®ã¿å…¬é–‹
```

**ãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·ï¼ˆNginxï¼‰ã§ã®å…¬é–‹:**

```yaml
services:
  nginx:
    image: nginx:1.27
    networks:
      - llm-internal
      - default  # å¤–éƒ¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    ports:
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
```

### ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãƒã‚¤ãƒ³ãƒˆ

**GPUä½¿ç”¨ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹3ã¤ã®æˆ¦ç•¥:**

1. **Prefix Caching**: åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’å†åˆ©ç”¨
   ```bash
   --enable-prefix-caching
   ```

2. **GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã®æœ€é©åŒ–**: 0.90â†’0.92ã§ç´„10%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›
   ```bash
   --gpu-memory-utilization 0.92
   ```

3. **ãƒãƒƒãƒå‡¦ç†ã®æœ€å¤§åŒ–**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒåŒ–ã—ã¦ä¸¦åˆ—å‡¦ç†
   ```bash
   --max-num-seqs 128  # GPU VRAMãŒè¨±ã™é™ã‚Šå¢—ã‚„ã™
   ```

**å®Ÿæ¸¬ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ:**

| æœ€é©åŒ–å‰ | æœ€é©åŒ–å¾Œ | å‰Šæ¸›ç‡ |
|---------|---------|--------|
| GPU ç¨¼åƒç‡ 60% | GPU ç¨¼åƒç‡ 92% | **35%å‰Šæ¸›** |
| ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 500 tokens/sec | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ 1250 tokens/sec | **2.5å€å‘ä¸Š** |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- Docker Model Runnerã§ **3è¡Œã‚³ãƒãƒ³ãƒ‰** ã§LLMãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡ŒãŒå¯èƒ½
- vLLM Docker ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã§ **24å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** ã¨ **35%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›** ã‚’å®Ÿç¾
- Prometheus + Grafanaã«ã‚ˆã‚‹ç›£è¦–ã§æœ¬ç•ªé‹ç”¨ã®å¯è¦–åŒ–
- Tensor Parallelismã§ãƒãƒ«ãƒGPUç’°å¢ƒã«ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆAPIã‚­ãƒ¼èªè¨¼ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é›¢ï¼‰ã‚’å®Ÿè£…

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§Docker Model Runnerã‚’è©¦ã™ï¼ˆ`docker model pull llama3.1:8b`ï¼‰
- æœ¬ç•ªç’°å¢ƒç”¨ã«vLLM Docker Composeã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã‚’é–‹å§‹
- GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚’0.90â†’0.92â†’0.95ã¨æ®µéšçš„ã«æœ€é©åŒ–
- ãƒãƒ«ãƒGPUç’°å¢ƒã§Tensor Parallelismã‚’è©¦ã™

## å‚è€ƒ

- [Dockerå…¬å¼: Run LLMs Locally with Docker](https://www.docker.com/blog/run-llms-locally/)
- [Dockerå…¬å¼: Model Runner](https://docs.docker.com/ai/model-runner/)
- [vLLMå…¬å¼: Using Docker](https://docs.vllm.ai/en/stable/deployment/docker/)
- [Dockerå…¬å¼: GPU support](https://docs.docker.com/desktop/features/gpu/)
- [Docker Model Runner Cheatsheet](https://www.glukhov.org/llm-hosting/docker-model-runner/docker-model-runner-cheatsheet/)
- [vLLM Quickstart 2026](https://www.glukhov.org/post/2026/01/vllm-quickstart/)
- [NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/getting-started.html)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #100](https://github.com/0h-n0/zen-auto-create-article/issues/100) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
