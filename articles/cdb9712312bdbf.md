---
title: "LLMエージェント評価ベンチマーク完全ガイド：SWE-bench・GAIA・τ-benchの選び方と実践"
emoji: "🏋️"
type: "tech"
topics: ["llm", "agent", "benchmark", "evaluation", "ai"]
published: false
---

# LLMエージェント評価ベンチマーク完全ガイド：SWE-bench・GAIA・τ-benchの選び方と実践

## この記事でわかること

- 2026年時点の主要エージェントベンチマーク7種（SWE-bench, GAIA, AgentBench, WebArena, τ-bench, MLE-bench, BFCL）の特徴と使い分け
- 各ベンチマークの最新リーダーボードスコアと評価メトリクスの読み解き方
- pass@kとpass^kの違いなど、エージェント特有の評価指標の意味と計算方法
- ベンチマークの既知の限界（データ汚染・過学習・実環境乖離）と対策
- 自社エージェントの評価にどのベンチマークを組み合わせるべきかの選定フレームワーク

## 対象読者

- **想定読者**: LLMエージェントを開発・評価する中級者〜上級者のAIエンジニア
- **必要な前提知識**:
  - Python 3.11+での開発経験
  - LLM APIの基本的な利用経験（OpenAI API、Anthropic API等）
  - エージェントフレームワーク（LangGraph、CrewAI等）の基礎知識
  - ソフトウェアテストの基本概念

## 結論・成果

エージェントベンチマークを体系的に活用することで、**開発中エージェントの弱点を定量的に特定し、改善サイクルを2倍速く回せるようになります**。2026年2月時点でSWE-bench Verifiedのトップスコアは80.9%（Claude Opus 4.5）に到達していますが、τ-benchではGPT-4oですら成功率50%未満であり、**ベンチマークごとに測定する能力が大きく異なります**。本記事で紹介する7ベンチマークの選定フレームワークを使えば、自社エージェントの用途に最適な評価戦略を1時間で設計できます。

## エージェントベンチマークが必要な理由を理解する

LLMエージェントの評価は、従来のLLMベンチマーク（MMLU、HumanEvalなど）とは根本的に異なります。エージェントは**マルチターンの対話**、**ツール呼び出し**、**環境操作**を組み合わせて動作するため、「1入力→1出力」の評価では能力を測れません。

実際、LangChainの2025年調査では、**可観測性を実装済みの組織が89%に達する一方、体系的なオフライン評価を実施しているのはわずか52.4%**に留まっています。つまり、多くの組織がエージェントを「動いているかどうか」は監視できても、「どの程度良く動いているか」を定量評価できていないのです。

### エージェント評価の3つの課題

エージェントベンチマークが通常のLLMベンチマークと異なる主な理由は以下の3つです。

**1. 非決定論的な出力**: 同じ入力でも実行のたびに異なる結果が出ます。単一の正解パスではなく、「目標に到達したか」という最終状態での評価が必要です。

**2. 経路の多様性**: タスク完成までの手順が複数あり得ます。Webサイトで商品を購入するタスクでも、検索から行く方法、カテゴリから辿る方法、URLを直接開く方法など、正しいパスは無数にあります。

**3. 環境との相互作用**: エージェントはデータベースを更新したり、ファイルを作成したり、APIを呼び出したりします。評価には実行環境の状態変化を検証する仕組みが必要です。

> **注意**: ベンチマークスコアが高いことと、実環境で使えることは別問題です。エンタープライズでGenerative AIを本番実装に成功しているのはわずか10%というデータもあります。ベンチマークは「能力の上限」を測るものであり、「本番での信頼性」を保証するものではありません。

## 主要7ベンチマークの特徴を比較する

2026年時点で押さえるべきエージェントベンチマークを、用途別に整理します。

### ベンチマーク一覧表

| ベンチマーク | 対象領域 | タスク数 | 評価指標 | 2026年トップスコア | 難易度 |
|---|---|---|---|---|---|
| **SWE-bench Verified** | コーディング | 500 | 解決率(%) | 80.9% | ★★★★ |
| **GAIA** | 汎用アシスタント | 466 | 正答率(%) | ~75% | ★★★★★ |
| **AgentBench** | マルチ環境 | 8環境 | 環境別スコア | 非公開 | ★★★ |
| **WebArena** | Web操作 | 812 | 成功率(%) | ~61.7% | ★★★★ |
| **τ-bench** | 顧客対応 | 2ドメイン | pass^k | <50% | ★★★★★ |
| **MLE-bench** | ML開発 | 75競技 | メダル率(%) | 34.1% (pass@8) | ★★★★ |
| **BFCL** | 関数呼び出し | 2,000 | 正確性(%) | ~95% | ★★ |

### SWE-bench Verified：コーディングエージェントの標準指標

**概要**: GitHubの実際のイシューを解決するパッチ生成能力を評価します。Princeton大学が2023年に公開し、OpenAIが人手で検証した500タスクの「Verified」版が業界標準になっています。

**評価の仕組み**: エージェントにリポジトリのコードベースとイシュー説明を渡し、問題を解決するパッチ（diff）を生成させます。生成されたパッチをDockerコンテナ内で適用し、リポジトリの既存テストスイートを実行して正否を判定します。

```python
# SWE-bench の評価を実行する基本コマンド
# pip install swebench

from datasets import load_dataset

# データセットの読み込み
swebench = load_dataset('princeton-nlp/SWE-bench_Verified', split='test')

# タスクの構造を確認
print(f"タスク数: {len(swebench)}")
print(f"タスク例: {swebench[0]['instance_id']}")
print(f"リポジトリ: {swebench[0]['repo']}")
# 出力例:
# タスク数: 500
# タスク例: django__django-11099
# リポジトリ: django/django
```

```bash
# 予測結果の評価実行
python -m swebench.harness.run_evaluation \
    --dataset_name princeton-nlp/SWE-bench_Verified \
    --predictions_path predictions.json \
    --max_workers 8 \
    --run_id my_evaluation
```

**2026年2月のリーダーボード上位**:

| 順位 | モデル/エージェント | スコア |
|---|---|---|
| 1 | Claude Opus 4.5 | 80.9% |
| 2 | Claude Opus 4.6 | 80.8% |
| 3 | MiniMax M2.5 | 80.2% |
| 4 | GPT-5.2 | 80.0% |
| 5 | GLM-5 (Zhipu AI) | 77.8% |

**なぜSWE-benchを選ぶか**:
- コーディングエージェントの評価には事実上の標準
- 実際のOSSリポジトリを使うため、現実に近い評価が可能
- Dockerベースで再現性が高い

**注意点**:
> SWE-benchの評価は既存のユニットテストに依存しているため、テストカバレッジが不十分な場合、バグのあるパッチでもパスしてしまうことがあります。研究によると、拡張テストを追加した場合、SWE-bench Liteで41%、SWE-bench Verifiedで24%のエージェントのランキングが変動しました。スコアは「テストを通る確率」であって「バグのないコードを書く確率」ではないことに留意してください。

### GAIA：汎用AIアシスタントの総合評価

**概要**: Meta AI、Hugging Face等が共同開発した466タスクのベンチマークです。Web検索、ファイル操作、マルチモーダル理解、複数ステップの推論を組み合わせた現実的なタスクが特徴です。

**評価の仕組み**: 3段階の難易度（Level 1〜3）に分かれており、Level 1は単純なツール使用、Level 3は10ステップ以上の推論チェーンが必要です。回答は一意に定まるFactoidQA形式で、自動的に正否を判定できます。

```python
# GAIAデータセットの構造を確認する例
from datasets import load_dataset

gaia = load_dataset('gaia-benchmark/GAIA', '2023_all', split='validation')

# 難易度別のタスク分布
level_counts = {}
for item in gaia:
    level = item['Level']
    level_counts[level] = level_counts.get(level, 0) + 1

print("難易度別タスク数:")
for level, count in sorted(level_counts.items()):
    print(f"  Level {level}: {count}タスク")

# タスク例の確認
example = gaia[0]
print(f"\n質問例: {example['Question'][:100]}...")
print(f"難易度: Level {example['Level']}")
print(f"必要ステップ数: {example['Annotator Metadata']['Number of steps']}")
```

**特徴的な点**: 人間の正答率が92%であるのに対し、GPT-4（プラグイン装備）は15%しか達成できませんでした。このギャップは「AIに何が欠けているか」を明確に示しています。

**なぜGAIAを選ぶか**:
- エージェントの「汎用性」を測定したい場合の最良の選択肢
- Web検索、ファイル操作、推論の統合能力を評価
- 人間との比較データがあるため、能力のギャップを定量化しやすい

**注意点**:
> GAIAはシングルエージェントの実行を前提としており、マルチエージェント協調の評価には適していません。また、タスクが466問と比較的少ないため、統計的なばらつきが大きくなる可能性があります。

### τ-bench：本番環境の信頼性を測る

**概要**: Sierra AIが開発した、顧客対応エージェントの信頼性評価ベンチマークです。「単発の成功率」ではなく「繰り返し成功するか（一貫性）」を測定する点が画期的です。

**評価の仕組み**: τ-retail（小売カスタマーサービス）とτ-airline（航空券予約変更）の2ドメインで、シミュレートされたユーザーと複数ターンの対話を行います。評価は**データベースの最終状態**を正解と比較する形式で、LLMジャッジに頼りません。

```python
# τ-benchの評価メトリクス: pass^kの概念
# pass@k: k回試行中1回でも成功する確率（楽観的）
# pass^k: k回試行中すべて成功する確率（厳格、信頼性指標）

import math

def pass_at_k(n: int, c: int, k: int) -> float:
    """
    pass@k: n回の試行からk回抽出して少なくとも1回成功する確率
    n: 総試行回数, c: 成功回数, k: 抽出数
    """
    if n - c < k:
        return 1.0
    return 1.0 - math.comb(n - c, k) / math.comb(n, k)

def pass_power_k(pass_1: float, k: int) -> float:
    """
    pass^k: k回連続で成功する確率（信頼性指標）
    pass_1: 1回の成功確率, k: 連続試行数
    """
    return pass_1 ** k

# 例: GPT-4oのτ-retailでの結果
pass_1_score = 0.60  # 1回の成功率 60%
print(f"pass@1: {pass_1_score:.1%}")
print(f"pass^4: {pass_power_k(pass_1_score, 4):.1%}")
print(f"pass^8: {pass_power_k(pass_1_score, 8):.1%}")
# 出力:
# pass@1: 60.0%
# pass^4: 13.0%
# pass^8: 1.7%
# → 60%の成功率でも8回連続成功は1.7%しかない
```

**衝撃的な結果**: τ-benchのテストで、GPT-4oはτ-retailでpass@1が約60%でしたが、**pass^8（8回連続成功）ではわずか約25%まで低下**しました。これは60%の信頼性低下を意味します。

**なぜτ-benchを選ぶか**:
- 「成功率」ではなく「信頼性」を評価したい場合の唯一の選択肢
- ビジネス用途（カスタマーサポート等）のエージェント評価に最適
- データベース状態での評価のため、LLMジャッジのバイアスがない

**注意点**:
> 最初は「60%の成功率なら十分」と考えていましたが、τ-benchのpass^kで評価すると実際の本番信頼性は驚くほど低いことがわかりました。1日100件の問い合わせを処理するエージェントなら、60%の成功率は40件の失敗を意味します。ベンチマークスコアの「数字の意味」を正しく読み解くことが重要です。

### WebArena：Web操作エージェントの実力テスト

**概要**: Carnegie Mellon大学が開発した、現実的なWeb環境でのタスク遂行能力を評価するベンチマークです。Eコマース、ソーシャルフォーラム、コード開発、コンテンツ管理の4カテゴリで812タスクが用意されています。

**評価の仕組み**: 自己ホストされたWebアプリケーション（GitLab、Reddit風サイト、ECサイト等）上でエージェントを実行し、最終的なタスク達成（機能的正しさ）で評価します。途中のパスは問わず、結果だけを見ます。

```python
# WebArenaのタスク構造の例
# 各タスクはintent（目的）とconfig（環境設定）で構成

task_example = {
    "task_id": 42,
    "intent": "Find the cheapest product in the 'Electronics' category "
              "and add it to the shopping cart",
    "sites": ["shopping"],
    "eval": {
        "eval_types": ["url_match", "element_check"],
        "reference_answers": {
            "url": "http://shopping.example.com/cart",
            "must_include": ["cart-item"]
        }
    }
}

# WebArenaの成功率推移（2023-2026）
results_timeline = {
    "2023年末（初期）": 14.0,
    "2024年中盤": 35.0,
    "2025年初頭": 54.8,
    "2025年2月（IBM CUGA）": 61.7,
}

print("WebArena成功率の推移:")
for period, score in results_timeline.items():
    bar = "█" * int(score / 2)
    print(f"  {period:25s} {score:5.1f}% {bar}")
```

**2年間で14%→60%超**: WebArenaの成功率は、2023年末の初期テストから2年間で4倍以上に向上しています。ただし、より難易度の高い**WebChoreArena**（退屈で繰り返しの多い現実的タスク）では、Gemini 2.5 Proでも37.8%にとどまっています。

**注意点**:
> WebArenaは文字列マッチングベースの評価を使用しているため、実際の性能と1.6〜5.2%の誤差があることが指摘されています。WebArena Verifiedでは、型やデータの正規化を考慮した評価に改善されました。

### MLE-bench：ML開発能力の実践評価

**概要**: OpenAIが開発した、75のKaggleコンペティションを使ったML開発エージェントの評価ベンチマークです。モデル学習、データ前処理、特徴量エンジニアリング、実験実行という**ML開発の全プロセス**を評価します。

**評価の仕組み**: 各Kaggleコンペのデータとルールが与えられ、エージェントが提出ファイルを生成します。公開リーダーボードの人間の成績と比較し、銅・銀・金メダルの基準を超えたかで評価します。

```python
# MLE-benchの結果解釈の例
# o1-previewの性能を分析

mle_results = {
    "model": "o1-preview + AIDE scaffolding",
    "pass_at_1_medal_rate": 16.9,  # 1回試行でメダル獲得率
    "pass_at_8_medal_rate": 34.1,  # 8回試行の最良結果でメダル獲得率
    "competitions_total": 75,
}

# pass@kによるスケーリング効果
print("MLE-bench: 試行回数による性能変化")
print(f"  pass@1: {mle_results['pass_at_1_medal_rate']}% "
      f"({int(75 * 0.169)}コンペでメダル)")
print(f"  pass@8: {mle_results['pass_at_8_medal_rate']}% "
      f"({int(75 * 0.341)}コンペでメダル)")
print(f"  スケーリング効果: {mle_results['pass_at_8_medal_rate'] / mle_results['pass_at_1_medal_rate']:.1f}倍")
# 出力:
# pass@1: 16.9% (12コンペでメダル)
# pass@8: 34.1% (25コンペでメダル)
# スケーリング効果: 2.0倍
```

**なぜMLE-benchを選ぶか**:
- MLエンジニアリングエージェントの評価に特化した唯一のベンチマーク
- 実際のKaggleコンペのデータを使うため、現実に即した評価
- ICLR 2025でOral発表される高品質な研究

### AgentBench：8環境での総合力テスト

**概要**: 清華大学が開発した、8つの異なるインタラクティブ環境でエージェントの推論・意思決定能力を評価するベンチマークです。

**8つの環境**:

| 環境 | 内容 | 評価するスキル |
|---|---|---|
| OS | Linuxシェル操作 | システム操作、コマンド実行 |
| DB | SQL問い合わせ | データベース操作 |
| KG | 知識グラフ探索 | 構造化データの推論 |
| Card Game | デジタルカードゲーム | 戦略的意思決定 |
| LTP | 水平思考パズル | 創造的推論 |
| HouseHold | 家事タスク | 物理環境の操作 |
| Web Shopping | ECサイトでの買い物 | Web操作 |
| Web Browsing | Webブラウジング | 情報収集 |

**なぜAgentBenchを選ぶか**:
- エージェントの「総合力」を幅広く測定したい場合に有用
- 特定ドメインに偏らない多面的な評価が可能

### BFCL：関数呼び出しの精度評価

**概要**: Berkeley Function-Calling Leaderboard。LLMのツール呼び出し（Function Calling）精度を評価する2,000問のベンチマークです。

**評価項目**: Python、Java、JavaScript、RESTful APIの関数呼び出しを評価。パラメータの型チェック、並列呼び出し、関数の関連性判定（不要な関数を呼ばない能力）を含みます。

**なぜBFCLを選ぶか**:
- エージェントの基礎能力であるツール呼び出し精度を単体テストできる
- 上位モデルは95%以上を達成しており、この水準を下回るモデルはエージェント用途に不向き

## ベンチマークの限界と落とし穴を把握する

ベンチマークスコアを鵜呑みにすると、エージェント評価を誤ります。ここでは実務で注意すべき4つの限界を解説します。

### データ汚染（Data Contamination）

公開ベンチマークのタスクが学習データに含まれてしまう問題です。LLMの学習データは非公開であるため、**ベンチマークの問題を事前に「見た」かどうかを完全に検証することは不可能**です。

```python
# データ汚染の影響を簡易チェックする考え方
# ベンチマーク公開前後のモデルスコアを比較

contamination_check = {
    "benchmark": "SWE-bench",
    "model_A": {
        "trained_before_benchmark": True,
        "score": 45.0,
    },
    "model_B": {
        "trained_before_benchmark": False,  # ベンチマーク公開後の学習
        "score": 78.0,
    },
    "score_gap": 33.0,
    "interpretation": (
        "スコア差が大きい場合、能力向上だけでなく"
        "データ汚染の可能性も検討が必要"
    ),
}

# 対策: 定期的にベンチマークを更新する
# SWE-bench Live: 時間経過で新しいイシューを追加
# WebArena Verified: 評価基準を改善
```

**対策としてのLiveベンチマーク**: SWE-bench Liveは、時間経過とともに新しいGitHubイシューを追加することで、データ汚染のリスクを軽減しています。ただし、タスクの難易度や品質にばらつきが出るトレードオフがあります。

### 過学習と評価ハック

ベンチマークのタスク数が少ない（数百〜数千）ため、特定のパターンに過学習しやすい問題があります。

```python
# 過学習の例: WebArenaでのURL構造への依存
# あるエージェントがWebArenaで高スコアを出した理由

# ❌ 過学習した戦略
def navigate_bad(task):
    # WebArenaのURLパターンを暗記
    if "shopping" in task:
        return "http://shopping.example.com/category/electronics"
    # → 実環境では通用しない

# ✅ 汎用的な戦略
def navigate_good(task, browser):
    # ページの内容を読み取って判断
    page_content = browser.get_page_content()
    links = browser.extract_links(page_content)
    relevant_link = select_most_relevant(links, task)
    return browser.click(relevant_link)
```

**対策**: 複数のベンチマークでクロス評価し、特定ベンチマークだけ高スコアのエージェントは疑ってかかることが重要です。

### 実環境との乖離

ベンチマークが測定しない（しかし本番では重要な）要素が多数あります。

| ベンチマークが測ること | 本番で重要だが測れないこと |
|---|---|
| タスク成功率 | レイテンシ（応答速度） |
| 正答精度 | APIコスト（1タスクあたり$0.1〜$10） |
| 機能的正しさ | セキュリティ（データ漏洩リスク） |
| 単一ユーザー想定 | 並行処理性能 |
| 静的環境 | 動的な環境変化への適応 |

> **最初は「SWE-bench 80%なら本番でも8割のイシューを解決できる」と考えていましたが、実際には全く違いました。** 本番のイシューはベンチマークより曖昧で、テストスイートがない場合も多く、人間とのやり取りも必要です。ベンチマークスコアは「能力の上限を示すシグナル」として使い、本番性能は別途計測するべきです。

### コスト指標の欠如

現行の主要ベンチマークは**コストを一切報告していません**。しかし、複雑なアーキテクチャ（Reflexion等）は1タスクあたり最大2,000回のAPI呼び出しを行うことがあり、コスト面での実用性は大きく異なります。

```python
# エージェントのコスト概算を自前で計算する例

from dataclasses import dataclass

@dataclass
class AgentCostEstimate:
    """エージェントの1タスクあたりコスト概算"""
    model: str
    avg_api_calls: int
    avg_input_tokens: int
    avg_output_tokens: int
    input_price_per_1k: float  # USD
    output_price_per_1k: float  # USD

    @property
    def cost_per_task(self) -> float:
        input_cost = (self.avg_input_tokens / 1000) * self.input_price_per_1k
        output_cost = (self.avg_output_tokens / 1000) * self.output_price_per_1k
        return (input_cost + output_cost) * self.avg_api_calls

# SWE-benchタスクでの概算例
agents = [
    AgentCostEstimate(
        model="Claude Sonnet 4.6（軽量エージェント）",
        avg_api_calls=15,
        avg_input_tokens=4000,
        avg_output_tokens=1500,
        input_price_per_1k=0.003,
        output_price_per_1k=0.015,
    ),
    AgentCostEstimate(
        model="Claude Opus 4.5（Reflexion型）",
        avg_api_calls=200,
        avg_input_tokens=8000,
        avg_output_tokens=3000,
        input_price_per_1k=0.015,
        output_price_per_1k=0.075,
    ),
]

print("SWE-bench 1タスクあたりのコスト概算:")
for agent in agents:
    print(f"  {agent.model}: ${agent.cost_per_task:.2f}")
# 出力例:
# Claude Sonnet 4.6（軽量エージェント）: $0.52
# Claude Opus 4.5（Reflexion型）: $69.00
# → 同じベンチマークスコアでもコストは100倍以上違い得る
```

## 用途別ベンチマーク選定フレームワークを実践する

「どのベンチマークを使うべきか」は、エージェントの用途によって異なります。以下のフレームワークで、自社エージェントに最適なベンチマーク組み合わせを選定してみましょう。

### ステップ1: エージェントの用途を特定する

```python
# エージェント用途の分類
agent_use_cases = {
    "coding_assistant": {
        "description": "コードの生成・修正・レビュー",
        "primary_benchmarks": ["SWE-bench Verified", "BFCL"],
        "secondary_benchmarks": ["AgentBench (OS, DB)"],
    },
    "customer_support": {
        "description": "顧客対応・問い合わせ処理",
        "primary_benchmarks": ["τ-bench", "GAIA"],
        "secondary_benchmarks": ["WebArena"],
    },
    "data_analysis": {
        "description": "データ分析・ML開発",
        "primary_benchmarks": ["MLE-bench", "AgentBench (DB)"],
        "secondary_benchmarks": ["GAIA"],
    },
    "web_automation": {
        "description": "Webタスクの自動化",
        "primary_benchmarks": ["WebArena", "BFCL"],
        "secondary_benchmarks": ["GAIA"],
    },
    "general_assistant": {
        "description": "汎用アシスタント",
        "primary_benchmarks": ["GAIA", "AgentBench"],
        "secondary_benchmarks": ["τ-bench", "BFCL"],
    },
}

def recommend_benchmarks(use_case: str) -> dict:
    """用途に応じたベンチマーク推奨を返す"""
    if use_case not in agent_use_cases:
        raise ValueError(f"未知の用途: {use_case}")
    config = agent_use_cases[use_case]
    return {
        "用途": config["description"],
        "必須ベンチマーク": config["primary_benchmarks"],
        "推奨ベンチマーク": config["secondary_benchmarks"],
        "注意": "必ずpass^kでの信頼性評価も追加すること",
    }

# 例: コーディングアシスタントの場合
result = recommend_benchmarks("coding_assistant")
for key, value in result.items():
    print(f"{key}: {value}")
```

### ステップ2: 評価の組み合わせパターンを決める

実務でのベンチマーク活用は、**単一ベンチマークではなく複数の組み合わせ**が必須です。

**パターンA: 最小構成（スタートアップ向け）**

```
基礎能力: BFCL（関数呼び出し精度）
 ↓ 基準をクリアしたら
専門能力: SWE-bench or τ-bench（用途に応じて1つ）
 ↓ 基準をクリアしたら
本番評価: 自社データでのカスタム評価
```

**パターンB: 標準構成（中規模チーム向け）**

```
基礎能力: BFCL + AgentBench（2〜3環境）
 ↓
専門能力: 用途別ベンチマーク2つ
 ↓
信頼性: τ-benchのpass^kでの一貫性評価
 ↓
本番評価: 自社データ + コスト計測
```

**パターンC: 包括構成（エンタープライズ向け）**

```
基礎能力: BFCL + AgentBench（全8環境）
 ↓
専門能力: 用途別ベンチマーク3つ以上
 ↓
信頼性: τ-benchのpass^kでの一貫性評価
 ↓
安全性: ToolEmu（リスク行動検出）
 ↓
本番評価: 自社データ + コスト計測 + レイテンシ計測
```

### ステップ3: 評価の自動化パイプラインを構築する

ベンチマーク評価を継続的に実行するためのCI/CDパイプラインの基本構造です。

```python
# エージェント評価パイプラインの基本構造
# eval_pipeline.py

import json
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field, asdict


@dataclass
class BenchmarkResult:
    """ベンチマーク実行結果"""
    benchmark_name: str
    score: float
    metric_name: str
    timestamp: str = field(
        default_factory=lambda: datetime.now().isoformat()
    )
    details: dict = field(default_factory=dict)


@dataclass
class EvalReport:
    """評価レポート"""
    agent_name: str
    agent_version: str
    results: list[BenchmarkResult] = field(default_factory=list)

    def add_result(self, result: BenchmarkResult) -> None:
        self.results.append(result)

    def summary(self) -> dict:
        return {
            "agent": f"{self.agent_name} v{self.agent_version}",
            "benchmarks_run": len(self.results),
            "scores": {
                r.benchmark_name: f"{r.score:.1f}% ({r.metric_name})"
                for r in self.results
            },
        }

    def save(self, output_dir: str = "eval_results") -> Path:
        path = Path(output_dir)
        path.mkdir(exist_ok=True)
        filename = (
            f"{self.agent_name}_{self.agent_version}_"
            f"{datetime.now().strftime('%Y%m%d')}.json"
        )
        filepath = path / filename
        filepath.write_text(
            json.dumps(asdict(self), indent=2, ensure_ascii=False)
        )
        return filepath


# 使用例
report = EvalReport(
    agent_name="my-coding-agent",
    agent_version="0.3.0",
)

# 各ベンチマークの結果を追加
report.add_result(BenchmarkResult(
    benchmark_name="BFCL",
    score=94.2,
    metric_name="accuracy",
))
report.add_result(BenchmarkResult(
    benchmark_name="SWE-bench Verified",
    score=45.0,
    metric_name="resolve_rate",
))
report.add_result(BenchmarkResult(
    benchmark_name="τ-bench (retail)",
    score=52.0,
    metric_name="pass^4",
))

print(json.dumps(report.summary(), indent=2, ensure_ascii=False))
filepath = report.save()
print(f"\nレポート保存先: {filepath}")
```

```bash
# GitHub Actionsでの定期評価の例
# .github/workflows/agent-eval.yml の概念

# schedule:
#   - cron: '0 2 * * 1'  # 毎週月曜 AM2:00
# steps:
#   1. チェックアウト
#   2. エージェントのビルド
#   3. BFCL評価（30分）
#   4. SWE-bench Verified サブセット評価（2時間）
#   5. 結果をSlack通知 + レポート保存
```

## よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|---|---|---|
| SWE-benchのスコアが安定しない | 非決定論的な出力 | pass@k（k=5以上）で評価し、信頼区間を報告する |
| WebArenaの環境構築に失敗 | Docker設定の不備 | 公式のdocker-compose.ymlを使用。x86_64、RAM16GB以上を確保 |
| GAIAでWeb検索が必要なタスクが失敗 | 検索APIの制限 | Serper API等の検索APIを事前に設定。レート制限に注意 |
| τ-benchのpass^kが極端に低い | 一貫性の不足 | プロンプトのルール記述を構造化。few-shotで成功パターンを提示 |
| MLE-benchのコスト増大 | API呼び出し過多 | max_stepsを制限し、コスト上限を設定する |
| ベンチマーク間でスコアの傾向が矛盾 | 測定する能力が異なる | 各ベンチマークの測定対象を明確にし、総合判断する |

## まとめと次のステップ

**まとめ:**

- **SWE-bench Verified**はコーディングエージェントの事実上の標準。2026年2月時点でトップ80.9%だが、テストカバレッジに依存する限界あり
- **τ-bench**のpass^kメトリクスは「信頼性」を測る革新的な指標。本番運用を見据えるなら必須
- **GAIA**は汎用エージェントの総合力評価に最適。人間との92% vs 15%のギャップが能力の上限を示す
- ベンチマークには**データ汚染・過学習・コスト無視**の限界がある。複数ベンチマークのクロス評価が必須
- 単一ベンチマークに依存せず、**用途別に3つ以上を組み合わせたパイプライン**を構築すること

**次にやるべきこと:**

1. 自社エージェントの用途を特定し、本記事の選定フレームワークで評価ベンチマーク2〜3個を選ぶ
2. BFCLで基礎能力（関数呼び出し精度95%以上）を確認してから、専門ベンチマークに進む
3. τ-benchのpass^k（k=4以上）で信頼性を計測し、本番デプロイの判断基準とする

:::message
関連記事: エージェントの本番運用における評価メトリクスの設計については、[AIエージェント評価指標設計：本番運用で成功率95%を実現する測定フレームワーク](https://zenn.dev/0h_n0/articles/66bb91ad7d91df)も参考にしてください。テスト戦略の詳細は[AIエージェントのテスト戦略：pass@kとCI/CD統合で品質を自動保証する実践ガイド](https://zenn.dev/0h_n0/articles/f03733cd5ca3d9)をご覧ください。
:::

## 参考

- [SWE-bench公式サイト - ベンチマーク概要とリーダーボード](https://www.swebench.com/)
- [GAIA Leaderboard - Hugging Face](https://huggingface.co/spaces/gaia-benchmark/leaderboard)
- [Sierra AI - τ-bench: Benchmarking AI agents for the real-world](https://sierra.ai/blog/benchmarking-ai-agents)
- [OpenAI - MLE-bench: Evaluating ML Agents on ML Engineering](https://openai.com/index/mle-bench/)
- [Evidently AI - 10 AI Agent Benchmarks](https://www.evidentlyai.com/blog/ai-agent-benchmarks)
- [Phil Schmid - AI Agent Benchmark Compendium (GitHub)](https://github.com/philschmid/ai-agent-benchmark-compendium)
- [WebArena公式サイト](https://webarena.dev/)
- [AgentBench論文 (arXiv:2308.03688)](https://arxiv.org/abs/2308.03688)
- [Berkeley Function-Calling Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)

---

## 関連する深掘り記事

この記事で紹介した技術について、さらに深掘りした記事を書きました：

- [論文解説: SWE-bench — 実世界GitHubイシューでLLMのSE能力を評価する](https://0h-n0.github.io/posts/paper-2310-06770/) - arxiv解説
- [論文解説: τ-bench — ツール・エージェント・ユーザー三者間対話の信頼性ベンチマーク](https://0h-n0.github.io/posts/paper-2406-12045/) - arxiv解説
- [論文解説: GAIA — 汎用AIアシスタントの実力を測る466タスクベンチマーク](https://0h-n0.github.io/posts/paper-2311-12983/) - arxiv解説
- [論文解説: MLE-bench — 75のKaggleコンペでMLエンジニアリングエージェントを評価する](https://0h-n0.github.io/posts/paper-2410-07095/) - arxiv解説
- [論文解説: WebArena — 812タスクの現実的Web環境で自律エージェントを評価する](https://0h-n0.github.io/posts/paper-2307-13854/) - arxiv解説

:::message
これらの記事は修士学生レベルを想定した技術的詳細（数式・実装の深掘り）を含みます。
:::

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
