---
title: "LangChain LCELå®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼šLLMãƒã‚§ãƒ¼ãƒ³ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’50%å‰Šæ¸›ã™ã‚‹æœ€é©åŒ–æ‰‹æ³•"
emoji: "ğŸ”—"
type: "tech"
topics: ["langchain", "python", "llm", "lcel", "rag"]
published: false
---

# LangChain LCELå®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼šLLMãƒã‚§ãƒ¼ãƒ³ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’50%å‰Šæ¸›ã™ã‚‹æœ€é©åŒ–æ‰‹æ³•

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LCELï¼ˆLangChain Expression Languageï¼‰ã®åŸºæœ¬æ§‹æ–‡ã¨Runnable APIã®ä»•çµ„ã¿
- RunnableParallelã«ã‚ˆã‚‹ä¸¦åˆ—å®Ÿè¡Œã§ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å‰Šæ¸›ã™ã‚‹æ–¹æ³•
- with_fallbacks / with_retryã‚’ä½¿ã£ãŸæœ¬ç•ªé‹ç”¨å‘ã‘ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- LangChain 1.0ã¸ã®ç§»è¡Œã‚’è¦‹æ®ãˆãŸè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¸ã³æ–¹
- LCEL vs LangGraphã®ä½¿ã„åˆ†ã‘åŸºæº–ã¨ç§»è¡Œæˆ¦ç•¥

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šè€…ã®Pythonãƒ»LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºè€…
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.10ä»¥ä¸Šã®åŸºç¤æ–‡æ³•ï¼ˆasync/awaitå«ã‚€ï¼‰
  - LangChain ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ï¼ˆChatModel, PromptTemplateï¼‰
  - LLM APIã®å‘¼ã³å‡ºã—çµŒé¨“ï¼ˆOpenAI, Anthropicç­‰ï¼‰

## çµè«–ãƒ»æˆæœ

LCELã®`RunnableParallel`ã‚’æ´»ç”¨ã—ã¦ç‹¬ç«‹ã—ãŸLLMå‘¼ã³å‡ºã—ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã§ã€**é€æ¬¡å®Ÿè¡Œã¨æ¯”è¼ƒã—ã¦ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’40-60%å‰Šæ¸›**ã§ãã¾ã™ã€‚å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€LCELã§æ§‹ç¯‰ã—ãŸãƒã‚§ãƒ¼ãƒ³ã¯åŒæœŸãƒ»éåŒæœŸãƒ»ãƒãƒƒãƒãƒ»ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®4ã¤ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æ¨™æº–ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ãŠã‚Šã€æœ¬ç•ªç’°å¢ƒã§ã®é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‡¦ç†ã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€LangChain 1.0ï¼ˆ2025å¹´9æœˆãƒªãƒªãƒ¼ã‚¹ï¼‰ã§ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ`create_agent`ã«ç§»è¡Œã—ã¤ã¤ã‚ã‚‹ãŸã‚ã€**LCELã¯ç·šå½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰ã«é›†ä¸­ã—ã¦ä½¿ã„ã€è¤‡é›‘ãªçŠ¶æ…‹ç®¡ç†ãŒå¿…è¦ãªå ´åˆã¯LangGraphã‚’é¸æŠã™ã‚‹**ã®ãŒ2026å¹´æ™‚ç‚¹ã§ã®æ¨å¥¨æˆ¦ç•¥ã§ã™ã€‚

## LCELã®åŸºæœ¬æ§‹æ–‡ã¨Runnable APIã‚’ç†è§£ã™ã‚‹

LCELã¯LangChainã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã«ä½¿ã‚ã‚Œã‚‹å®£è¨€çš„ãªè¡¨ç¾è¨€èªã§ã™ã€‚Pythonã®ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ï¼ˆ`|`ï¼‰ã‚’ä½¿ã£ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆâ†’LLMâ†’å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã¨ã„ã£ãŸå‡¦ç†ã®æµã‚Œã‚’1è¡Œã§è¨˜è¿°ã§ãã¾ã™ã€‚

### ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ã®ä»•çµ„ã¿

LCELã®ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ã¯ã€Pythonã®`__or__`ãƒã‚¸ãƒƒã‚¯ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ­ãƒ¼ãƒ‰ã—ã¦å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚`a | b`ã¨æ›¸ãã¨ã€`a`ã®å‡ºåŠ›ãŒ`b`ã®å…¥åŠ›ã«è‡ªå‹•çš„ã«æ¸¡ã•ã‚Œã‚‹`RunnableSequence`ãŒæ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚

```python
# lcel_basic.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å®šç¾©
prompt = ChatPromptTemplate.from_template(
    "ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„: {topic}"
)
model = ChatOpenAI(model="gpt-4o", temperature=0.0)
parser = StrOutputParser()

# ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ã§ãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
chain = prompt | model | parser

# å®Ÿè¡Œ
result = chain.invoke({"topic": "LangChain Expression Language"})
print(result)
```

**ãªãœã“ã®å®Ÿè£…ã‚’é¸ã‚“ã ã‹:**
- ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ã«ã‚ˆã‚Š**å·¦ã‹ã‚‰å³ã¸ã®å‡¦ç†ã®æµã‚Œ**ãŒè¦–è¦šçš„ã«æ˜ç¢ºã«ãªã‚‹
- å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ†ã‚¹ãƒˆã‚„å·®ã—æ›¿ãˆãŒå®¹æ˜“
- `invoke()`ã‚’å‘¼ã¶ã ã‘ã§å‹å¤‰æ›ãŒè‡ªå‹•çš„ã«è¡Œã‚ã‚Œã‚‹

**æ³¨æ„ç‚¹:**
> ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ã¯å¯èª­æ€§ãŒé«˜ã„ä¸€æ–¹ã€ãƒã‚§ãƒ¼ãƒ³ãŒé•·ããªã‚‹ã¨ãƒ‡ãƒãƒƒã‚°ãŒé›£ã—ããªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚5æ®µéšä»¥ä¸Šã®ãƒã‚§ãƒ¼ãƒ³ã«ãªã‚‹å ´åˆã¯ã€ä¸­é–“å¤‰æ•°ã«åˆ†å‰²ã—ã¦LangSmithã§ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

### Runnable APIã®4ã¤ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰

LCELã§æ§‹ç¯‰ã—ãŸãƒã‚§ãƒ¼ãƒ³ã¯ã€ä»¥ä¸‹ã®4ã¤ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’**è¿½åŠ ã‚³ãƒ¼ãƒ‰ãªã—ã§**åˆ©ç”¨ã§ãã¾ã™ã€‚ã“ã‚Œã¯Runnableã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå„ãƒ¢ãƒ¼ãƒ‰ã‚’æ¨™æº–å®Ÿè£…ã—ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚

| å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ | ãƒ¡ã‚½ãƒƒãƒ‰ | éåŒæœŸç‰ˆ | ä¸»ãªç”¨é€” |
|-----------|---------|---------|---------|
| å˜ä¸€å®Ÿè¡Œ | `.invoke(input)` | `.ainvoke(input)` | 1ä»¶ã®å…¥åŠ›ã‚’å‡¦ç† |
| ãƒãƒƒãƒå®Ÿè¡Œ | `.batch(inputs)` | `.abatch(inputs)` | è¤‡æ•°å…¥åŠ›ã‚’ä¸€æ‹¬å‡¦ç† |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | `.stream(input)` | `.astream(input)` | ãƒˆãƒ¼ã‚¯ãƒ³é€æ¬¡å‡ºåŠ› |
| ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ  | `.astream_events(input)` | - | ãƒã‚§ãƒ¼ãƒ³å†…éƒ¨ã®è©³ç´°è¿½è·¡ |

```python
# lcel_execution_modes.py
import asyncio

# ãƒãƒƒãƒå®Ÿè¡Œ: è¤‡æ•°ã®å…¥åŠ›ã‚’åŒæ™‚ã«å‡¦ç†
results = chain.batch([
    {"topic": "RAG"},
    {"topic": "Fine-tuning"},
    {"topic": "Prompt Engineering"},
])
# â†’ 3ä»¶ã®çµæœãŒãƒªã‚¹ãƒˆã§è¿”ã‚‹

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: ãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚Œã‚‹ãŸã³ã«å‡ºåŠ›
for chunk in chain.stream({"topic": "LCEL"}):
    print(chunk, end="", flush=True)

# éåŒæœŸå®Ÿè¡Œ: Webã‚µãƒ¼ãƒãƒ¼ç­‰ã§ã®é«˜ä¸¦è¡Œå‡¦ç†ã«å¯¾å¿œ
async def async_example():
    result = await chain.ainvoke({"topic": "LangGraph"})
    return result

asyncio.run(async_example())
```

å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹ã¨ã€éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ã†ã“ã¨ã§ã€ŒLCELãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ãƒ›ã‚¹ãƒˆã™ã‚‹ã‚µãƒ¼ãƒãƒ¼ã¯ã€ã‚ˆã‚Šé«˜ã„åŒæ™‚æ¥ç¶šè² è·ã«å¯¾ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã§ãã‚‹ã€ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

## RunnableParallelã§ç‹¬ç«‹å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹

ãƒã‚§ãƒ¼ãƒ³å†…ã«**äº’ã„ã«ä¾å­˜ã—ãªã„å‡¦ç†**ãŒã‚ã‚‹å ´åˆã€`RunnableParallel`ã§ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚å…¸å‹çš„ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã¯ã€ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹RAGã§ã®è¤‡æ•°ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã®åŒæ™‚å®Ÿè¡Œã§ã™ã€‚

### ä¸¦åˆ—å®Ÿè¡Œã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# lcel_parallel.py
import time
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI(model="gpt-4o", temperature=0.0)

# ç‹¬ç«‹ã—ãŸ3ã¤ã®åˆ†æã‚’ä¸¦åˆ—å®Ÿè¡Œ
parallel_chain = RunnableParallel({
    "summary": (
        ChatPromptTemplate.from_template("{topic}ã®æ¦‚è¦ã‚’2æ–‡ã§è¿°ã¹ã¦ãã ã•ã„")
        | model
        | StrOutputParser()
    ),
    "pros_cons": (
        ChatPromptTemplate.from_template("{topic}ã®ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆã‚’å„3ã¤æŒ™ã’ã¦ãã ã•ã„")
        | model
        | StrOutputParser()
    ),
    "use_cases": (
        ChatPromptTemplate.from_template("{topic}ã®å®Ÿè·µçš„ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã‚’3ã¤æŒ™ã’ã¦ãã ã•ã„")
        | model
        | StrOutputParser()
    ),
})

# RunnablePassthroughã§å…ƒã®å…¥åŠ›ã‚‚ä¿æŒ
chain_with_context = RunnableParallel({
    "analysis": parallel_chain,
    "original_topic": RunnablePassthrough(),
})

start = time.time()
result = chain_with_context.invoke({"topic": "LCEL"})
elapsed = time.time() - start
print(f"ä¸¦åˆ—å®Ÿè¡Œ: {elapsed:.1f}ç§’")
```

**ãªãœRunnableParallelã‚’é¸ã‚“ã ã‹:**
- 3ã¤ã®LLMå‘¼ã³å‡ºã—ãŒäº’ã„ã«ç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€**é€æ¬¡å®Ÿè¡Œã§ã¯3å€ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**ãŒã‹ã‹ã‚‹
- `RunnableParallel`ã¯è¾æ›¸å½¢å¼ã§åˆ†å²ã‚’å®šç¾©ã™ã‚‹ã ã‘ã§ä¸¦åˆ—å®Ÿè¡Œã•ã‚Œã‚‹
- çµæœã¯è¾æ›¸ã®ã‚­ãƒ¼åã§ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãŸã‚ã€å¾Œç¶šå‡¦ç†ã¨ã®é€£æºãŒå®¹æ˜“

### ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹RAGã§ã®å®Ÿè·µä¾‹

å®Ÿéš›ã®RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åŒæ™‚ã«æ¤œç´¢çµæœã‚’å–å¾—ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé »å‡ºã—ã¾ã™ã€‚

```python
# lcel_multi_source_rag.py
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser

model = ChatOpenAI(model="gpt-4o", temperature=0.0)

# ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã®å®šç¾©ï¼ˆå®Ÿéš›ã«ã¯ãƒ™ã‚¯ãƒˆãƒ«DBã‹ã‚‰å–å¾—ï¼‰
def search_docs(query: dict) -> str:
    """ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ï¼ˆæ¨¡æ“¬ï¼‰"""
    return f"ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢çµæœ: {query['question']}ã«é–¢ã™ã‚‹æƒ…å ±..."

def search_faq(query: dict) -> str:
    """FAQæ¤œç´¢ï¼ˆæ¨¡æ“¬ï¼‰"""
    return f"FAQæ¤œç´¢çµæœ: {query['question']}ã«é–¢ã™ã‚‹ã‚ˆãã‚ã‚‹è³ªå•..."

def search_api_docs(query: dict) -> str:
    """APIä»•æ§˜æ›¸æ¤œç´¢ï¼ˆæ¨¡æ“¬ï¼‰"""
    return f"APIä»•æ§˜æ›¸: {query['question']}ã«é–¢ã™ã‚‹æŠ€è¡“ä»•æ§˜..."

# 3ã¤ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä¸¦åˆ—å®Ÿè¡Œ
retrieval_chain = RunnableParallel({
    "doc_context": RunnableLambda(search_docs),
    "faq_context": RunnableLambda(search_faq),
    "api_context": RunnableLambda(search_api_docs),
    "question": lambda x: x["question"],
})

# æ¤œç´¢çµæœã‚’çµ±åˆã—ã¦LLMã§å›ç­”ç”Ÿæˆ
answer_prompt = ChatPromptTemplate.from_template(
    """ä»¥ä¸‹ã®æƒ…å ±æºã‚’å‚ç…§ã—ã¦è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
{doc_context}

## FAQ
{faq_context}

## APIä»•æ§˜æ›¸
{api_context}

## è³ªå•
{question}

å›ç­”:"""
)

rag_chain = retrieval_chain | answer_prompt | model | StrOutputParser()

result = rag_chain.invoke({"question": "èªè¨¼APIã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã¯ï¼Ÿ"})
print(result)
```

> **æ³¨æ„**: `RunnableParallel`ã¯å„ãƒ–ãƒ©ãƒ³ãƒã«**åŒã˜å…¥åŠ›**ã‚’æ¸¡ã—ã¾ã™ã€‚ãƒ–ãƒ©ãƒ³ãƒé–“ã§ç•°ãªã‚‹å…¥åŠ›ãŒå¿…è¦ãªå ´åˆã¯ã€`RunnableLambda`ã§å…¥åŠ›ã‚’å¤‰æ›ã—ã¦ã‹ã‚‰æ¸¡ã—ã¦ãã ã•ã„ã€‚

### RunnableLambdaã§ã‚«ã‚¹ã‚¿ãƒ å‡¦ç†ã‚’çµ„ã¿è¾¼ã‚€

`RunnableLambda`ã¯ä»»æ„ã®Pythoné–¢æ•°ã‚’Runnableã«å¤‰æ›ã—ã¾ã™ã€‚ãƒã‚§ãƒ¼ãƒ³å†…ã§ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚„ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†éš›ã«ä½¿ç”¨ã—ã¾ã™ã€‚

```python
# lcel_lambda.py
from langchain_core.runnables import RunnableLambda

def validate_and_transform(data: dict) -> dict:
    """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨å¤‰æ›"""
    question = data.get("question", "").strip()
    if len(question) < 3:
        raise ValueError("è³ªå•ã¯3æ–‡å­—ä»¥ä¸Šã§å…¥åŠ›ã—ã¦ãã ã•ã„")
    return {
        "question": question,
        "question_length": len(question),
        "timestamp": "2026-02-22T22:00:00+09:00",
    }

def format_output(result: str) -> dict:
    """å‡ºåŠ›ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
    return {
        "answer": result,
        "char_count": len(result),
    }

# RunnableLambdaã§ãƒã‚§ãƒ¼ãƒ³ã«çµ„ã¿è¾¼ã¿
validated_chain = (
    RunnableLambda(validate_and_transform)
    | retrieval_chain
    | answer_prompt
    | model
    | StrOutputParser()
    | RunnableLambda(format_output)
)
```

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ:**
> `RunnableLambda`ã«æ¸¡ã™é–¢æ•°ã¯**å˜ä¸€ã®å¼•æ•°ã®ã¿**ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚è¤‡æ•°ã®å€¤ã‚’æ¸¡ã—ãŸã„å ´åˆã¯è¾æ›¸ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚ã¾ãŸã€`RunnableLambda`ã§å®šç¾©ã—ãŸRunnableã¯`.invoke()`ã§å‘¼ã³å‡ºã™å¿…è¦ãŒã‚ã‚Šã€ç›´æ¥`()`ã§é–¢æ•°å‘¼ã³å‡ºã—ã¯ã§ãã¾ã›ã‚“ã€‚

## æœ¬ç•ªé‹ç”¨ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹

LLM APIã¯å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã¸ã®ä¾å­˜ãŒã‚ã‚‹ãŸã‚ã€æœ¬ç•ªç’°å¢ƒã§ã¯ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ»ä¸€æ™‚çš„ãªã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ã¸ã®å¯¾ç­–ãŒä¸å¯æ¬ ã§ã™ã€‚LCELã¯`with_fallbacks()`ã¨`with_retry()`ã‚’ãƒã‚§ãƒ¼ãƒ³ã®ä»»æ„ã®ä½ç½®ã«é©ç”¨ã§ãã¾ã™ã€‚

### with_fallbacksã«ã‚ˆã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

```python
# lcel_fallback.py
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«
primary_model = ChatOpenAI(
    model="gpt-4o",
    temperature=0.0,
    max_retries=0,  # â† ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨æ™‚ã¯ãƒªãƒˆãƒ©ã‚¤ã‚’ç„¡åŠ¹åŒ–
)

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«
fallback_model = ChatAnthropic(
    model="claude-sonnet-4-5-20250514",
    temperature=0.0,
    max_retries=0,
)

# ä½ã‚³ã‚¹ãƒˆã®ã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
secondary_fallback = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.0,
    max_retries=0,
)

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³: gpt-4o â†’ Claude Sonnet â†’ gpt-4o-mini
model_with_fallback = primary_model.with_fallbacks(
    [fallback_model, secondary_fallback]
)

prompt = ChatPromptTemplate.from_template(
    "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã®ãƒã‚°ã‚’ç‰¹å®šã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„:\n{code}"
)

chain = prompt | model_with_fallback | StrOutputParser()
```

**ãªãœmax_retries=0ã«ã™ã‚‹ã®ã‹:**
- LangChainã®LLMãƒ©ãƒƒãƒ‘ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§APIã‚¨ãƒ©ãƒ¼æ™‚ã«è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹
- `with_fallbacks()`ã‚’ä½¿ã†å ´åˆã€ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ãŒå®Œäº†ã™ã‚‹ã¾ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«åˆ‡ã‚Šæ›¿ã‚ã‚‰ãªã„
- `max_retries=0`ã«è¨­å®šã™ã‚‹ã“ã¨ã§ã€**å³åº§ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ã‚ã‚‹**

ã“ã‚Œã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚‚ã€Œãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ã†éš›ã¯ãƒªãƒˆãƒ©ã‚¤ã‚’ç„¡åŠ¹åŒ–ã™ã¹ãã€ã¨è¨€åŠã•ã‚Œã¦ã„ã‚‹è¨­å®šã§ã™ã€‚

### with_retryã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥

```python
# lcel_retry.py
from langchain_core.runnables import RunnableConfig

# æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ããƒªãƒˆãƒ©ã‚¤
robust_chain = chain.with_retry(
    retry_if_exception_type=(TimeoutError, ConnectionError),
    stop_after_attempt=3,
    wait_exponential_jitter=True,
)

# ç‰¹å®šã®ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ã«ãƒªãƒˆãƒ©ã‚¤ã‚’é©ç”¨
model_with_retry = model.with_retry(
    stop_after_attempt=3,
    wait_exponential_jitter=True,
)

reliable_chain = prompt | model_with_retry | StrOutputParser()
```

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ãƒªãƒˆãƒ©ã‚¤ã®çµ„ã¿åˆã‚ã›

```python
# lcel_production.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.output_parsers import StrOutputParser

def build_production_chain():
    """æœ¬ç•ªé‹ç”¨å‘ã‘ãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰"""

    # ãƒ—ãƒ©ã‚¤ãƒãƒª: gpt-4oï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰
    primary = ChatOpenAI(
        model="gpt-4o",
        temperature=0.0,
        max_retries=0,
        request_timeout=30,
    ).with_retry(
        stop_after_attempt=2,
        wait_exponential_jitter=True,
    )

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Claude Sonnet
    fallback = ChatAnthropic(
        model="claude-sonnet-4-5-20250514",
        temperature=0.0,
        max_retries=0,
        timeout=30.0,
    )

    # ãƒªãƒˆãƒ©ã‚¤â†’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®é †ã§é©ç”¨
    resilient_model = primary.with_fallbacks([fallback])

    prompt = ChatPromptTemplate.from_template("{query}")
    chain = prompt | resilient_model | StrOutputParser()

    return chain

chain = build_production_chain()
result = chain.invoke({"query": "Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€æ–¹æ³•ã¯ï¼Ÿ"})
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**
> ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’å¢—ã‚„ã™ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå¢—åŠ ã—ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã€Œãƒªãƒˆãƒ©ã‚¤2å› + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯1æ®µã€ç¨‹åº¦ãŒå¿œç­”é€Ÿåº¦ã¨ä¿¡é ¼æ€§ã®ãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚ŒãŸæ§‹æˆã§ã™ã€‚ãƒªãƒˆãƒ©ã‚¤å¾…æ©Ÿä¸­ã‚‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å¾…ãŸã•ã‚Œã‚‹ãŸã‚ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨çµ„ã¿åˆã‚ã›ã¦**æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã®å¿œç­”æ™‚é–“ï¼ˆTTFTï¼‰ã‚’çŸ­ç¸®ã™ã‚‹**ã“ã¨ã‚‚æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

## ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†ã§å¿œç­”æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹

LCELãƒã‚§ãƒ¼ãƒ³ã®å¿œç­”æ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†ãŒã‚ã‚Šã¾ã™ã€‚

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§TTFTã‚’çŸ­ç¸®ã™ã‚‹

LLMã®å¿œç­”ç”Ÿæˆã«ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ä½¿ãˆã°ãƒˆãƒ¼ã‚¯ãƒ³ãŒç”Ÿæˆã•ã‚Œã‚‹ãŸã³ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å±Šã‘ã‚‰ã‚Œã¾ã™ã€‚

```python
# lcel_streaming.py
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template(
    "ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¦ãã ã•ã„: {topic}"
)
model = ChatOpenAI(model="gpt-4o", temperature=0.0, streaming=True)
parser = StrOutputParser()

chain = prompt | model | parser

# åŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
for chunk in chain.stream({"topic": "LCELã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"}):
    print(chunk, end="", flush=True)
print()  # æ”¹è¡Œ
```

**FastAPIã¨ã®çµ±åˆ:**

```python
# lcel_fastapi_streaming.py
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

app = FastAPI()

prompt = ChatPromptTemplate.from_template("{query}")
model = ChatOpenAI(model="gpt-4o", temperature=0.0)
chain = prompt | model | StrOutputParser()

@app.post("/chat")
async def chat(query: str):
    async def generate():
        async for chunk in chain.astream({"query": query}):
            yield chunk

    return StreamingResponse(generate(), media_type="text/plain")
```

### ãƒãƒƒãƒå‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å‘ä¸Šã™ã‚‹

è¤‡æ•°ã®å…¥åŠ›ã‚’ä¸€æ‹¬å‡¦ç†ã™ã‚‹å ´åˆã€`batch()`ãƒ¡ã‚½ãƒƒãƒ‰ãŒè‡ªå‹•çš„ã«ä¸¦åˆ—å®Ÿè¡Œã‚’è¡Œã„ã¾ã™ã€‚

```python
# lcel_batch.py
import time

queries = [
    {"topic": "RAG"},
    {"topic": "Fine-tuning"},
    {"topic": "Prompt Engineering"},
    {"topic": "Agent"},
    {"topic": "Embedding"},
]

# é€æ¬¡å®Ÿè¡Œï¼ˆæ¯”è¼ƒç”¨ï¼‰
start = time.time()
sequential_results = [chain.invoke(q) for q in queries]
sequential_time = time.time() - start

# ãƒãƒƒãƒå®Ÿè¡Œ
start = time.time()
batch_results = chain.batch(queries, config={"max_concurrency": 5})
batch_time = time.time() - start

print(f"é€æ¬¡å®Ÿè¡Œ: {sequential_time:.1f}ç§’")
print(f"ãƒãƒƒãƒå®Ÿè¡Œ: {batch_time:.1f}ç§’")
print(f"é«˜é€ŸåŒ–ç‡: {sequential_time / batch_time:.1f}å€")
```

`max_concurrency`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚APIã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

**åˆ¶ç´„æ¡ä»¶:**
> ãƒãƒƒãƒå‡¦ç†ã¯å„å…¥åŠ›ãŒç‹¬ç«‹ã—ã¦ã„ã‚‹å ´åˆã«ã®ã¿æœ‰åŠ¹ã§ã™ã€‚å…¥åŠ›é–“ã«ä¾å­˜é–¢ä¿‚ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹: å‰ã®å‡ºåŠ›ã‚’æ¬¡ã®å…¥åŠ›ã«ä½¿ã†ï¼‰ã¯ã€é€æ¬¡å®Ÿè¡Œã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚OpenAIã®å ´åˆã€Tier 1ã§æ¯åˆ†500ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®åˆ¶é™ãŒã‚ã‚‹ãŸã‚ã€å¤§é‡ãƒãƒƒãƒå‡¦ç†ã§ã¯`max_concurrency`ã‚’é©åˆ‡ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## LangChain 1.0ç§»è¡Œã‚’è¦‹æ®ãˆãŸè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠã™ã‚‹

2025å¹´9æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸLangChain 1.0ã§ã¯ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è¨­è¨ˆæ€æƒ³ãŒå¤§ããå¤‰ã‚ã‚Šã¾ã—ãŸã€‚LCELã®ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­è‡ªä½“ã¯`langchain-core`ã§å¼•ãç¶šãã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ãŒã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ã¯`create_agent`ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç§»è¡Œã—ã¦ã„ã¾ã™ã€‚

### LCEL vs LangGraph: ä½¿ã„åˆ†ã‘ã®åˆ¤æ–­åŸºæº–

| åŸºæº– | LCELï¼ˆlangchain-coreï¼‰ | LangGraph |
|------|----------------------|-----------|
| **å‡¦ç†ãƒ•ãƒ­ãƒ¼** | ç·šå½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ | å¾ªç’°ãƒ»åˆ†å²ãƒ»çŠ¶æ…‹ç®¡ç† |
| **é©ã—ãŸãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹** | RAGã€è¦ç´„ã€åˆ†é¡ã€æŠ½å‡º | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—æ¨è«– |
| **çŠ¶æ…‹ç®¡ç†** | ãªã—ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ï¼‰ | ã‚ã‚Šï¼ˆæ°¸ç¶šçš„ãªçŠ¶æ…‹ï¼‰ |
| **ã‚¨ãƒ©ãƒ¼å‡¦ç†** | with_fallbacks / with_retry | ãƒãƒ¼ãƒ‰å˜ä½ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° |
| **ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ã‚¤ãƒ³ã‚¶ãƒ«ãƒ¼ãƒ—** | éå¯¾å¿œ | ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ |
| **å­¦ç¿’ã‚³ã‚¹ãƒˆ** | ä½ï¼ˆãƒ‘ã‚¤ãƒ—æ§‹æ–‡ã®ã¿ï¼‰ | ä¸­ï¼ˆã‚°ãƒ©ãƒ•æ¦‚å¿µã®ç†è§£ãŒå¿…è¦ï¼‰ |
| **æ¨å¥¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³** | langchain-core 1.2.x | langgraph 1.0.x |

**åˆ¤æ–­ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ:**

1. å‡¦ç†ãŒ**ä¸€æ–¹å‘**ï¼ˆå…¥åŠ›â†’å‡¦ç†â†’å‡ºåŠ›ï¼‰ã§å®Œçµã™ã‚‹ â†’ **LCEL**
2. **ãƒ«ãƒ¼ãƒ—ã‚„æ¡ä»¶åˆ†å²**ãŒå¿…è¦ â†’ **LangGraph**
3. **ä¼šè©±ã®æ–‡è„ˆã‚’ä¿æŒ**ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ â†’ **LangGraph**
4. ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®çµæœã«å¿œã˜ã¦**æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š**ã™ã‚‹ â†’ **LangGraph**

### LangChain 1.0ã§ã®å¤‰æ›´ç‚¹

LangChain 1.0ã§ã¯ä»¥ä¸‹ã®å¤‰æ›´ãŒè¡Œã‚ã‚Œã¾ã—ãŸã€‚

```python
# âŒ v0.3ä»¥å‰: LLMChainï¼ˆéæ¨å¥¨ï¼‰
from langchain.chains import LLMChain
chain = LLMChain(llm=model, prompt=prompt)
result = chain.run("å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ")

# âœ… v1.0: LCELï¼ˆæ¨å¥¨ï¼‰
chain = prompt | model | StrOutputParser()
result = chain.invoke({"topic": "å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ"})

# âœ… v1.0: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ã¯ create_agent ã‚’ä½¿ç”¨
from langchain.agents import create_agent

agent = create_agent(
    model="gpt-4o",
    tools=[search_tool, calculator_tool],
    system_prompt="ã‚ãªãŸã¯èª¿æŸ»ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™",
)
```

**ã‚ˆãã‚ã‚‹é–“é•ã„:**
> ã€ŒLCELã®ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­ãŒéæ¨å¥¨ã«ãªã£ãŸã€ã¨ã„ã†èª¤è§£ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­è‡ªä½“ã¯`langchain-core`ã§å¼•ãç¶šãã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚éæ¨å¥¨ã«ãªã£ãŸã®ã¯`LLMChain`ã‚„`AgentExecutor`ãªã©ã®æ—§æ¥ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ã«ã¯LCELã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ§‹ç¯‰ã«ã¯`create_agent`ï¼ˆå†…éƒ¨çš„ã«LangGraphã‚’ä½¿ç”¨ï¼‰ã¨ã„ã†ä½¿ã„åˆ†ã‘ãŒ2026å¹´æ™‚ç‚¹ã®æ¨å¥¨ã§ã™ã€‚

### å°†æ¥ã‚’è¦‹æ®ãˆãŸè¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ

```python
# lcel_future_proof.py
"""
LangChain 1.0å¯¾å¿œã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³
- ãƒã‚§ãƒ¼ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã‚’é–¢æ•°ã«åˆ†é›¢
- Runnableã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ä¾å­˜
- LangGraphã¸ã®ç§»è¡ŒãŒå®¹æ˜“ãªæ§‹é€ 
"""
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser


def create_summarization_chain(model_name: str = "gpt-4o"):
    """è¦ç´„ãƒã‚§ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    model = ChatOpenAI(model=model_name, temperature=0.0)
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®æ–‡æ›¸ã‚’3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„:\n\n{document}"
    )
    return prompt | model | StrOutputParser()


def create_classification_chain(model_name: str = "gpt-4o"):
    """åˆ†é¡ãƒã‚§ãƒ¼ãƒ³ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    model = ChatOpenAI(model=model_name, temperature=0.0)
    prompt = ChatPromptTemplate.from_template(
        "ä»¥ä¸‹ã®æ–‡æ›¸ã®ã‚«ãƒ†ã‚´ãƒªã‚’1ã¤é¸ã‚“ã§ãã ã•ã„ "
        "(æŠ€è¡“/ãƒ“ã‚¸ãƒã‚¹/ãã®ä»–):\n\n{document}"
    )
    return prompt | model | StrOutputParser()


def create_analysis_pipeline(model_name: str = "gpt-4o"):
    """ä¸¦åˆ—åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    return RunnableParallel({
        "summary": create_summarization_chain(model_name),
        "category": create_classification_chain(model_name),
    })


# ä½¿ç”¨ä¾‹
pipeline = create_analysis_pipeline()
result = pipeline.invoke({"document": "LangChain 1.0ãŒ2025å¹´9æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œ..."})
print(f"è¦ç´„: {result['summary']}")
print(f"ã‚«ãƒ†ã‚´ãƒª: {result['category']}")
```

**è¨­è¨ˆã®ãƒã‚¤ãƒ³ãƒˆ:**
- ãƒã‚§ãƒ¼ãƒ³ã‚’**ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°**ã§å®šç¾©ã—ã€ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã—ã¦ãŠãã¨ãƒ†ã‚¹ãƒˆã‚„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“
- `RunnableParallel`ã§ä¸¦åˆ—åŒ–å¯èƒ½ãªç²’åº¦ã«ãƒã‚§ãƒ¼ãƒ³ã‚’åˆ†å‰²ã—ã¦ãŠã
- LangGraphã«ç§»è¡Œã™ã‚‹å ´åˆã¯ã€å„ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ã‚’ãã®ã¾ã¾**ã‚°ãƒ©ãƒ•ã®ãƒãƒ¼ãƒ‰**ã¨ã—ã¦å†åˆ©ç”¨å¯èƒ½

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| `TypeError: Expected a Runnable` | é€šå¸¸ã®Pythoné–¢æ•°ã‚’ãƒ‘ã‚¤ãƒ—ã«æ¸¡ã—ãŸ | `RunnableLambda(func)`ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ |
| ãƒãƒƒãƒå®Ÿè¡Œã§429ã‚¨ãƒ©ãƒ¼ | APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆè¶…é | `max_concurrency`ã‚’ä¸‹ã’ã‚‹ï¼ˆä¾‹: 3ï¼‰ |
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒãƒ£ãƒ³ã‚¯ãŒæ¥ãªã„ | å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ãŒãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚° | `StrOutputParser()`ã‚’ä½¿ç”¨ã™ã‚‹ |
| ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒé…ã„ | ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ | `max_retries=0`ã«è¨­å®šã™ã‚‹ |
| RunnableParallelã®å‹ã‚¨ãƒ©ãƒ¼ | è¾æ›¸ã®valueå‹ãŒä¸ä¸€è‡´ | å„ãƒ–ãƒ©ãƒ³ãƒã®å‡ºåŠ›å‹ã‚’çµ±ä¸€ã™ã‚‹ |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—å¤§ | å¤§é‡ãƒãƒƒãƒå‡¦ç† | `max_concurrency`ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- LCELã¯ãƒ‘ã‚¤ãƒ—æ¼”ç®—å­(`|`)ã§**å®£è¨€çš„ã«LLMãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰**ã§ãã‚‹ä»•çµ„ã¿ã§ã€`langchain-core`ã«å«ã¾ã‚Œã‚‹
- `RunnableParallel`ã§ç‹¬ç«‹å‡¦ç†ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã¨ã€ãƒã‚§ãƒ¼ãƒ³å…¨ä½“ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’40-60%å‰Šæ¸›ã§ãã‚‹
- æœ¬ç•ªé‹ç”¨ã§ã¯`with_fallbacks()`ã¨`with_retry()`ã®çµ„ã¿åˆã‚ã›ã§ã‚¨ãƒ©ãƒ¼è€æ€§ã‚’ç¢ºä¿ã™ã‚‹
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¨ãƒãƒƒãƒå‡¦ç†ã¯è¿½åŠ ã‚³ãƒ¼ãƒ‰ãªã—ã§åˆ©ç”¨å¯èƒ½ï¼ˆ`.stream()` / `.batch()`ï¼‰
- LangChain 1.0ã§ã¯ç·šå½¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³â†’LCELã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆâ†’`create_agent`ï¼ˆLangGraphï¼‰ã®ä½¿ã„åˆ†ã‘ãŒæ¨å¥¨

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- [LangChainã®å…¬å¼Runnable APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹](https://reference.langchain.com/python/langchain_core/runnables/)ã§å„Runnableã®è©³ç´°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªã™ã‚‹
- LangSmithã‚’å°å…¥ã—ã¦ãƒã‚§ãƒ¼ãƒ³ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ»ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ã‚’è¡Œã†
- æ—¢å­˜ã®`LLMChain`ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’LCELã«ç§»è¡Œã™ã‚‹

## å‚è€ƒ

- [LangChain Expression Language | Pinecone](https://www.pinecone.io/learn/series/langchain/langchain-expression-language/)
- [LangChain Expression Language (LCEL) | Aurelio AI](https://www.aurelio.ai/learn/langchain-lcel)
- [LangChain Runnables API Reference](https://reference.langchain.com/python/langchain_core/runnables/)
- [LangChain v1 Migration Guide](https://docs.langchain.com/oss/python/migrate/langchain-v1)
- [LangChain vs LangGraph 2026 Decision Guide](https://myengineeringpath.dev/tools/langchain-vs-langgraph/)
- [Lessons Learned from Upgrading to LangChain 1.0 in Production](https://towardsdatascience.com/lessons-learnt-from-upgrading-to-langchain-1-0-in-production/)

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
