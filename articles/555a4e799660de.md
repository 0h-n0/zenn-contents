---
title: "LangGraph×Claude Sonnet 4.6のプロンプトキャッシュ最適化でAgentic RAGコスト90%削減"
emoji: "⚡"
type: "tech"
topics: ["langgraph", "claude", "rag", "python", "llm"]
published: false
---

# LangGraph×Claude Sonnet 4.6のプロンプトキャッシュ最適化でAgentic RAGコスト90%削減

## この記事でわかること

- Claude Sonnet 4.6のプロンプトキャッシュ（KVキャッシュ）の仕組みと、自動キャッシュ・明示的ブレークポイントの使い分け
- LangGraphのAgentic RAGパイプラインにプロンプトキャッシュを統合し、APIコストを最大90%削減する実装方法
- ツール定義・システムプロンプト・RAGコンテキスト・会話履歴の4層キャッシュブレークポイント設計パターン
- キャッシュヒット率をモニタリングし、本番環境で安定したコスト・レイテンシ改善を維持する運用手法

## 対象読者

- **想定読者**: 中級〜上級のPythonエンジニアでLLMアプリケーション開発経験者
- **必要な前提知識**:
  - Python 3.12+の基礎文法
  - LangGraph 0.3+の基本的なStateGraph構築経験
  - Claude API（Messages API）の基本的な利用経験
  - RAG（Retrieval-Augmented Generation）の基本概念

## 結論・成果

Anthropicの公式ドキュメントによると、プロンプトキャッシュにより**APIコスト最大90%削減**、**レイテンシ最大85%改善**が報告されています。さらに、学術論文「Don't Break the Cache」（arxiv:2601.06007）では、Claude Sonnetでコスト78.5%削減・TTFT（Time to First Token）22.9%改善が実測されています。

本記事では、LangGraphのAgentic RAGパイプラインにClaude Sonnet 4.6のプロンプトキャッシュを組み込み、社内検索システムのAPIコストとレスポンス時間を改善する実装手法を解説します。

| 指標 | キャッシュなし | キャッシュあり | 改善率 |
|------|-------------|-------------|--------|
| 入力トークン単価 | $3.00/MTok | $0.30/MTok（読取） | 90%削減 |
| 10万トークン会話の1ターンコスト | $0.30 | $0.03 | 90%削減 |
| TTFT（Time to First Token） | ベースライン | 22.9%改善 | - |

（出典: [Anthropic公式ドキュメント](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)、[Don't Break the Cache](https://arxiv.org/abs/2601.06007)）

> **制約**: キャッシュの効果はプロンプトの構造に依存します。動的コンテンツが多い場合や、5分以内に同一プレフィックスが再利用されない場合、コスト削減効果は低下します。

## プロンプトキャッシュの仕組みを理解する

プロンプトキャッシュは、LLMの推論過程で生成される**KV（Key-Value）テンソル**をAPI側で保存・再利用する仕組みです。通常、LLMはリクエストごとに全入力トークンのKVテンソルを計算しますが、キャッシュが有効な場合、以前と同一のプレフィックス部分はキャッシュから読み込まれ、計算が省略されます。

### 自動キャッシュと明示的ブレークポイントの違い

Claude Sonnet 4.6では2種類のキャッシュ設定方法が提供されています。

| 方式 | 設定方法 | 適用シーン | ブレークポイント管理 |
|------|---------|-----------|------------------|
| **自動キャッシュ** | リクエストレベルで`cache_control`指定 | マルチターン会話 | システム自動 |
| **明示的ブレークポイント** | 各コンテンツブロックに`cache_control`配置 | 異なる頻度で変更されるセクション | 手動（最大4つ） |

```python
# 自動キャッシュ: リクエストレベルで1パラメータ追加
import anthropic

client = anthropic.Anthropic()

response = client.messages.create(
    model="claude-sonnet-4-6-20250929",
    max_tokens=4096,
    cache_control={"type": "ephemeral"},  # これだけで自動キャッシュ有効
    system="あなたは社内文書検索アシスタントです。...",
    messages=[
        {"role": "user", "content": "売上レポートの要約を教えてください"}
    ],
)

# レスポンスのusageフィールドでキャッシュ効果を確認
print(f"キャッシュ読取: {response.usage.cache_read_input_tokens}")
print(f"キャッシュ書込: {response.usage.cache_creation_input_tokens}")
print(f"非キャッシュ入力: {response.usage.input_tokens}")
```

**なぜ自動キャッシュが便利か:**
- 1パラメータの追加だけでキャッシュが有効になる
- 会話が進むごとにキャッシュブレークポイントが自動的に最終ブロックへ移動する
- 既存のコードへの変更が最小限で済む

**注意点:**
> 自動キャッシュは最後のキャッシュ可能ブロックにブレークポイントを配置します。4層の細かい制御が必要な場合は、明示的ブレークポイントを使用してください。また、最小キャッシュ可能トークン数はClaude Sonnet 4.6で**1024トークン**です。これを下回るプロンプトではキャッシュが適用されません。

### キャッシュの価格構造を把握する

コスト最適化の前提として、Claude Sonnet 4.6のキャッシュ関連価格を整理します。

| トークン種別 | 単価（/MTok） | ベース比 |
|------------|-------------|---------|
| 通常入力 | $3.00 | 1.0x |
| 5分キャッシュ書込 | $3.75 | 1.25x |
| 1時間キャッシュ書込 | $6.00 | 2.0x |
| キャッシュ読取 | $0.30 | 0.1x |
| 出力 | $15.00 | - |

（出典: [Anthropic公式価格表](https://platform.claude.com/docs/en/about-claude/pricing)）

初回リクエストではキャッシュ書込コスト（1.25x）が発生しますが、2回目以降はキャッシュ読取（0.1x）で処理されるため、**繰り返し利用されるプレフィックスほどコスト削減効果が大きく**なります。

**よくある間違い**: 「キャッシュを有効にすれば常にコスト削減できる」と考えがちですが、キャッシュ書込は通常入力より25%高くなります。5分以内に同一プレフィックスが再利用されない1回限りのリクエストでは、キャッシュなしより高コストになります。

## LangGraphのAgentic RAGにプロンプトキャッシュを統合する

ここからは、LangGraphのAgentic RAGパイプラインにClaude Sonnet 4.6のプロンプトキャッシュを統合する実装を見ていきます。

### アーキテクチャ概要

Agentic RAGでは、クエリごとに複数回のLLM呼び出し（ルーティング判定→検索→関連性評価→回答生成）が発生します。各ステップで共通するツール定義やシステムプロンプトをキャッシュすることで、繰り返しの計算コストを削減します。

```
┌──────────────────────────────────────────────┐
│  ツール定義 (cache breakpoint 1)              │  ← 変更頻度: 低
├──────────────────────────────────────────────┤
│  システムプロンプト (cache breakpoint 2)       │  ← 変更頻度: 低
├──────────────────────────────────────────────┤
│  RAGコンテキスト (cache breakpoint 3)         │  ← 変更頻度: 中（検索結果依存）
├──────────────────────────────────────────────┤
│  会話履歴 (cache breakpoint 4 / 自動)         │  ← 変更頻度: 高（毎ターン追加）
├──────────────────────────────────────────────┤
│  ユーザーメッセージ (非キャッシュ)             │  ← 毎回異なる
└──────────────────────────────────────────────┘
```

### AnthropicPromptCachingMiddlewareを使った統合

LangChainの`langchain-anthropic`パッケージは、`AnthropicPromptCachingMiddleware`を提供しており、LangGraphのエージェントと直接統合できます。

```python
# requirements: langchain-anthropic>=0.4, langgraph>=0.3
from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

# RAG用ツール定義
def search_documents(query: str) -> str:
    """社内文書を検索して関連ドキュメントを返す"""
    # 実際にはベクトルDBへの検索処理
    # ここでは簡略化
    return f"検索結果: '{query}'に関連する文書3件を取得"

def grade_relevance(document: str, query: str) -> str:
    """検索結果の関連性を評価する"""
    return f"関連性スコア: 0.85 - '{document}'は'{query}'に関連あり"

# システムプロンプト（長いほどキャッシュ効果が高い）
SYSTEM_PROMPT = """あなたは社内文書検索アシスタントです。

# 指示
- ユーザーの質問に対して、必ず社内文書を検索してから回答してください
- 検索結果の関連性を評価し、関連度の高い文書のみを使用してください
- 回答には必ず参照元の文書名を含めてください
- 情報が見つからない場合は正直に伝えてください

# 回答フォーマット
1. 検索した文書の概要
2. 質問への回答
3. 参照元文書リスト
"""

# プロンプトキャッシュ + 状態永続化を組み合わせたエージェント
agent = create_react_agent(
    model=ChatAnthropic(
        model="claude-sonnet-4-6-20250929",
        max_tokens=4096,
    ),
    tools=[search_documents, grade_relevance],
    prompt=SYSTEM_PROMPT,
    middleware=[
        AnthropicPromptCachingMiddleware(ttl="5m"),
    ],
    checkpointer=MemorySaver(),
)

# 実行（thread_idでセッションを管理）
config = {"configurable": {"thread_id": "user-session-001"}}

# 1回目: キャッシュ書込が発生
result1 = agent.invoke(
    {"messages": [{"role": "user", "content": "Q3の売上データを教えて"}]},
    config=config,
)

# 2回目: キャッシュ読取でコスト90%削減
result2 = agent.invoke(
    {"messages": [{"role": "user", "content": "前年比はどうなっている？"}]},
    config=config,
)
```

**なぜAnthropicPromptCachingMiddlewareを選んだか:**
- `create_react_agent`と直接統合でき、カスタムグラフの構築が不要
- `MemorySaver`（チェックポイント）と組み合わせることで、会話状態の永続化とキャッシュ最適化を同時に実現できる
- TTLの指定（`5m`/`1h`）だけで設定が完了する

**注意点:**
> `AnthropicPromptCachingMiddleware`はAPI側のプロンプトキャッシュのみを管理します。会話のメモリ（文脈の保持）は`MemorySaver`などのチェックポイント機構が担当します。この2つは独立した機能であり、両方を組み合わせて初めて「キャッシュ効率の高いステートフルなエージェント」が実現できます。

### カスタムStateGraphで4層キャッシュブレークポイントを実装する

より細かい制御が必要な場合、LangGraphのStateGraphを使って明示的キャッシュブレークポイントを配置できます。

```python
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langchain_anthropic import ChatAnthropic
import anthropic

# 状態定義
class RAGState(TypedDict):
    query: str
    documents: list[str]
    relevance_scores: list[float]
    answer: str
    cache_metrics: dict  # キャッシュメトリクス追跡用

# Anthropicクライアント（明示的キャッシュブレークポイント用）
raw_client = anthropic.Anthropic()

# ツール定義（変更頻度: 低 → キャッシュブレークポイント1）
TOOLS = [
    {
        "name": "search_internal_docs",
        "description": "社内文書をキーワード検索する",
        "input_schema": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "検索キーワード"},
                "top_k": {"type": "integer", "description": "取得件数", "default": 5},
            },
            "required": ["query"],
        },
    },
    {
        "name": "grade_document",
        "description": "文書の関連性を0-1でスコアリングする",
        "input_schema": {
            "type": "object",
            "properties": {
                "document": {"type": "string"},
                "query": {"type": "string"},
            },
            "required": ["document", "query"],
        },
        "cache_control": {"type": "ephemeral"},  # ← BP1: ツール定義末尾
    },
]

# システムプロンプト（変更頻度: 低 → キャッシュブレークポイント2）
SYSTEM_BLOCKS = [
    {
        "type": "text",
        "text": "あなたは社内文書検索の専門アシスタントです。...",
        "cache_control": {"type": "ephemeral"},  # ← BP2: システムプロンプト
    },
]


def route_query(state: RAGState) -> RAGState:
    """クエリをルーティング: 検索が必要か判定"""
    response = raw_client.messages.create(
        model="claude-sonnet-4-6-20250929",
        max_tokens=256,
        tools=TOOLS,
        system=SYSTEM_BLOCKS,
        messages=[
            {"role": "user", "content": state["query"]},
        ],
    )
    state["cache_metrics"] = {
        "cache_read": response.usage.cache_read_input_tokens,
        "cache_write": response.usage.cache_creation_input_tokens,
        "uncached": response.usage.input_tokens,
    }
    return state


def generate_answer(state: RAGState) -> RAGState:
    """検索結果を基に回答を生成"""
    # RAGコンテキストを組み立て（キャッシュブレークポイント3）
    rag_context = {
        "type": "text",
        "text": f"## 検索結果\n\n" + "\n".join(state["documents"]),
        "cache_control": {"type": "ephemeral"},  # ← BP3: RAGコンテキスト
    }

    response = raw_client.messages.create(
        model="claude-sonnet-4-6-20250929",
        max_tokens=4096,
        tools=TOOLS,
        system=SYSTEM_BLOCKS + [rag_context],
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": state["query"],
                        "cache_control": {"type": "ephemeral"},  # ← BP4: 会話
                    }
                ],
            }
        ],
    )
    state["answer"] = response.content[0].text
    return state


# StateGraph構築
graph = StateGraph(RAGState)
graph.add_node("route", route_query)
graph.add_node("generate", generate_answer)
graph.add_edge("route", "generate")
graph.add_edge("generate", END)
graph.set_entry_point("route")

app = graph.compile()
```

このパターンでは、ツール定義・システムプロンプト・RAGコンテキスト・会話履歴それぞれに独立したキャッシュブレークポイントを配置しています。ツール定義やシステムプロンプトが変更されない限り、それらの部分は常にキャッシュから読み込まれます。

**トレードオフ**: 明示的ブレークポイント方式は制御性が高い一方、コードの複雑さが増します。多くのユースケースでは`AnthropicPromptCachingMiddleware`による自動キャッシュで十分です。明示的ブレークポイントが必要なのは、異なるTTLを混在させたい場合や、20ブロックを超える長い会話でキャッシュヒット率を最大化したい場合です。

## キャッシュを壊さないプロンプト設計の原則を実践する

学術論文「Don't Break the Cache」（arxiv:2601.06007）の知見と、Anthropicの実装経験に基づくプロンプト設計の原則を実践してみましょう。

### 原則1: 静的コンテンツを先頭に配置する

キャッシュは**プレフィックスマッチング**で動作します。プロンプトの先頭から一致する部分だけがキャッシュされるため、変更頻度の低いコンテンツを先頭に配置することが重要です。

```python
# NG: 動的コンテンツが先頭にある
messages = [
    {"role": "system", "content": f"現在時刻: {datetime.now()}。あなたは..."},
    #                                ^^^^^^^^^^^^^^^^ 毎回変わるのでキャッシュが壊れる
]

# OK: 静的コンテンツが先頭、動的コンテンツは末尾
system_blocks = [
    {
        "type": "text",
        "text": "あなたは社内文書検索アシスタントです。...",  # 静的
        "cache_control": {"type": "ephemeral"},
    },
    {
        "type": "text",
        "text": f"現在時刻: {datetime.now()}",  # 動的（キャッシュ対象外）
    },
]
```

「Don't Break the Cache」論文では、**システムプロンプトのみをキャッシュする戦略が最も安定した効果を発揮した**と報告されています。動的なツール結果のキャッシュは、キャッシュオーバーヘッドに見合う効果が得られないケースがあります。

### 原則2: ツール定義を安定させる

Anthropicの公式情報によると、Claude Codeチームは「すべてのツールを毎回のAPIリクエストに含め、キャッシュプレフィックスを保持する」設計を採用しています。ツール定義はキャッシュ階層の最上位（`tools` → `system` → `messages`の順序）にあるため、ツールの変更は全キャッシュを無効化します。

```python
# NG: リクエストごとにツールを動的に選択
def get_tools_for_query(query_type: str) -> list:
    if query_type == "search":
        return [search_tool]
    elif query_type == "analyze":
        return [analyze_tool]
    # ツール構成が変わるとキャッシュが壊れる

# OK: 全ツールを常に含め、LLMに選択を委ねる
ALL_TOOLS = [search_tool, analyze_tool, grade_tool, summarize_tool]
# ツール構成が安定するのでキャッシュが有効に機能する
```

### 原則3: 1時間キャッシュの使いどころを見極める

5分TTLと1時間TTLの選択基準を整理します。

| シナリオ | 推奨TTL | 理由 |
|---------|---------|------|
| 高頻度マルチターン会話（5分以内に次のターン） | 5分 | 自動リフレッシュされるため追加コスト不要 |
| サブエージェントの実行（5分超） | 1時間 | 処理完了まで5分を超える可能性がある |
| バッチ処理の共通プレフィックス | 1時間 | 最初のリクエストでキャッシュ作成、後続で再利用 |
| 低頻度のユーザー対話（数十分間隔） | 1時間 | 5分TTLでは間に合わないが、1時間なら再利用可能 |

```python
# 1時間キャッシュの設定例
response = client.messages.create(
    model="claude-sonnet-4-6-20250929",
    max_tokens=4096,
    cache_control={"type": "ephemeral", "ttl": "1h"},  # 1時間TTL
    system="...",
    messages=[...],
)
```

**ハマりポイント**: 1時間キャッシュ書込は通常入力の**2倍**のコストです（$6.00/MTok vs $3.00/MTok）。5分以内に必ず再利用されるプロンプトに1時間キャッシュを設定すると、書込コストが無駄に高くなります。5分TTLでは自動リフレッシュが無料で行われるため、再利用間隔に応じたTTL選択が重要です。

## キャッシュヒット率をモニタリングして運用する

本番環境では、キャッシュの効果を定量的に把握するモニタリングが不可欠です。

### キャッシュメトリクスの収集

Claude APIのレスポンスには、キャッシュパフォーマンスを計測するためのフィールドが含まれています。

```python
import json
from datetime import datetime, timezone

def log_cache_metrics(response, request_id: str) -> dict:
    """キャッシュメトリクスを構造化ログとして記録"""
    usage = response.usage

    total_input = (
        usage.cache_read_input_tokens
        + usage.cache_creation_input_tokens
        + usage.input_tokens
    )

    # キャッシュヒット率を算出
    cache_hit_rate = (
        usage.cache_read_input_tokens / total_input * 100
        if total_input > 0
        else 0.0
    )

    metrics = {
        "event": "llm_cache_metrics",
        "level": "info",
        "ts": datetime.now(timezone.utc).isoformat(),
        "request_id": request_id,
        "cache_read_tokens": usage.cache_read_input_tokens,
        "cache_write_tokens": usage.cache_creation_input_tokens,
        "uncached_tokens": usage.input_tokens,
        "total_input_tokens": total_input,
        "output_tokens": usage.output_tokens,
        "cache_hit_rate_pct": round(cache_hit_rate, 2),
    }

    # 構造化JSONログ出力
    print(json.dumps(metrics, ensure_ascii=False))

    return metrics
```

### キャッシュヒット率の目安

Anthropicの公式情報およびMeta Manus社の知見によると、本番AIエージェントではKVキャッシュヒット率が最重要メトリクスとして位置付けられています。

| ヒット率 | 評価 | 対応 |
|---------|------|------|
| 80%以上 | 良好 | 現行の設計を維持 |
| 50-80% | 改善余地あり | プロンプト構造の見直し、動的コンテンツの後置化 |
| 50%未満 | 要対応 | キャッシュブレークポイント配置の再設計、TTL変更の検討 |

Anthropicの公式情報によれば、Claude Codeチームはキャッシュヒット率の低下を本番インシデントとして扱い、ページャーアラートを設定しています。これはキャッシュ効率がインフラコストと直結するためです。

### LangGraphノードレベルキャッシュとの併用

プロンプトキャッシュ（API側）に加え、LangGraphのノードレベルキャッシュ（アプリ側）を併用することで、同一クエリに対するLLM呼び出し自体をスキップできます。

```python
from langgraph.graph import StateGraph
from langgraph.cache import InMemoryCache, CachePolicy

cache = InMemoryCache()

graph = StateGraph(RAGState)

# ノードレベルキャッシュ: 同一入力のノードは実行をスキップ
graph.add_node(
    "route",
    route_query,
    cache_policy=CachePolicy(ttl=300),  # 5分間キャッシュ
)
graph.add_node(
    "generate",
    generate_answer,
    cache_policy=CachePolicy(ttl=60),   # 回答生成は1分間キャッシュ
)

app = graph.compile(cache=cache)
```

**2層キャッシュの効果:**

| 層 | 対象 | 効果 |
|----|------|------|
| LangGraphノードキャッシュ | 同一入力のノード実行 | LLM API呼び出し自体を省略 |
| Anthropicプロンプトキャッシュ | 同一プレフィックスの入力トークン | トークン処理コスト90%削減 |

ノードキャッシュはAPI呼び出しを完全にスキップする（キャッシュヒット時のレイテンシはほぼゼロ）一方、プロンプトキャッシュはAPI呼び出しは発生するがトークン処理を高速化します。両者は補完的に機能します。

**制約**: ノードレベルキャッシュは入力が完全一致するときのみ有効です。RAGの検索結果のように毎回異なるコンテンツが含まれるノードでは効果が限定的です。一方、ルーティング判定のように入力パターンが限られるノードでは有効に機能します。

## よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| キャッシュヒット率が0%のまま | プロンプトのトークン数が最小閾値（Sonnet: 1024）未満 | システムプロンプトやツール定義を含めてプレフィックスを1024トークン以上にする |
| 2回目のリクエストでもキャッシュミス | ツール定義のJSONキー順序が言語/ライブラリにより変動 | ツール定義のキー順序を固定する（Pythonの`dict`は3.7+で順序保証あり） |
| 1時間キャッシュが期待より高コスト | 5分以内に再利用されるプロンプトに1時間TTLを設定 | 再利用間隔を計測し、5分以内なら5分TTLに変更 |
| 画像追加後にキャッシュが全無効化 | 画像の有無変更はメッセージキャッシュを無効化 | 画像を含むリクエストと含まないリクエストを分離する |
| `cache_read_input_tokens`が`usage`に含まれない | 古いSDKバージョンを使用 | `anthropic>=0.40`にアップデート |

## まとめと次のステップ

**まとめ:**

- Claude Sonnet 4.6のプロンプトキャッシュは、**自動キャッシュ**（1パラメータ追加）と**明示的ブレークポイント**（最大4つ）の2方式を提供し、用途に応じた使い分けが可能
- LangGraphのAgentic RAGに統合する際は、`AnthropicPromptCachingMiddleware`を使う方法と、カスタムStateGraphで4層ブレークポイントを配置する方法がある
- 「Don't Break the Cache」論文の知見に基づき、**静的コンテンツを先頭に配置**し、**ツール定義を安定させる**ことがキャッシュ効率の鍵
- キャッシュヒット率80%以上を目標にモニタリングし、LangGraphのノードレベルキャッシュとの2層構成で更なるコスト削減が可能

**次にやるべきこと:**

- 既存のLangGraph RAGパイプラインに`cache_control={"type": "ephemeral"}`を追加し、`usage`フィールドでキャッシュ効果を計測する
- キャッシュヒット率が低い場合は、プロンプト構造を見直し、動的コンテンツを末尾に移動する
- [Anthropic公式のPrompt Cachingクックブック](https://platform.claude.com/cookbook/misc-prompt-caching)でより詳細な実装パターンを確認する

**関連記事:**

- [LangGraph Agentic RAGで社内検索の回答精度を78%改善する実装手法](https://zenn.dev/0h_n0/articles/4c869d366e5200)
- [LangGraph×Claude Sonnet 4.6で実装する階層的Agentic RAG検索パイプライン](https://zenn.dev/0h_n0/articles/a4cd3a7f1cf4ce)
- [LangGraphエージェント型RAGのレイテンシ最適化：ストリーミング×非同期実行で応答速度を3倍改善する](https://zenn.dev/0h_n0/articles/433702e83b26ed)

## 関連する深掘り記事

本記事で扱ったプロンプトキャッシュ最適化の技術的背景を、1次情報（arXiv論文・企業テックブログ・カンファレンス論文）から詳しく解説した記事です。

- [論文解説: Don't Break the Cache — プロンプトキャッシュヒット率を最大化する設計原則（arXiv:2601.06007）](https://0h-n0.github.io/posts/paper-2601-06007/)
- [論文解説: Prompt Cache — Modular Attention Reuseによるプロンプト推論高速化（arXiv:2311.04934）](https://0h-n0.github.io/posts/paper-2311-04934/)
- [論文解説: RAGCache — RAG向けKVキャッシュでスループット4倍改善（arXiv:2401.02038）](https://0h-n0.github.io/posts/paper-2401-02038/)
- [NVIDIA TensorRT-LLM解説: KV Cache Reuseによる推論最適化と優先度ベースEviction](https://0h-n0.github.io/posts/techblog-nvidia-kv-cache-reuse-tensorrt-llm/)
- [論文解説: vLLM PagedAttention — OS仮想メモリ方式でKVキャッシュ管理を革新（SOSP 2023）](https://0h-n0.github.io/posts/conf-vllm-pagedattention/)

## 参考

- [Prompt caching - Claude API Docs（公式ドキュメント）](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
- [Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks（arxiv:2601.06007）](https://arxiv.org/abs/2601.06007)
- [Anthropic Says Cache Misses Are Production Incidents](https://www.implicator.ai/anthropic-says-cache-misses-are-production-incidents-reveals-caching-shaped-claude-code/)
- [Anthropic middleware integration - LangChain Docs](https://docs.langchain.com/oss/python/integrations/middleware/anthropic)
- [Node-level Caching in LangGraph](https://www.analyticsvidhya.com/blog/2025/10/caching-in-langgraph/)
- [Build a custom RAG agent with LangGraph](https://docs.langchain.com/oss/python/langgraph/agentic-rag)
- [Prompt caching with Claude（Anthropic公式ブログ）](https://www.anthropic.com/news/prompt-caching)

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
