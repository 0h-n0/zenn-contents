---
title: "LLMバッチ処理最適化：APIコスト50%削減と推論スループット23倍を実現する実践ガイド"
emoji: "⚡"
type: "tech"
topics: ["llm", "python", "openai", "anthropic", "vllm"]
published: false
---

# LLMバッチ処理最適化：APIコスト50%削減と推論スループット23倍を実現する実践ガイド

## この記事でわかること

- OpenAI Batch API・Anthropic Message Batches APIを使った**50%コスト削減**の実装方法
- vLLMのContinuous Batchingで**推論スループットを最大23倍**に向上させる手法
- Prompt Caching × Batch APIの組み合わせで**さらなるコスト削減**を達成するテクニック
- 用途別のバッチ処理戦略の選び方

## 対象読者

- **想定読者**: 中級〜上級のLLMアプリケーション開発者
- **必要な前提知識**:
  - Python 3.11+の基礎（async/await構文を含む）
  - OpenAIまたはAnthropic APIの基本的な利用経験
- **動作確認環境**: anthropic SDK 0.79+, vLLM 0.16.x（2026年2月時点）

## 結論・成果

LLMのバッチ処理を最適化することで、**APIコストを50%削減**しつつ、自前推論ではvLLMのPagedAttentionにより**スループットを最大23倍**（OPT-13B, A100での測定値）まで向上できます。Prompt Cachingの併用でさらに追加コスト削減が見込めます。

## API Batch処理でコストを半減させる

大量のLLMリクエストを処理する場面で、最も即効性があるのが**プロバイダ提供のBatch API**です。OpenAI・Anthropicはいずれもバッチリクエストに対して**通常料金の50%**で処理するAPIを提供しています。

### Anthropic Message Batches APIを実装する

Anthropicの Message Batches APIは最大**100,000リクエスト**を1バッチで処理でき、多くが**1時間以内**に完了します。

```python
# batch_processor.py
import anthropic
import time
from anthropic.types.message_create_params import MessageCreateParamsNonStreaming
from anthropic.types.messages.batch_create_params import Request

client = anthropic.Anthropic()

def create_batch(prompts: list[str], model: str = "claude-sonnet-5-20260203") -> str:
    """プロンプトリストからバッチジョブを作成"""
    requests = [
        Request(
            custom_id=f"req-{i:06d}",
            params=MessageCreateParamsNonStreaming(
                model=model, max_tokens=1024,
                messages=[{"role": "user", "content": prompt}],
            ),
        )
        for i, prompt in enumerate(prompts)
    ]
    return client.messages.batches.create(requests=requests).id

def wait_and_collect(batch_id: str) -> dict[str, str]:
    """バッチ完了までポーリングし、結果を収集"""
    while True:
        batch = client.messages.batches.retrieve(batch_id)
        if batch.processing_status == "ended":
            break
        time.sleep(30)

    results = {}
    for entry in client.messages.batches.results(batch_id):
        if entry.result.type == "succeeded":
            results[entry.custom_id] = entry.result.message.content[0].text
    return results
```

**なぜBatch APIを選ぶのか:**
- 同じモデル・同じ品質の出力で**コストが半額**
- 個別リクエストのレート制限とは独立しており、大量処理に向く

> **注意**: Batch APIは非同期処理のため、リアルタイム応答が必要な用途には適しません。即時応答が必要な場合はasyncioベースの並列処理（`AsyncAnthropic` + `asyncio.Semaphore`）を検討してください。

### OpenAI Batch APIとの比較

| 項目 | Anthropic Message Batches | OpenAI Batch API |
|------|---------------------------|------------------|
| **コスト削減** | 50% | 50% |
| **最大バッチサイズ** | 100,000リクエスト or 256MB | 50,000行 or 200MB |
| **処理時間** | 多くは1時間以内 | 最大24時間 |
| **Prompt Caching** | 対応（ヒット率30-98%） | 対応 |

### Prompt Cachingを組み合わせてさらに削減する

共通のシステムプロンプトを持つバッチでは、**Prompt Caching**との併用が有効です。`cache_control`を全リクエストで統一することでキャッシュヒット率が向上します。

```python
# 共通コンテキストにcache_controlを指定
system = [
    {"type": "text", "text": "あなたはレビュー分析AIです。"},
    {
        "type": "text",
        "text": "<商品カタログ全文 ... 数万トークンの共通コンテキスト>",
        "cache_control": {"type": "ephemeral"},  # ← キャッシュ対象
    },
]
# このsystemを全バッチリクエストのparamsに渡す
```

Batch APIの50%割引はキャッシュトークンにも適用されるため、共通コンテキストが大きいほど効果が高まります。ただしバッチ内は並列処理されるため、**キャッシュヒット率は30%〜98%**と幅がある点に注意してください。

## vLLM Continuous Batchingで自前推論を高速化する

自前のGPUでLLMを推論する場合、**vLLM**のContinuous Batchingが業界標準です。従来のstatic batchingでは全リクエストの完了を待つ必要がありましたが、Continuous Batchingはイテレーション単位でスケジューリングし、完了したスロットに即座に新リクエストを投入します。

### vLLMサーバーを構築する

```bash
# vLLMのインストールと起動（Python 3.11+, CUDA 12.x）
pip install vllm

# OpenAI互換APIサーバーとして起動
vllm serve meta-llama/Llama-3.3-70B-Instruct \
  --tensor-parallel-size 2 \
  --max-model-len 8192 \
  --enable-chunked-prefill \
  --max-num-batched-tokens 16384
```

| パラメータ | 効果 | 推奨値 |
|-----------|------|--------|
| `--tensor-parallel-size` | GPUの並列数 | GPU枚数に合わせる |
| `--max-num-batched-tokens` | 1イテレーションの最大トークン数 | VRAM依存（A100 80GBなら16384〜） |
| `--enable-chunked-prefill` | 長いプロンプトを分割しGPU利用率を維持 | 常に有効推奨 |

### PagedAttentionがスループットを向上させる仕組み

vLLMの核心技術**PagedAttention**は、KVキャッシュをOSの仮想メモリのように固定サイズブロックで管理します。従来の連続メモリ割り当てでは最大シーケンス長分のメモリを事前確保する必要がありましたが、PagedAttentionではブロック単位で動的割り当てするため、メモリの無駄がほぼゼロになります。

**最初はstatic batchingで十分だと考えていましたが**、リクエスト長にばらつきがある実運用データでは短いリクエストが長いリクエストの完了を待つ「パディング問題」でスループットが大幅に低下します。Continuous Batchingに切り替えることで、同じGPUリソースで**スループットが数倍〜最大23倍**改善します（改善幅はモデルサイズやリクエスト長の分散に依存）。

## よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| バッチが24時間後に期限切れ | リクエスト量過多・需要集中 | バッチを分割（1万件ずつ）して並列投入 |
| Prompt Cacheのヒット率が低い | リクエスト順序がバラバラ | `cache_control`ブロックを全リクエストで統一 |
| vLLMでOOMエラー | `max-num-batched-tokens`が大きすぎる | 値を半分にして再試行 |
| バッチ結果の順序が入力と異なる | Batch APIの仕様（順序非保証） | `custom_id`で結果をマッピング |

## まとめと次のステップ

**まとめ:**
- **API Batch処理**（OpenAI・Anthropic）で即座に**50%コスト削減**を実現できる
- **Prompt Caching**との併用で共通コンテキストのコストをさらに削減可能
- **vLLM Continuous Batching**は自前推論のスループットを最大23倍に引き上げる（[Anyscale測定](https://www.anyscale.com/blog/continuous-batching-llm-inference)、OPT-13B/A100基準）
- 用途に応じてBatch API・asyncio並列処理・vLLMを組み合わせるのが最適解

**次にやるべきこと:**
- まずは既存のLLM処理をBatch APIに置き換え、コスト削減効果を計測する
- 自前推論が必要な場合はvLLMの`--enable-chunked-prefill`から始める

## 参考

- [Anthropic Batch Processing公式ドキュメント](https://platform.claude.com/docs/en/build-with-claude/batch-processing)
- [OpenAI Batch API Cookbook](https://developers.openai.com/cookbook/examples/batch_processing)
- [vLLM公式ドキュメント](https://docs.vllm.ai/en/latest/)
- [Continuous Batching解説（Hugging Face）](https://huggingface.co/blog/continuous_batching)
- [Continuous Batchingベンチマーク（Anyscale）](https://www.anyscale.com/blog/continuous-batching-llm-inference)
- [vLLMアーキテクチャ解説（vLLM Blog）](https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html)

詳細なリサーチ内容は [Issue #137](https://github.com/0h-n0/zen-auto-create-article/issues/137) を参照してください。

---

## 関連する深掘り記事

この記事で紹介した技術について、さらに深掘りした記事を書きました：

- [論文解説: PagedAttention — KVキャッシュ管理でLLM推論スループット最大4倍](https://0h-n0.github.io/posts/paper-2309-06180/) - arxiv解説
- [論文解説: Orca — Continuous Batchingの原論文でスループット36.9倍](https://0h-n0.github.io/posts/paper-2209-01188/) - arxiv解説
- [論文解説: Prompt Cache — モジュラーKVキャッシュ再利用でTTFT 3.5倍高速化](https://0h-n0.github.io/posts/paper-2311-04934/) - arxiv解説
- [論文解説: SGLang — RadixAttentionで構造化LLMプログラムを最大5倍高速化](https://0h-n0.github.io/posts/paper-2312-07104/) - arxiv解説
- [論文解説: Sarathi — Chunked PrefillとDecode Piggybackingでレイテンシ74%削減](https://0h-n0.github.io/posts/paper-2308-16369/) - arxiv解説

:::message
これらの記事は修士学生レベルを想定した技術的詳細（数式・実装の深掘り）を含みます。
:::

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
