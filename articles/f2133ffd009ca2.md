---
title: "LangGraphÃ—Gemini 3.1 Proã§å®Ÿè£…ã™ã‚‹éšå±¤çš„AgenticRAGæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"
emoji: "ğŸ”"
type: "tech"
topics: ["langgraph", "gemini", "rag", "python", "llm"]
published: false
---

# LangGraphÃ—Gemini 3.1 Proã§å®Ÿè£…ã™ã‚‹éšå±¤çš„AgenticRAGæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LangGraphã¨Gemini 3.1 Proã‚’çµ„ã¿åˆã‚ã›ãŸAgenticRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆã¨å®Ÿè£…æ–¹æ³•
- A-RAGè«–æ–‡ã«åŸºã¥ã**éšå±¤çš„æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãƒ»ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãƒ»ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼‰ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- Gemini 3.1 Proã®1Mãƒˆãƒ¼ã‚¯ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨`thinking_level`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸã‚³ã‚¹ãƒˆãƒ»ç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®åˆ¶å¾¡æ‰‹æ³•
- æ¡ä»¶ä»˜ããƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªåˆ†é¡ã¨ãƒãƒ«ãƒãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã®å‹•çš„é¸æŠ
- æœ¬ç•ªé‹ç”¨ã‚’æƒ³å®šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¸ã‚§ãƒƒãƒˆç®¡ç†ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­è¨ˆ

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šã€œä¸Šç´šã®Pythonã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãƒ»é‹ç”¨çµŒé¨“ãŒã‚ã‚‹æ–¹
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.11ä»¥ä¸Šã®åŸºæœ¬æ–‡æ³•
  - LangChain / LangGraphã®åŸºæœ¬æ¦‚å¿µï¼ˆStateã€Nodeã€Edgeï¼‰
  - RAGï¼ˆRetrieval-Augmented Generationï¼‰ã®åŸºæœ¬ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
  - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ï¼ˆEmbeddingã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã®åŸºç¤ç†è§£

## çµè«–ãƒ»æˆæœ

æœ¬è¨˜äº‹ã§å®Ÿè£…ã™ã‚‹éšå±¤çš„AgenticRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ã€A-RAGè«–æ–‡ï¼ˆarXiv: 2602.03442ï¼‰ã®è¨­è¨ˆæ€æƒ³ã‚’LangGraphã§å†ç¾ã—ã€Gemini 3.1 Proã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚A-RAGè«–æ–‡ã®è‘—è€…ã‚‰ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯ã€éšå±¤çš„æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ã‚ˆã‚Š**HotpotQAã§94.5%ã€2WikiMultiHopQAã§89.7%ã®LLM-Acc**ã‚’é”æˆã—ã€å¾“æ¥ã®å˜ä¸€æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦åŒç­‰ä»¥ä¸‹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ã§ç²¾åº¦ãŒå‘ä¸Šã—ãŸã¨å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

Gemini 3.1 Proã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³å˜ä¾¡ãŒ**$2.00/1Mãƒˆãƒ¼ã‚¯ãƒ³**ã¨ã€åŒç­‰æ€§èƒ½å¸¯ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒé«˜ãã€1Mãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã‚ˆã‚Šå¤§é‡ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸€åº¦ã«å‡¦ç†ã§ãã¾ã™ã€‚ã•ã‚‰ã«`thinking_level`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆLow / Medium / Highï¼‰ã‚’æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºã”ã¨ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã§ã€ã‚³ã‚¹ãƒˆã¨æ¨è«–å“è³ªã®ãƒãƒ©ãƒ³ã‚¹ã‚’å‹•çš„ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚

:::message
æœ¬è¨˜äº‹ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã¯è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®è§£èª¬ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã®åˆ©ç”¨æ™‚ã¯ã€Gemini APIãŠã‚ˆã³LangGraphã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æœ€æ–°ã®APIä»•æ§˜ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
:::

## éšå±¤çš„æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¨­è¨ˆã™ã‚‹

AgenticRAGã§ã¯ã€LLMãŒæ¤œç´¢æˆ¦ç•¥ã‚’è‡ªå¾‹çš„ã«é¸æŠã—ã¾ã™ã€‚A-RAGè«–æ–‡ï¼ˆDu et al., 2026ï¼‰ã§ã¯ã€3ã¤ã®ç²’åº¦ã®ç•°ãªã‚‹æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å…¬é–‹ã—ã€ã‚¯ã‚¨ãƒªç‰¹æ€§ã«å¿œã˜ãŸé©å¿œçš„ãªæ¤œç´¢ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚ã“ã®è¨­è¨ˆæ€æƒ³ã‚’LangGraphã®ã‚°ãƒ©ãƒ•æ§‹é€ ã«è½ã¨ã—è¾¼ã‚“ã§ã¿ã¾ã—ã‚‡ã†ã€‚

### 3ã¤ã®æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®å½¹å‰²

A-RAGãŒææ¡ˆã™ã‚‹éšå±¤çš„æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ã€ä»¥ä¸‹ã®3å±¤ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚

| ãƒ„ãƒ¼ãƒ« | ç²’åº¦ | ç”¨é€” | æ¤œç´¢æ–¹å¼ |
|--------|------|------|----------|
| **keyword_search** | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ« | å›ºæœ‰åè©ãƒ»æŠ€è¡“ç”¨èªã®æ­£ç¢ºãªãƒãƒƒãƒãƒ³ã‚° | å­—å¥ä¸€è‡´ï¼ˆBM25ãƒ™ãƒ¼ã‚¹ï¼‰ |
| **semantic_search** | æ–‡ãƒ¬ãƒ™ãƒ« | æ¦‚å¿µçš„ãªé¡ä¼¼æ€§ã«åŸºã¥ãæ¤œç´¢ | Embeddingã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ |
| **chunk_read** | ãƒãƒ£ãƒ³ã‚¯ãƒ¬ãƒ™ãƒ« | ç‰¹å®šãƒãƒ£ãƒ³ã‚¯ã®å…¨æ–‡èª­ã¿è¾¼ã¿+éš£æ¥ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ | IDæŒ‡å®šã«ã‚ˆã‚‹ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ |

**ãªãœ3å±¤æ§‹é€ ãªã®ã‹:**

å˜ä¸€ã®æ¤œç´¢æ–¹å¼ã§ã¯ã€ã‚¯ã‚¨ãƒªã®æ€§è³ªã«ã‚ˆã£ã¦ç²¾åº¦ãŒå¤§ããå¤‰å‹•ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ŒTransformerã®ææ¡ˆè€…ã¯èª°ã‹ã€ã¨ã„ã†äº‹å®Ÿè³ªå•ã«ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ãŒæœ‰åŠ¹ã§ã™ãŒã€ã€ŒAttentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ©ç‚¹ã€ã®ã‚ˆã†ãªæ¦‚å¿µçš„ãªè³ªå•ã«ã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ãŒé©ã—ã¦ã„ã¾ã™ã€‚A-RAGè«–æ–‡ã®è‘—è€…ã‚‰ã®å®Ÿé¨“ã§ã¯ã€ã“ã®3å±¤æ§‹é€ ã«ã‚ˆã‚Š**ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã”ã¨ã®ç²¾åº¦ã®ã°ã‚‰ã¤ããŒä½æ¸›ã—ãŸ**ã¨å ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

**æ³¨æ„ç‚¹:**
> 3ã¤ã®ãƒ„ãƒ¼ãƒ«ã™ã¹ã¦ã‚’æ¯å›ä½¿ç”¨ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ**ã‚¯ã‚¨ãƒªç‰¹æ€§ã«åŸºã¥ã„ã¦å‹•çš„ã«é¸æŠ**ã™ã‚‹ç‚¹ãŒé‡è¦ã§ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®ã¿ã§ååˆ†ãªå ´åˆã«ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å¼·åˆ¶ã™ã‚‹ã¨ã€ä¸è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¢—åŠ ã‚’æ‹›ãã¾ã™ã€‚

### LangGraphã§ã®ãƒ„ãƒ¼ãƒ«å®šç¾©

ã¾ãšã€3ã¤ã®æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’LangChainã®ãƒ„ãƒ¼ãƒ«ã¨ã—ã¦å®šç¾©ã—ã¾ã™ã€‚

```python
# tools.py
from langchain_core.tools import tool
from pydantic import BaseModel, Field


class KeywordSearchInput(BaseModel):
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒ"""
    keywords: list[str] = Field(
        description="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆå›ºæœ‰åè©ãƒ»æŠ€è¡“ç”¨èªï¼‰"
    )
    max_results: int = Field(default=10, description="æœ€å¤§å–å¾—ä»¶æ•°")


class SemanticSearchInput(BaseModel):
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒ"""
    query: str = Field(description="è‡ªç„¶è¨€èªã®æ¤œç´¢ã‚¯ã‚¨ãƒª")
    top_k: int = Field(default=5, description="ä¸Šä½Kä»¶ã‚’å–å¾—")


class ChunkReadInput(BaseModel):
    """ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã®å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒ"""
    chunk_ids: list[str] = Field(description="èª­ã¿è¾¼ã‚€ãƒãƒ£ãƒ³ã‚¯IDã®ãƒªã‚¹ãƒˆ")
    include_adjacent: bool = Field(
        default=True, description="éš£æ¥ãƒãƒ£ãƒ³ã‚¯ã‚‚å«ã‚ã‚‹ã‹"
    )


@tool("keyword_search", args_schema=KeywordSearchInput)
def keyword_search(keywords: list[str], max_results: int = 10) -> str:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å®Œå…¨ä¸€è‡´ãƒ»éƒ¨åˆ†ä¸€è‡´ã§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã™ã‚‹ã€‚
    å›ºæœ‰åè©ã‚„æŠ€è¡“ç”¨èªã®æ­£ç¢ºãªãƒãƒƒãƒãƒ³ã‚°ã«é©ã—ã¦ã„ã‚‹ã€‚"""
    # BM25ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
    results = bm25_retriever.invoke(
        " ".join(keywords), top_k=max_results
    )
    return _format_search_results(results)


@tool("semantic_search", args_schema=SemanticSearchInput)
def semantic_search(query: str, top_k: int = 5) -> str:
    """Embeddingãƒ™ãƒ¼ã‚¹ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    æ¦‚å¿µçš„ãªé¡ä¼¼æ€§ã«åŸºã¥ãæ¤œç´¢ã«é©ã—ã¦ã„ã‚‹ã€‚"""
    results = vector_retriever.invoke(query, top_k=top_k)
    return _format_search_results(results)


@tool("chunk_read", args_schema=ChunkReadInput)
def chunk_read(
    chunk_ids: list[str], include_adjacent: bool = True
) -> str:
    """æŒ‡å®šã—ãŸãƒãƒ£ãƒ³ã‚¯IDã®å…¨æ–‡ã‚’èª­ã¿è¾¼ã‚€ã€‚
    éš£æ¥ãƒãƒ£ãƒ³ã‚¯ã‚’å«ã‚ã‚‹ã“ã¨ã§å‰å¾Œã®æ–‡è„ˆã‚‚å–å¾—ã§ãã‚‹ã€‚"""
    chunks = []
    for cid in chunk_ids:
        chunk = chunk_store.get(cid)
        if chunk and include_adjacent:
            chunks.extend(chunk_store.get_adjacent(cid, window=1))
        elif chunk:
            chunks.append(chunk)
    return _format_chunks(chunks)


def _format_search_results(results: list) -> str:
    """æ¤œç´¢çµæœã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
    formatted = []
    for i, doc in enumerate(results):
        formatted.append(
            f"[{i+1}] chunk_id={doc.metadata['chunk_id']}\n"
            f"    score={doc.metadata.get('score', 'N/A')}\n"
            f"    content: {doc.page_content[:200]}..."
        )
    return "\n".join(formatted)


def _format_chunks(chunks: list) -> str:
    """ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
    return "\n---\n".join(
        f"[chunk_id={c.metadata['chunk_id']}]\n{c.page_content}"
        for c in chunks
    )
```

**å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ:**

- å„ãƒ„ãƒ¼ãƒ«ã«Pydanticã‚¹ã‚­ãƒ¼ãƒã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€Gemini 3.1 Proã®ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«æ™‚ã«å‹å®‰å…¨ãªå¼•æ•°ã‚’ç”Ÿæˆã•ã›ã¾ã™
- `keyword_search`ã¨`semantic_search`ã¯æ¤œç´¢çµæœã®ã‚µãƒãƒªã‚’è¿”ã—ã€`chunk_read`ã¯å…¨æ–‡ã‚’è¿”ã™ã¨ã„ã†**æƒ…å ±ç²’åº¦ã®å·®**ã‚’æ„è­˜ã—ã¦ã„ã¾ã™
- `_format_search_results`ã§`chunk_id`ã‚’å«ã‚ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¾Œç¶šã®`chunk_read`ã§IDã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™

## Gemini 3.1 Proã§AgenticRAGã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹

LangGraphã®StateGraphã‚’ä½¿ã„ã€æ¤œç´¢â†’è©•ä¾¡â†’å›ç­”ç”Ÿæˆã®ãƒ«ãƒ¼ãƒ—ã‚’çµ„ã¿ç«‹ã¦ã¾ã™ã€‚Gemini 3.1 Proã®`thinking_level`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦ã€å„ãƒãƒ¼ãƒ‰ã§ã®æ¨è«–ã‚³ã‚¹ãƒˆã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

### çŠ¶æ…‹å®šç¾©ã¨ã‚°ãƒ©ãƒ•ã®å…¨ä½“è¨­è¨ˆ

```python
# graph.py
from typing import Annotated, Literal
from pydantic import BaseModel, Field
from langchain_core.messages import BaseMessage
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode


class RetrievalState(BaseModel):
    """AgenticRAGãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®çŠ¶æ…‹"""
    messages: Annotated[list[BaseMessage], add_messages]
    query: str = ""
    retrieved_chunk_ids: set[str] = Field(default_factory=set)
    retrieval_count: int = 0
    max_retrievals: int = 5
    token_budget: int = 100_000
    tokens_used: int = 0


class DocumentGrade(BaseModel):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–¢é€£æ€§ã®è©•ä¾¡çµæœ"""
    is_relevant: bool = Field(
        description="å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒã‚¯ã‚¨ãƒªã«é–¢é€£ã—ã¦ã„ã‚‹ã‹"
    )
    reasoning: str = Field(
        description="åˆ¤æ–­ã®æ ¹æ‹ "
    )
```

å…¨ä½“ã®ã‚°ãƒ©ãƒ•æ§‹é€ ã¯æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```mermaid
graph TD
    A[START] --> B[generate_query_or_respond]
    B -->|ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚ã‚Š| C[retrieve]
    B -->|ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãªã—| G[END]
    C --> D[grade_documents]
    D -->|é–¢é€£æ€§ã‚ã‚Š| E[generate_answer]
    D -->|é–¢é€£æ€§ãªã— & ãƒªãƒˆãƒ©ã‚¤å¯| F[rewrite_question]
    D -->|ãƒªãƒˆãƒ©ã‚¤ä¸Šé™| E
    E --> G
    F --> B
```

### Gemini 3.1 Proãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–

```python
# model.py
from langchain_google_genai import ChatGoogleGenerativeAI

# ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ„ãƒ¼ãƒ«é¸æŠç”¨ï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰
router_model = ChatGoogleGenerativeAI(
    model="gemini-3.1-pro",
    temperature=0.0,
    max_retries=2,
    thinking={"thinking_level": "low"},  # ä½ã‚³ã‚¹ãƒˆæ¨è«–
)

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©•ä¾¡ç”¨ï¼ˆä¸­ç¨‹åº¦ã®ã‚³ã‚¹ãƒˆï¼‰
grader_model = ChatGoogleGenerativeAI(
    model="gemini-3.1-pro",
    temperature=0.0,
    thinking={"thinking_level": "medium"},  # ãƒãƒ©ãƒ³ã‚¹é‡è¦–
)

# æœ€çµ‚å›ç­”ç”Ÿæˆç”¨ï¼ˆé«˜å“è³ªæ¨è«–ï¼‰
generator_model = ChatGoogleGenerativeAI(
    model="gemini-3.1-pro",
    temperature=0.3,
    thinking={"thinking_level": "high"},  # é«˜å“è³ªæ¨è«–
    max_output_tokens=4096,
)
```

**ãªãœthinking_levelã‚’ä½¿ã„åˆ†ã‘ã‚‹ã®ã‹:**

Gemini 3.1 Proã§ã¯`thinking_level`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ¨è«–é‡ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚å…¨ãƒãƒ¼ãƒ‰ã§`high`ã‚’ä½¿ã†ã¨ã‚³ã‚¹ãƒˆãŒä¸å¿…è¦ã«å¢—åŠ ã—ã¾ã™ã€‚

| ãƒãƒ¼ãƒ‰ | thinking_level | ç†ç”± |
|--------|----------------|------|
| ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚° | low | ãƒ„ãƒ¼ãƒ«é¸æŠã¯æ¯”è¼ƒçš„å˜ç´”ãªåˆ¤æ–­ |
| ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©•ä¾¡ | medium | é–¢é€£æ€§åˆ¤å®šã¯ä¸­ç¨‹åº¦ã®æ¨è«–ãŒå¿…è¦ |
| å›ç­”ç”Ÿæˆ | high | è¤‡æ•°ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’çµ±åˆã™ã‚‹è¤‡é›‘ãªæ¨è«– |

Googleå…¬å¼ãƒ–ãƒ­ã‚°ã«ã‚ˆã‚‹ã¨ã€`thinking_level`ã‚’é©åˆ‡ã«ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã§ã€åŒä¸€å“è³ªã®å‡ºåŠ›ã‚’ã‚ˆã‚Šä½ã‚³ã‚¹ãƒˆã§å¾—ã‚‰ã‚Œã‚‹è¨­è¨ˆã«ãªã£ã¦ã„ã¾ã™ã€‚

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ:**
> `langchain-google-genai` 4.0.0ä»¥é™ã§ã¯ã€å†…éƒ¨SDKãŒ`google-ai-generativelanguage`ã‹ã‚‰çµ±åˆ`google-genai` SDKã«å¤‰æ›´ã•ã‚Œã¦ã„ã¾ã™ã€‚å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆ3.xç³»ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€`thinking`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒèªè­˜ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚`pip install --upgrade langchain-google-genai`ã§æœ€æ–°ç‰ˆã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚

### å„ãƒãƒ¼ãƒ‰ã®å®Ÿè£…

```python
# nodes.py
from langchain_core.messages import HumanMessage, SystemMessage

# æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ä¸€è¦§
retrieval_tools = [keyword_search, semantic_search, chunk_read]

# ãƒ„ãƒ¼ãƒ«é¸æŠç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ„ãƒ¼ãƒ«ã‚’ãƒã‚¤ãƒ³ãƒ‰ï¼‰
tool_bound_model = router_model.bind_tools(retrieval_tools)

ROUTER_SYSTEM_PROMPT = """ã‚ãªãŸã¯æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€
ä»¥ä¸‹ã®3ã¤ã®æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’é©åˆ‡ã«é¸æŠã—ã¦æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„ã€‚

## ãƒ„ãƒ¼ãƒ«é¸æŠã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³
- keyword_search: å›ºæœ‰åè©ãƒ»æŠ€è¡“ç”¨èªãƒ»æ—¥ä»˜ãªã©æ­£ç¢ºãªãƒãƒƒãƒãŒå¿…è¦ãªå ´åˆ
- semantic_search: æ¦‚å¿µãƒ»æ„å‘³çš„ã«è¿‘ã„æƒ…å ±ã‚’æ¢ã™å ´åˆ
- chunk_read: ã™ã§ã«å–å¾—ã—ãŸchunk_idã®å…¨æ–‡ã‚’èª­ã‚€å ´åˆ

## åˆ¤æ–­åŸºæº–
- ååˆ†ãªæƒ…å ±ãŒé›†ã¾ã£ãŸã‚‰ã€ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã°ãšã«ç›´æ¥å›ç­”ã—ã¦ãã ã•ã„
- æ¤œç´¢å›æ•°ã®ä¸Šé™ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼ˆæ®‹ã‚Š: {remaining_retrievals}å›ï¼‰
- åŒã˜chunk_idã®é‡è¤‡èª­ã¿è¾¼ã¿ã¯é¿ã‘ã¦ãã ã•ã„
"""


def generate_query_or_respond(state: RetrievalState):
    """è³ªå•ã«å¯¾ã—ã¦ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’è¡Œã†ã‹ã€ç›´æ¥å›ç­”ã™ã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹"""
    remaining = state.max_retrievals - state.retrieval_count
    system_msg = SystemMessage(
        content=ROUTER_SYSTEM_PROMPT.format(
            remaining_retrievals=remaining
        )
    )
    response = tool_bound_model.invoke(
        [system_msg] + state.messages
    )
    return {"messages": [response]}


def grade_documents(
    state: RetrievalState,
) -> Literal["generate_answer", "rewrite_question"]:
    """å–å¾—ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã€æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®šã™ã‚‹"""
    question = state.query or state.messages[0].content
    # æœ€å¾Œã®ãƒ„ãƒ¼ãƒ«çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
    last_tool_msg = state.messages[-1]
    context = last_tool_msg.content

    grade_prompt = f"""ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒè³ªå•ã«å¯¾ã—ã¦é–¢é€£æ€§ãŒã‚ã‚‹ã‹è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {question}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

é–¢é€£æ€§ãŒã‚ã‚‹å ´åˆã¯is_relevant=Trueã€ãªã„å ´åˆã¯Falseã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"""

    result = grader_model.with_structured_output(
        DocumentGrade
    ).invoke([HumanMessage(content=grade_prompt)])

    if result.is_relevant:
        return "generate_answer"

    # ãƒªãƒˆãƒ©ã‚¤ä¸Šé™ãƒã‚§ãƒƒã‚¯
    if state.retrieval_count >= state.max_retrievals:
        return "generate_answer"  # ä¸Šé™ã«é”ã—ãŸã‚‰ç¾åœ¨ã®æƒ…å ±ã§å›ç­”

    return "rewrite_question"


def rewrite_question(state: RetrievalState):
    """é–¢é€£æ€§ã®ä½ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿”ã•ã‚ŒãŸå ´åˆã€ã‚¯ã‚¨ãƒªã‚’æ›¸ãæ›ãˆã‚‹"""
    question = state.query or state.messages[0].content

    rewrite_prompt = f"""ä»¥ä¸‹ã®è³ªå•ã§ã¯ååˆ†ãªæ¤œç´¢çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚
åˆ¥ã®è¡¨ç¾ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è³ªå•ã‚’æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {question}

æ›¸ãæ›ãˆã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:
- ã‚ˆã‚Šå…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã†
- ç•°ãªã‚‹è§’åº¦ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã™ã‚‹
- ç•¥èªãŒã‚ã‚Œã°ãƒ•ãƒ«ã‚¹ãƒšãƒ«ã«å±•é–‹ã™ã‚‹"""

    response = router_model.invoke(
        [HumanMessage(content=rewrite_prompt)]
    )
    return {
        "messages": [HumanMessage(content=response.content)],
        "retrieval_count": state.retrieval_count + 1,
    }


ANSWER_SYSTEM_PROMPT = """ã‚ãªãŸã¯æ­£ç¢ºã§æœ‰ç”¨ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä¼šè©±å±¥æ­´ã«å«ã¾ã‚Œã‚‹æ¤œç´¢çµæœã‚’ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã¨ã—ã¦ä½¿ã„ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## å›ç­”ãƒ«ãƒ¼ãƒ«
- æ¤œç´¢çµæœã«åŸºã¥ã‹ãªã„æƒ…å ±ã¯ã€Œæ¤œç´¢çµæœã‹ã‚‰ã¯ç¢ºèªã§ãã¾ã›ã‚“ã§ã—ãŸã€ã¨æ˜è¨˜ã™ã‚‹
- çŸ›ç›¾ã™ã‚‹æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ä¸¡æ–¹ã®æƒ…å ±ã‚’æç¤ºã™ã‚‹
- æƒ…å ±æºã®chunk_idã‚’å¼•ç”¨ã¨ã—ã¦å«ã‚ã‚‹
"""


def generate_answer(state: RetrievalState):
    """åé›†ã—ãŸæƒ…å ±ã‚’å…ƒã«æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    system_msg = SystemMessage(content=ANSWER_SYSTEM_PROMPT)
    response = generator_model.invoke(
        [system_msg] + state.messages
    )
    return {"messages": [response]}
```

### ã‚°ãƒ©ãƒ•ã®çµ„ã¿ç«‹ã¦ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

```python
# pipeline.py
from langgraph.prebuilt import ToolNode, tools_condition

def build_hierarchical_rag_graph() -> StateGraph:
    """éšå±¤çš„AgenticRAGã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹"""
    workflow = StateGraph(RetrievalState)

    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    workflow.add_node(
        "generate_query_or_respond",
        generate_query_or_respond,
    )
    workflow.add_node(
        "retrieve",
        ToolNode(retrieval_tools),
    )
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("generate_answer", generate_answer)

    # ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
    workflow.add_edge(START, "generate_query_or_respond")

    # ãƒ«ãƒ¼ã‚¿ãƒ¼ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®æ¡ä»¶åˆ†å²
    # tools_conditionã¯ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®æœ‰ç„¡ã‚’åˆ¤å®šã™ã‚‹çµ„ã¿è¾¼ã¿é–¢æ•°
    workflow.add_conditional_edges(
        "generate_query_or_respond",
        tools_condition,
        {
            "tools": "retrieve",  # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚ã‚Š â†’ æ¤œç´¢å®Ÿè¡Œ
            END: END,             # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãªã— â†’ ç›´æ¥å›ç­”
        },
    )

    # æ¤œç´¢çµæœã®è©•ä¾¡
    workflow.add_conditional_edges(
        "retrieve",
        grade_documents,
    )

    # å›ç­”ç”Ÿæˆ â†’ çµ‚äº†
    workflow.add_edge("generate_answer", END)

    # ã‚¯ã‚¨ãƒªæ›¸ãæ›ãˆ â†’ å†åº¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    workflow.add_edge(
        "rewrite_question", "generate_query_or_respond"
    )

    return workflow.compile()


# ã‚°ãƒ©ãƒ•ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
graph = build_hierarchical_rag_graph()
```

## ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã¨ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¸ã‚§ãƒƒãƒˆç®¡ç†ã‚’å®Ÿè£…ã™ã‚‹

1Mãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã¯å¤§ããªåˆ©ç‚¹ã§ã™ãŒã€ç„¡åˆ¶é™ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¶ˆè²»ã™ã‚‹ã¨ã‚³ã‚¹ãƒˆãŒæ€¥å¢—ã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¸ã‚§ãƒƒãƒˆã«ã‚ˆã‚‹æ¶ˆè²»é‡åˆ¶å¾¡ã¨ã€`thinking_level`ã®å‹•çš„åˆ‡ã‚Šæ›¿ãˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¸ã‚§ãƒƒãƒˆãƒˆãƒ©ãƒƒã‚«ãƒ¼

```python
# budget.py
from dataclasses import dataclass, field
from datetime import datetime


@dataclass
class TokenBudgetTracker:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ã‚’è¿½è·¡ã—ã€ãƒã‚¸ã‚§ãƒƒãƒˆåˆ¶é™ã‚’ç®¡ç†ã™ã‚‹"""
    max_budget: int = 100_000
    used_input: int = 0
    used_output: int = 0
    cost_per_1m_input: float = 2.00   # Gemini 3.1 Pro
    cost_per_1m_output: float = 12.00  # Gemini 3.1 Pro
    history: list[dict] = field(default_factory=list)

    @property
    def total_used(self) -> int:
        return self.used_input + self.used_output

    @property
    def remaining(self) -> int:
        return max(0, self.max_budget - self.total_used)

    @property
    def estimated_cost_usd(self) -> float:
        input_cost = (self.used_input / 1_000_000) * self.cost_per_1m_input
        output_cost = (self.used_output / 1_000_000) * self.cost_per_1m_output
        return input_cost + output_cost

    def record(
        self, node: str, input_tokens: int, output_tokens: int
    ) -> None:
        self.used_input += input_tokens
        self.used_output += output_tokens
        self.history.append({
            "node": node,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "timestamp": datetime.now().isoformat(),
        })

    def can_proceed(self, estimated_tokens: int = 5000) -> bool:
        """æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€ã®ã«ååˆ†ãªãƒã‚¸ã‚§ãƒƒãƒˆãŒã‚ã‚‹ã‹"""
        return self.remaining >= estimated_tokens

    def get_recommended_thinking_level(self) -> str:
        """æ®‹ã‚Šãƒã‚¸ã‚§ãƒƒãƒˆã«åŸºã¥ã„ã¦thinking_levelã‚’æ¨å¥¨ã™ã‚‹"""
        usage_ratio = self.total_used / self.max_budget
        if usage_ratio < 0.3:
            return "high"
        elif usage_ratio < 0.7:
            return "medium"
        else:
            return "low"
```

**æœ€åˆã¯`thinking_level=high`ã‚’å…¨ãƒãƒ¼ãƒ‰ã«è¨­å®šã—ã¦ãƒ†ã‚¹ãƒˆã—ãŸã¨ã“ã‚ã€10å›ã®æ¤œç´¢ãƒ«ãƒ¼ãƒ—ã§ç´„15,000ãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ æ¶ˆè²»ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚** `thinking_level`ã‚’æ®µéšçš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹æ–¹å¼ã«å¤‰æ›´ã—ãŸã“ã¨ã§ã€åŒç­‰å“è³ªã®å‡ºåŠ›ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ã‚’æŠ‘ãˆã‚‹è¨­è¨ˆã¨ã—ã¦ã„ã¾ã™ã€‚

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**
`thinking_level=low`ã§ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§åˆ¤å®šã§å½é™½æ€§ãŒå¢—ãˆã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã€**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©•ä¾¡ãƒãƒ¼ãƒ‰ã¯`medium`ä»¥ä¸Šã‚’æ¨å¥¨**ã—ã¾ã™ã€‚ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒ‰ã¯å˜ç´”ãªãƒ„ãƒ¼ãƒ«é¸æŠãªã®ã§`low`ã§ååˆ†ã§ã™ã€‚

### å‹•çš„thinking_levelåˆ‡ã‚Šæ›¿ãˆã®çµ„ã¿è¾¼ã¿

```python
# adaptive_nodes.py
from langchain_google_genai import ChatGoogleGenerativeAI


def create_adaptive_model(
    tracker: TokenBudgetTracker,
    base_thinking_level: str = "medium",
) -> ChatGoogleGenerativeAI:
    """æ®‹ã‚Šãƒã‚¸ã‚§ãƒƒãƒˆã«å¿œã˜ã¦thinking_levelã‚’å‹•çš„ã«æ±ºå®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆ"""
    recommended = tracker.get_recommended_thinking_level()

    # base_thinking_levelã¨recommendedã®ä½ã„æ–¹ã‚’æ¡ç”¨
    levels = {"low": 0, "medium": 1, "high": 2}
    final_level = min(
        levels[base_thinking_level],
        levels[recommended],
    )
    level_name = {0: "low", 1: "medium", 2: "high"}[final_level]

    return ChatGoogleGenerativeAI(
        model="gemini-3.1-pro",
        temperature=0.0,
        thinking={"thinking_level": level_name},
    )
```

## æœ¬ç•ªé‹ç”¨ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¨­è¨ˆã™ã‚‹

AgenticRAGã¯è¤‡æ•°ã®å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆGemini APIã€ãƒ™ã‚¯ãƒˆãƒ«DBã€BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã«ä¾å­˜ã™ã‚‹ãŸã‚ã€éšœå®³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ãŒä¸å¯æ¬ ã§ã™ã€‚

### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥ã®å®Ÿè£…

```python
# fallback.py
import logging
from typing import Callable
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential_jitter,
    retry_if_exception_type,
)
from google.api_core.exceptions import ResourceExhausted, ServiceUnavailable

logger = logging.getLogger(__name__)


def with_retrieval_fallback(
    primary_fn: Callable,
    fallback_fn: Callable,
    node_name: str,
) -> Callable:
    """æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ©ãƒƒãƒ‘ãƒ¼

    primary_fnãŒå¤±æ•—ã—ãŸå ´åˆã«fallback_fnã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    ä¾‹: semantic_searchãŒå¤±æ•— â†’ keyword_searchã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    """
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential_jitter(initial=1, max=10),
        retry=retry_if_exception_type(
            (ResourceExhausted, ServiceUnavailable)
        ),
    )
    def _execute_with_retry(*args, **kwargs):
        return primary_fn(*args, **kwargs)

    def wrapped(*args, **kwargs):
        try:
            return _execute_with_retry(*args, **kwargs)
        except Exception as e:
            logger.warning(
                "Primary retrieval failed in %s: %s. "
                "Falling back to secondary.",
                node_name,
                str(e),
            )
            return fallback_fn(*args, **kwargs)

    return wrapped
```

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| `ResourceExhausted` (429) | Gemini APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«åˆ°é” | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•+ã‚¸ãƒƒã‚¿ã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤ï¼ˆä¸Šè¨˜ã®`tenacity`è¨­å®šï¼‰ |
| ç„¡é™ãƒ«ãƒ¼ãƒ—ï¼ˆæ¤œç´¢â†’è©•ä¾¡â†’æ›¸ãæ›ãˆâ†’æ¤œç´¢...ï¼‰ | `grade_documents`ãŒå¸¸ã«`rewrite_question`ã‚’è¿”ã™ | `max_retrievals`ã§æ¤œç´¢å›æ•°ã«ä¸Šé™ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5å›ï¼‰ |
| ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¸ã‚§ãƒƒãƒˆè¶…é | å¤§é‡ã®ãƒãƒ£ãƒ³ã‚¯ã‚’`chunk_read`ã§å–å¾— | `TokenBudgetTracker.can_proceed()`ã§äº‹å‰ãƒã‚§ãƒƒã‚¯ |
| `chunk_read`ã§åŒã˜ãƒãƒ£ãƒ³ã‚¯ã‚’ç¹°ã‚Šè¿”ã—å–å¾— | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå–å¾—æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ã‚’è¨˜æ†¶ã—ã¦ã„ãªã„ | `retrieved_chunk_ids`ï¼ˆsetå‹ï¼‰ã§å–å¾—æ¸ˆã¿IDã‚’è¿½è·¡ |
| `thinking`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒåŠ¹ã‹ãªã„ | `langchain-google-genai` 3.xç³»ã‚’ä½¿ç”¨ | 4.0.0ä»¥ä¸Šã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆï¼ˆ`pip install --upgrade langchain-google-genai`ï¼‰ |

**åˆ¶ç´„æ¡ä»¶:**
> æœ¬è¨˜äº‹ã®éšå±¤çš„æ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã€**äº‹å‰ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ãŒå®Œäº†ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³**ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ Webã‚¯ãƒ­ãƒ¼ãƒ«ã‚„å‹•çš„ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«ã¯ã€åˆ¥é€”ã‚¤ãƒ³ã‚¸ã‚§ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è¨­è¨ˆãŒå¿…è¦ã§ã™ã€‚ã¾ãŸã€Gemini 3.1 Proã®1Mã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸Šé™ã§ã‚ã‚Šã€å¤§è¦æ¨¡ãªãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ï¼ˆæ•°ç™¾ä¸‡ãƒãƒ£ãƒ³ã‚¯ï¼‰ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚

## ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œã¨æ¤œç´¢ãƒ•ãƒ­ãƒ¼ã‚’ç¢ºèªã™ã‚‹

å®Ÿè£…ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å‹•ä½œã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
# main.py
from langchain_core.messages import HumanMessage

# ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
graph = build_hierarchical_rag_graph()

# å®Ÿè¡Œ
inputs = {
    "messages": [
        HumanMessage(
            content="Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹"
            "Multi-Head Attentionã®è¨ˆç®—é‡å‰Šæ¸›æ‰‹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„"
        )
    ],
    "query": "Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹"
    "Multi-Head Attentionã®è¨ˆç®—é‡å‰Šæ¸›æ‰‹æ³•",
    "max_retrievals": 5,
    "token_budget": 100_000,
}

# ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œã§å„ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ…‹ã‚’ç¢ºèª
for event in graph.stream(inputs, stream_mode="updates"):
    for node_name, state_update in event.items():
        print(f"\n--- Node: {node_name} ---")
        if "messages" in state_update:
            last_msg = state_update["messages"][-1]
            # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
            if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                for tc in last_msg.tool_calls:
                    print(f"  Tool: {tc['name']}")
                    print(f"  Args: {tc['args']}")
            else:
                print(f"  Response: {last_msg.content[:200]}...")
```

å®Ÿè¡Œã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãªæ¤œç´¢ãƒ•ãƒ­ãƒ¼ãŒè¦³å¯Ÿã•ã‚Œã¾ã™ã€‚

```
--- Node: generate_query_or_respond ---
  Tool: keyword_search
  Args: {'keywords': ['Multi-Head Attention', 'è¨ˆç®—é‡å‰Šæ¸›', 'Transformer'], 'max_results': 10}

--- Node: retrieve ---
  [1] chunk_id=doc_042_chunk_3  score=8.7
      content: Multi-Head Attentionã®è¨ˆç®—é‡ã¯O(nÂ²d)ã§...
  [2] chunk_id=doc_089_chunk_1  score=7.2
      content: Linear Attentionã¯è¨ˆç®—é‡ã‚’O(ndÂ²)ã«å‰Šæ¸›...

--- Node: grade_documents ---
  â†’ generate_answerï¼ˆé–¢é€£æ€§ã‚ã‚Šï¼‰

--- Node: generate_answer ---
  Response: Multi-Head Attentionã®è¨ˆç®—é‡å‰Šæ¸›æ‰‹æ³•ã«ã¯ä¸»ã«ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ...
```

ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã¾ãš`keyword_search`ã§å…·ä½“çš„ãªæŠ€è¡“ç”¨èªã‚’æ¤œç´¢ã—ã€é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã£ãŸãŸã‚è¿½åŠ æ¤œç´¢ãªã—ã§å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™ã€‚**æ¦‚å¿µçš„ãªè³ªå•ï¼ˆã€ŒAttentionã®åˆ©ç‚¹ã¯ï¼Ÿã€ï¼‰ã®å ´åˆã¯`semantic_search`ãŒé¸æŠã•ã‚Œã‚‹**å‚¾å‘ãŒã‚ã‚Šã€ã‚¯ã‚¨ãƒªç‰¹æ€§ã«å¿œã˜ãŸãƒ„ãƒ¼ãƒ«é¸æŠãŒå‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã§ãã¾ã™ã€‚

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- A-RAGè«–æ–‡ã®éšå±¤çš„æ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆkeyword / semantic / chunk_readï¼‰ã‚’LangGraphã®ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã¨ã—ã¦å®Ÿè£…ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¯ã‚¨ãƒªç‰¹æ€§ã«å¿œã˜ã¦å‹•çš„ã«æ¤œç´¢æˆ¦ç•¥ã‚’é¸æŠã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ
- Gemini 3.1 Proã®`thinking_level`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆLow / Medium / Highï¼‰ã‚’ãƒãƒ¼ãƒ‰ã”ã¨ã«ä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã§ã€æ¨è«–å“è³ªã‚’ç¶­æŒã—ãªãŒã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã‚’åˆ¶å¾¡ã™ã‚‹è¨­è¨ˆã‚’ç¤ºã—ã¾ã—ãŸ
- `TokenBudgetTracker`ã«ã‚ˆã‚‹ãƒã‚¸ã‚§ãƒƒãƒˆç®¡ç†ã¨ã€æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•+ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã§ã€æœ¬ç•ªé‹ç”¨ã«å¿…è¦ãªè€éšœå®³æ€§ã‚’å®Ÿè£…ã—ã¾ã—ãŸ
- Gemini 3.1 Proã®å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³å˜ä¾¡ã¯$2.00/1Mã¨ã€åŒç­‰æ€§èƒ½å¸¯ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒé«˜ã„ç‚¹ã‚‚ã€æœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é‹ç”¨ã‚³ã‚¹ãƒˆä¸Šã®åˆ©ç‚¹ã§ã™

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- è‡ªç¤¾ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã§ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’è¡Œã„ã€`bm25_retriever`ã¨`vector_retriever`ã‚’å®Ÿè£…ã™ã‚‹
- [LangSmith](https://smith.langchain.com/)ã‚’ä½¿ã£ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¤œç´¢ãƒ‘ã‚¹ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»é‡ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã€`max_retrievals`ã¨`token_budget`ã®ã—ãã„å€¤ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹
- [RAGAS](https://docs.ragas.io/)ç­‰ã®è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§Context Precision / Answer Relevancyã‚’å®šé‡è©•ä¾¡ã—ã€æ¤œç´¢ç²¾åº¦ã®æ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã‚’å›ã™

**é–¢é€£è¨˜äº‹:**
- [LangGraphÃ—Claude Sonnet 4.6ã§å®Ÿè£…ã™ã‚‹éšå±¤çš„Agentic RAGæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³](https://zenn.dev/0h_n0/articles/a4cd3a7f1cf4ce)ï¼ˆClaude Sonnet 4.6ã‚’ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ä½¿ç”¨ã—ãŸé¡ä¼¼ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

## å‚è€ƒ

- [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfacesï¼ˆarXiv: 2602.03442ï¼‰](https://arxiv.org/abs/2602.03442)
- [Googleå…¬å¼ãƒ–ãƒ­ã‚°: Gemini 3.1 Pro](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-1-pro/)
- [LangGraphå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: Build a custom RAG agent](https://docs.langchain.com/oss/python/langgraph/agentic-rag)
- [Gemini 3.1 Pro APIãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ï¼ˆGoogle Cloud Vertex AIï¼‰](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/3-1-pro)
- [langchain-google-genai 4.0.0ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆ](https://github.com/langchain-ai/langchain-google/discussions/1422)
- [ReAct agent from scratch with Gemini and LangGraphï¼ˆGoogle AI for Developersï¼‰](https://ai.google.dev/gemini-api/docs/langgraph-example)
- [Gemini Developer API Pricing](https://ai.google.dev/gemini-api/docs/pricing)

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
