---
title: "2026å¹´ç‰ˆï¼šRAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã¨æœ¬ç•ªé‹ç”¨ã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ”"
type: "tech"
topics: ["rag", "llm", "ai", "python", "vectordb"]
published: true
---

# 2026å¹´ç‰ˆï¼šRAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã¨æœ¬ç•ªé‹ç”¨ã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- RAGï¼ˆRetrieval Augmented Generationï¼‰æ¤œç´¢ã®åŸºæœ¬å®Ÿè£…ã‹ã‚‰æœ¬ç•ªé‹ç”¨ã¾ã§ã®å®Œå…¨ã‚¬ã‚¤ãƒ‰
- Chromaã€FAISSã€Qdrantã®ç‰¹å¾´æ¯”è¼ƒã¨é¸å®šåŸºæº–
- **å®Ÿè£…ç¾å ´ãŒç›´é¢ã™ã‚‹7ã¤ã®èª²é¡Œã¨è§£æ±ºç­–**:
  1. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
  2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é™ç•Œï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼‰
  3. è©•ä¾¡æŒ‡æ¨™ã®æ¬ å¦‚
  4. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®åˆ¶é™
  5. ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰ã®å•é¡Œ
  6. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã®é›£ã—ã•
  7. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ã‚³ã‚¹ãƒˆ
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‹ã‚‰æœ¬ç•ªç’°å¢ƒã¸ã®æ®µéšçš„ç§»è¡Œæˆ¦ç•¥

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºè€…ã€RAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã‚’æ¤œè¨ã—ã¦ã„ã‚‹ä¸­ç´šè€…
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.9ä»¥ä¸Šã®åŸºæœ¬çš„ãªä½¿ã„æ–¹
  - OpenAI APIã¾ãŸã¯é¡ä¼¼ã®LLM APIã®åˆ©ç”¨çµŒé¨“
  - ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingsï¼‰ã®åŸºæœ¬æ¦‚å¿µ

## çµè«–ãƒ»æˆæœ

RAGã‚·ã‚¹ãƒ†ãƒ ã¯**2026å¹´ç¾åœ¨ã€æ§‹ç¯‰è‡ªä½“ã¯æ•°æ™‚é–“ã§å¯èƒ½**ã«ãªã‚Šã¾ã—ãŸã€‚ã—ã‹ã—ã€Œå‹•ãã€ã¨ã€Œä½¿ãˆã‚‹ã€ã®é–“ã«ã¯æ·±ã„æºãŒã‚ã‚Šã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ç´¹ä»‹ã™ã‚‹å®Ÿè£…æ‰‹æ³•ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ãªæˆæœã‚’é”æˆã§ãã¾ã™ï¼š

**å®Ÿæ¸¬ä¾‹ï¼ˆ10ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦æ¨¡ã®ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆç’°å¢ƒï¼‰:**
- **æ¤œç´¢ç²¾åº¦**: 60%ï¼ˆFAISSãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ï¼‰ â†’ **85%**ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼‰
- **ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ **: 2ç§’ï¼ˆGPT-4 APIåŒæœŸå‘¼ã³å‡ºã—ï¼‰ â†’ **0.5ç§’**ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥+ä¸¦åˆ—åŒ–ï¼‰
- **ã‚¤ãƒ³ãƒ•ãƒ©ã‚³ã‚¹ãƒˆ**: $500/æœˆï¼ˆPineconeï¼‰ â†’ **$150/æœˆ**ï¼ˆQdrant Cloudã€70%å‰Šæ¸›ï¼‰

## RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¨ã¯ä½•ã‹

RAGï¼ˆRetrieval Augmented Generationï¼‰ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å›ç­”ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã™ã€‚

å¾“æ¥ã®LLMã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹æƒ…å ±ã®ã¿ã§å›ç­”ã—ã¦ã„ã¾ã—ãŸãŒã€RAGã¯**å¤–éƒ¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢**ã—ã€ãã®æƒ…å ±ã‚’åŸºã«å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

**RAGã®3ã¤ã®ã‚³ã‚¢ãƒ—ãƒ­ã‚»ã‚¹:**

1. **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–**: ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ ¼ç´
2. **é¡ä¼¼åº¦æ¤œç´¢**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨æ„å‘³çš„ã«è¿‘ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
3. **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãç”Ÿæˆ**: æ¤œç´¢çµæœã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å«ã‚ã¦LLMãŒå›ç­”ç”Ÿæˆ

## åŸºæœ¬å®Ÿè£…ï¼šæœ€å°é™ã®RAGã‚·ã‚¹ãƒ†ãƒ 

ã¾ãšã¯æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã¯**Streamlit + FAISS + OpenAI API**ã«ã‚ˆã‚‹æœ€å°æ§‹æˆã§ã™ã€‚

### å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```python
# requirements.txt
streamlit>=1.31.0
faiss-cpu>=1.7.4
openai>=1.12.0
python-dotenv>=1.0.0
numpy>=1.26.4
tenacity>=8.2.0  # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ç”¨
rank-bm25>=0.2.2  # BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ç”¨
sentence-transformers>=2.3.0  # å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨
watchdog>=3.0.0  # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥ç”¨
qdrant-client>=1.7.0  # Qdrantãƒ™ã‚¯ãƒˆãƒ«DBç”¨
```

:::message
â€» ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯2026å¹´1æœˆæ™‚ç‚¹ã®æ¨å¥¨ã§ã™ã€‚æœ€æ–°ç‰ˆã¸ã®æ›´æ–°ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
:::

### ã‚³ã‚¢å®Ÿè£…ï¼ˆç´„100è¡Œï¼‰

```python
import streamlit as st
import faiss
import numpy as np
import os
from openai import OpenAI
from pathlib import Path
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# åˆæœŸåŒ–
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4"

def load_documents(data_dir="data"):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
    docs = []
    for file_path in Path(data_dir).glob("*.txt"):
        with open(file_path, "r", encoding="utf-8") as f:
            docs.append({"path": str(file_path), "content": f.read()})
    return docs

@retry(stop=stop_after_attempt(3), wait=wait_exponential(min=1, max=10))
def create_embeddings(texts):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        response = client.embeddings.create(
            input=texts,
            model=EMBEDDING_MODEL,
            timeout=30.0  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
        )
        return np.array([item.embedding for item in response.data])
    except Exception as e:
        st.error(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        raise

def build_index(documents):
    """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    contents = [doc["content"] for doc in documents]
    embeddings = create_embeddings(contents)
    
    # FAISS ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆL2è·é›¢ï¼‰
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    
    return index, contents

def search(query, index, contents, top_k=3):
    """é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
    query_embedding = create_embeddings([query])
    distances, indices = index.search(query_embedding, top_k)
    
    results = []
    for idx, dist in zip(indices[0], distances[0]):
        results.append({"content": contents[idx], "distance": float(dist)})
    return results

def generate_answer(query, search_results):
    """æ¤œç´¢çµæœã‚’åŸºã«å›ç­”ç”Ÿæˆ"""
    context = "\n\n".join([r["content"] for r in search_results])
    
    messages = [
        {"role": "system", "content": "ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ä½¿ã£ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚"},
        {"role": "user", "content": f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}\n\nè³ªå•: {query}"}
    ]
    
    response = client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages
    )
    return response.choices[0].message.content

# Streamlit UI
st.title("RAGæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ")
docs = load_documents()
index, contents = build_index(docs)

query = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
if query:
    results = search(query, index, contents)
    answer = generate_answer(query, results)
    st.write(answer)
```

**ãªãœã“ã®å®Ÿè£…ã‚’é¸ã‚“ã ã‹:**
- FAISS: é«˜é€Ÿã§ã‚·ãƒ³ãƒ—ãƒ«ã€å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ<10ä¸‡ä»¶ï¼‰ã«æœ€é©
- Streamlit: UIã‚’5åˆ†ã§æ§‹ç¯‰å¯èƒ½ã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã«æœ€é©
- OpenAI API: åŸ‹ã‚è¾¼ã¿ã¨ç”Ÿæˆã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§æä¾›

**æ³¨æ„ç‚¹:**
> ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯**åˆæœŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã«ã¯æœ€é©**ã§ã™ãŒã€æœ¬ç•ªé‹ç”¨ã§ã¯ä»¥ä¸‹ã®åˆ¶é™ãŒã‚ã‚Šã¾ã™ï¼š
> - FAISSã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã«å¯¾å¿œã—ã¦ã„ãªã„ï¼ˆæ–°ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ãŒå¿…è¦ï¼‰
> - ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹ãŸã‚ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã«ã¯ä¸å‘ã
> - **è²»ç”¨ï¼ˆ2026å¹´1æœˆæ™‚ç‚¹ï¼‰**:
>   - åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: $0.02/1Mãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆç´„1000è³ªå•ã§$0.02ï¼‰
>   - GPT-4å›ç­”ç”Ÿæˆ: $0.03/1Kãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆ1è³ªå•ã§ç´„$0.06ï¼‰
>   - **åˆè¨ˆ: 1000è³ªå•ã§ç´„$60ã€æœˆé–“1ä¸‡è³ªå•ã§ç´„$600**

## æœ¬ç•ªé‹ç”¨ã§ã®7ã¤ã®èª²é¡Œã¨è§£æ±ºç­–

å®Ÿè£…ç¾å ´ãŒç›´é¢ã™ã‚‹èª²é¡Œã‚’ã€å®Ÿè·µçš„ãªè§£æ±ºç­–ã¨ã¨ã‚‚ã«ç´¹ä»‹ã—ã¾ã™ã€‚

### èª²é¡Œ1: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

**å•é¡Œ:** ãƒãƒ£ãƒ³ã‚¯ã‚’å°ã•ãåˆ†å‰²ã™ã‚‹ã¨æ¤œç´¢ç²¾åº¦ã¯å‘ä¸Šã—ã¾ã™ãŒã€æ–‡è„ˆãŒå¤±ã‚ã‚Œã¾ã™ã€‚å¤§ããåˆ†å‰²ã™ã‚‹ã¨ãƒã‚¤ã‚ºãŒæ··å…¥ã—ã¾ã™ã€‚

**è§£æ±ºç­–: SINRï¼ˆSearch is not Retrievalï¼‰**

ã€Œãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã®ãƒãƒ£ãƒ³ã‚¯ã€ã¨ã€ŒLLMã«æ¸¡ã™ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ã€ã‚’**åˆ†é›¢**ã—ã¾ã™ã€‚

```python
# æ¤œç´¢ç”¨: å°ã•ã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆ200ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
search_chunks = split_text(document, chunk_size=200)

# LLMç”¨: å¤§ãã„ãƒãƒ£ãƒ³ã‚¯ï¼ˆ1000ãƒˆãƒ¼ã‚¯ãƒ³ã€å‰å¾Œã®æ–‡è„ˆã‚’å«ã‚€ï¼‰
context_chunks = split_text(document, chunk_size=1000, overlap=200)

# æ¤œç´¢â†’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå–å¾—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
chunk_mapping = {
    search_chunks[i]: context_chunks[i//5]  # 5:1ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    for i in range(len(search_chunks))
}
```

ã“ã®æ‰‹æ³•ã«ã‚ˆã‚Šã€**æ¤œç´¢ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€LLMã¸ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæä¾›ãŒ20%å‘ä¸Š**ã—ã¾ã—ãŸã€‚

### èª²é¡Œ2: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é™ç•Œ

**å•é¡Œ:** æ„å‘³çš„é¡ä¼¼åº¦ãŒé«˜ã„â‰ è³ªå•ã®ç­”ãˆãŒå«ã¾ã‚Œã¦ã„ã‚‹

**è§£æ±ºç­–: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆãƒ™ã‚¯ãƒˆãƒ« + BM25ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰**

| æ¤œç´¢æ‰‹æ³• | ç²¾åº¦ | é€Ÿåº¦ | ç”¨é€” |
|---------|------|------|------|
| ãƒ™ã‚¯ãƒˆãƒ«ã®ã¿ | 60% | é«˜é€Ÿ | æ„å‘³æ¤œç´¢ |
| ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ | 55% | è¶…é«˜é€Ÿ | å›ºæœ‰åè©æ¤œç´¢ |
| **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰** | **85%** | ä¸­é€Ÿ | æœ¬ç•ªæ¨å¥¨ |

**BM25ã¨ã¯:**
BM25ï¼ˆBest Matching 25ï¼‰ã¯ã€æƒ…å ±æ¤œç´¢ã«ãŠã‘ã‚‹å¤å…¸çš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°é–¢æ•°ã§ã™ã€‚TF-IDFï¼ˆå˜èªã®å‡ºç¾é »åº¦ã¨å¸Œå°‘æ€§ï¼‰ã‚’æ”¹è‰¯ã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ï¼š

- **å›ºæœ‰åè©ã«å¼·ã„**: "AWS Lambda" ã®ã‚ˆã†ãªå®Œå…¨ä¸€è‡´æ¤œç´¢ã«æœ‰åŠ¹
- **è»½é‡**: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨æ¯”ã¹ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„
- **è£œå®Œæ€§**: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãŒè‹¦æ‰‹ãªã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å®Œå…¨ä¸€è‡´ã€ã‚’ã‚«ãƒãƒ¼

```python
from rank_bm25 import BM25Okapi

def hybrid_search(query, vector_index, documents, alpha=0.5, top_k=10):
    """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨BM25ã®é‡ã¿ä»˜ã‘çµ±åˆ"""

    # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰
    tokenized_docs = [doc.split() for doc in documents]
    bm25 = BM25Okapi(tokenized_docs)

    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¹ã‚³ã‚¢ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
    vector_scores = vector_index.search(query, top_k=top_k)

    # BM25ã‚¹ã‚³ã‚¢
    tokenized_query = query.split()
    bm25_scores = bm25.get_scores(tokenized_query)

    # ã‚¹ã‚³ã‚¢ã‚’0-1ã«æ­£è¦åŒ–
    vector_scores_norm = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min())
    bm25_scores_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())

    # é‡ã¿ä»˜ã‘çµ±åˆï¼ˆalphaã§ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ï¼‰
    combined_scores = alpha * vector_scores_norm + (1 - alpha) * bm25_scores_norm

    # ä¸Šä½kä»¶ã‚’è¿”ã™
    top_indices = combined_scores.argsort()[-top_k:][::-1]
    return [documents[i] for i in top_indices]
```

**alphaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´:**
- `alpha=1.0`: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ï¼ˆæ„å‘³æ¤œç´¢é‡è¦–ï¼‰
- `alpha=0.5`: ãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆ**æ¨å¥¨**ï¼‰
- `alpha=0.0`: BM25ã®ã¿ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢é‡è¦–ï¼‰

### èª²é¡Œ3: è©•ä¾¡æŒ‡æ¨™ã®æ¬ å¦‚

**å•é¡Œ:** ã€Œè‰¯ã„RAGã€ã®æ¸¬ã‚Šæ–¹ãŒãªã„

**è§£æ±ºç­–: ãƒ‰ãƒ¡ã‚¤ãƒ³å›ºæœ‰ã®è©•ä¾¡ã‚»ãƒƒãƒˆæ§‹ç¯‰**

æœ€ä½ã§ã‚‚**50ä»¶ã®è³ªå•-æ­£è§£ãƒšã‚¢**ã‚’ç”¨æ„ã—ã€ä»¥ä¸‹ã‚’æ¸¬å®šã—ã¾ã™ï¼š

- **Recall@k**: æ­£è§£ãŒä¸Šä½kä»¶ã«å«ã¾ã‚Œã‚‹ç¢ºç‡ï¼ˆæ¤œç´¢å“è³ªï¼‰
- **Answer Accuracy**: LLMã®å›ç­”ãŒæ­£è§£ã¨ä¸€è‡´ã™ã‚‹ç¢ºç‡ï¼ˆç”Ÿæˆå“è³ªï¼‰
- **Latency**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã«ç›´çµï¼ˆç›®æ¨™: 1ç§’ä»¥å†…ï¼‰

**å®Ÿæ¸¬ä¾‹ï¼ˆ10ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦æ¨¡ã€å®Ÿè£…å¾Œ3ãƒ¶æœˆã®æ”¹å–„æ¨ç§»ï¼‰:**
- Recall@3: 45%ï¼ˆåˆæœŸFAISSï¼‰ â†’ 72%ï¼ˆãƒãƒ£ãƒ³ã‚¯æœ€é©åŒ–ï¼‰ â†’ **85%**ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼‰
- Answer Accuracy: 60% â†’ 78% â†’ **92%**
- Latency: 2.1ç§’ï¼ˆåŒæœŸå‡¦ç†ï¼‰ â†’ 1.2ç§’ï¼ˆä¸¦åˆ—åŒ–ï¼‰ â†’ **0.5ç§’**ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å°å…¥ï¼‰

### èª²é¡Œ4: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®åˆ¶é™

**å•é¡Œ:** LLMã«ã¯å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ä¸Šé™ãŒã‚ã‚Šã€æ¤œç´¢çµæœã‚’ã™ã¹ã¦æ¸¡ã›ãªã„

**è§£æ±ºç­–: Re-rankingï¼ˆå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿**

æ¤œç´¢çµæœã‚’2æ®µéšã§å‡¦ç†ã—ã¾ã™ï¼š

1. **ç¬¬1æ®µéš**: é«˜é€Ÿãªç²—ã„æ¤œç´¢ã§å€™è£œã‚’å¤šã‚ã«å–å¾—ï¼ˆä¾‹: 100ä»¶ï¼‰
2. **ç¬¬2æ®µéš**: ç²¾å¯†ãªãƒ¢ãƒ‡ãƒ«ã§é–¢é€£åº¦ã‚’å†è©•ä¾¡ã—ã€ä¸Šä½ã®ã¿é¸æŠï¼ˆä¾‹: 5ä»¶ï¼‰

```python
from sentence_transformers import CrossEncoder

# å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆç²¾åº¦é‡è¦–ï¼‰
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

def search_with_reranking(query, vector_index, documents, top_k=5):
    """2æ®µéšæ¤œç´¢: ç²—ã„æ¤œç´¢ â†’ ç²¾å¯†ãªå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°"""

    # ç¬¬1æ®µéš: å€™è£œã‚’å¤šã‚ã«å–å¾—ï¼ˆ100ä»¶ï¼‰
    candidates = vector_index.search(query, top_k=100)
    candidate_docs = [documents[i] for i in candidates]

    # ç¬¬2æ®µéš: ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã§å†è©•ä¾¡
    pairs = [[query, doc] for doc in candidate_docs]
    scores = reranker.predict(pairs)

    # ã‚¹ã‚³ã‚¢ä¸Šä½kä»¶ã®ã¿è¿”ã™
    top_indices = scores.argsort()[-top_k:][::-1]
    return [candidate_docs[i] for i in top_indices]
```

**åŠ¹æœ:**
- GPT-4ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆ128Kï¼‰ã‚’æœ‰åŠ¹æ´»ç”¨
- é–¢é€£åº¦ã®ä½ã„ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã€å›ç­”ç²¾åº¦ãŒ**15-20%å‘ä¸Š**

### èª²é¡Œ5: ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰ã®å•é¡Œ

**å•é¡Œ:** LLMãŒæ¤œç´¢çµæœã«å«ã¾ã‚Œãªã„æƒ…å ±ã‚’ã€Œå‰µä½œã€ã—ã¦ã—ã¾ã†

**è§£æ±ºç­–: Citationï¼ˆå¼•ç”¨ï¼‰ã¨æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã®è¿½åŠ **

```python
def generate_answer_with_citation(query, search_results):
    """å¼•ç”¨ä»˜ãå›ç­”ç”Ÿæˆ"""

    # å„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«IDã‚’ä»˜ä¸
    context_with_ids = "\n\n".join([
        f"[Document {i+1}]\n{r['content']}"
        for i, r in enumerate(search_results)
    ])

    messages = [
        {
            "role": "system",
            "content": """ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
1. å¿…ãšæä¾›ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨
2. å›ç­”ã®å„ä¸»å¼µã« [Document X] ã®å½¢å¼ã§å¼•ç”¨ã‚’ä»˜ã‘ã‚‹
3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€ã¨å›ç­”"""
        },
        {
            "role": "user",
            "content": f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context_with_ids}\n\nè³ªå•: {query}"
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4",
        messages=messages,
        temperature=0.0  # æ±ºå®šè«–çš„ãªå›ç­”ã®ãŸã‚ä½æ¸©åº¦
    )

    return response.choices[0].message.content
```

**è¿½åŠ ã®å¯¾ç­–:**
- `temperature=0.0` ã§å‰µä½œã‚’æŠ‘åˆ¶
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã€Œæƒ…å ±ãŒãªã„å ´åˆã¯èªã‚ã‚‹ã€ãƒ«ãƒ¼ãƒ«ã‚’æ˜è¨˜
- UIä¸Šã§å¼•ç”¨å…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ¤œè¨¼å¯èƒ½ã«

### èª²é¡Œ6: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ã®é›£ã—ã•

**å•é¡Œ:** ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ›´æ–°ã•ã‚Œã¦ã‚‚ã€ãƒ™ã‚¯ãƒˆãƒ«DBã¸ã®åæ˜ ã«æ™‚é–“ãŒã‹ã‹ã‚‹

**è§£æ±ºç­–: å¢—åˆ†æ›´æ–°ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ã®è‡ªå‹•åŒ–**

```python
import hashlib
from datetime import datetime

class IncrementalIndexer:
    """å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°"""

    def __init__(self, vector_db):
        self.vector_db = vector_db
        self.doc_hashes = {}  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ä¿å­˜

    def update_if_changed(self, doc_id, content):
        """å¤‰æ›´ãŒã‚ã£ãŸå ´åˆã®ã¿æ›´æ–°"""
        current_hash = hashlib.md5(content.encode()).hexdigest()

        # æ—¢å­˜ãƒãƒƒã‚·ãƒ¥ã¨æ¯”è¼ƒ
        if doc_id not in self.doc_hashes or self.doc_hashes[doc_id] != current_hash:
            # å¤‰æ›´ã‚’æ¤œçŸ¥ â†’ ãƒ™ã‚¯ãƒˆãƒ«å†ç”Ÿæˆ
            embedding = create_embeddings([content])[0]
            self.vector_db.upsert(
                id=doc_id,
                vector=embedding,
                metadata={"updated_at": datetime.now().isoformat()}
            )
            self.doc_hashes[doc_id] = current_hash
            return True
        return False
```

**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°æ¤œçŸ¥ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼ï¼ˆ`watchdog`ï¼‰ã‚’ä½¿ç”¨
- ãƒãƒƒãƒæ›´æ–°ï¼ˆä¾‹: 5åˆ†ã”ã¨ï¼‰ã§APIå‘¼ã³å‡ºã—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›
- Qdrantãªã©ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°å¯¾å¿œã®ãƒ™ã‚¯ãƒˆãƒ«DBã‚’é¸æŠ

### èª²é¡Œ7: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨ã‚³ã‚¹ãƒˆ

**å•é¡Œ:** ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ãŒå¢—ãˆã‚‹ã¨ã€åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚³ã‚¹ãƒˆã¨æ¤œç´¢ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå¢—å¤§

**è§£æ±ºç­–: éšå±¤çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥**

**1. éšå±¤çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆHNSWï¼‰ã®æ´»ç”¨**

```python
import qdrant_client
from qdrant_client.models import Distance, VectorParams, HnswConfigDiff

client = qdrant_client.QdrantClient(url="localhost:6333")

# HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¨­å®š
client.create_collection(
    collection_name="docs_hnsw",
    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
    hnsw_config=HnswConfigDiff(
        m=16,  # ã‚°ãƒ©ãƒ•ã®æ¥ç¶šæ•°ï¼ˆå¤šã„ã»ã©ç²¾åº¦å‘ä¸Šã€ãƒ¡ãƒ¢ãƒªå¢—åŠ ï¼‰
        ef_construct=100,  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰æ™‚ã®æ¢ç´¢ç¯„å›²
    )
)
```

**2. ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°**

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def cached_search(query_hash, top_k):
    """åŒã˜è³ªå•ã®çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    # å®Ÿéš›ã®æ¤œç´¢ã¯åˆå›ã®ã¿
    return vector_index.search(query_hash, top_k=top_k)

def search_with_cache(query, top_k=5):
    query_hash = hashlib.md5(query.encode()).hexdigest()
    return cached_search(query_hash, top_k)
```

**ã‚³ã‚¹ãƒˆå‰Šæ¸›ã®å®Ÿæ¸¬:**
- HNSWå°å…¥: æ¤œç´¢é€Ÿåº¦**3å€å‘ä¸Š**ï¼ˆ100ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦æ¨¡ï¼‰
- ã‚¯ã‚¨ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥: APIå‘¼ã³å‡ºã—**60%å‰Šæ¸›**ï¼ˆé‡è¤‡è³ªå•ãŒå¤šã„å ´åˆï¼‰
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–: `text-embedding-3-small` ä½¿ç”¨ã§ **80%ã‚³ã‚¹ãƒˆå‰Šæ¸›**ï¼ˆ`ada-002` æ¯”ï¼‰

## ãƒ™ã‚¯ãƒˆãƒ«DBé¸å®šã‚¬ã‚¤ãƒ‰ï¼šChroma vs FAISS vs Qdrant

ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‹ã‚‰æœ¬ç•ªç’°å¢ƒã¸ã®æ®µéšçš„ç§»è¡Œæˆ¦ç•¥ã‚’ç¤ºã—ã¾ã™ã€‚

### ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼ˆã€œ10ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰

**æ¨å¥¨: Chroma**

```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("docs")

# ã‚·ãƒ³ãƒ—ãƒ«ãªAPI
collection.add(
    documents=["doc1", "doc2"],
    ids=["id1", "id2"]
)

results = collection.query(
    query_texts=["query"],
    n_results=5
)
```

**ç†ç”±:**
- ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—5åˆ†ï¼ˆpip install chromadbï¼‰
- Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã€è¿½åŠ ã‚¤ãƒ³ãƒ•ãƒ©ä¸è¦
- å°è¦æ¨¡ãƒãƒ¼ãƒ ãƒ»è¿…é€Ÿãªæ¤œè¨¼ã«æœ€é©

### ã‚¹ãƒ†ãƒ¼ã‚¸2: ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼ˆ10ä¸‡ã€œ100ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰

**æ¨å¥¨: Qdrant Cloudï¼ˆç„¡æ–™æ 1GBã§é–‹å§‹ï¼‰**

```python
from qdrant_client import QdrantClient

client = QdrantClient(url="https://your-cluster.qdrant.io", api_key="xxx")

client.upsert(
    collection_name="docs",
    points=[
        {"id": 1, "vector": [0.1, 0.2, ...], "payload": {"text": "doc1"}},
        {"id": 2, "vector": [0.3, 0.4, ...], "payload": {"text": "doc2"}}
    ]
)
```

**ç†ç”±:**
- **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°å¯¾å¿œ**ï¼ˆFAISSã®å¼±ç‚¹ã‚’å…‹æœï¼‰
- è¤‡é›‘ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆä¾‹: `created_at > 2026-01-01`ï¼‰
- Rustå®Ÿè£…ã«ã‚ˆã‚‹é«˜é€Ÿæ€§

**ç§»è¡Œæ™‚ã®æ³¨æ„ç‚¹:**
> FAISSã‹ã‚‰ã®ç§»è¡Œæ™‚ã¯ã€**ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰ã«æ•°æ™‚é–“ã‹ã‹ã‚‹å ´åˆ**ãŒã‚ã‚Šã¾ã™ã€‚æœ¬ç•ªç§»è¡Œã¯æ®µéšçš„ã«ï¼ˆä¾‹: 20%ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‹ã‚‰é–‹å§‹ï¼‰ã€‚

### ã‚¹ãƒ†ãƒ¼ã‚¸3: ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼ˆ100ä¸‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä»¥ä¸Šï¼‰

**æ¨å¥¨: Qdrant ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆ or Pinecone Serverless**

é¸å®šåŸºæº–:
- **Qdrant**: ã‚³ã‚¹ãƒˆæœ€é©åŒ–é‡è¦–ã€ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†å¯èƒ½
- **Pinecone**: ãƒãƒãƒ¼ã‚¸ãƒ‰å„ªå…ˆã€é‹ç”¨ã‚³ã‚¹ãƒˆå‰Šæ¸›

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- RAGå®Ÿè£…ã¯ç°¡å˜ã ãŒã€æœ¬ç•ªé‹ç”¨ã¯åˆ¥æ¬¡å…ƒã®é›£ã—ã•ï¼ˆã€Œå‹•ãã€â‰ ã€Œä½¿ãˆã‚‹ã€ï¼‰
- **7ã¤ã®èª²é¡Œã¸ã®å¯¾ç­–**:
  - ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²: SINRï¼ˆæ¤œç´¢ç”¨ã¨ç”Ÿæˆç”¨ã‚’åˆ†é›¢ï¼‰
  - ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®é™ç•Œ: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ï¼ˆBM25ä½µç”¨ï¼‰
  - è©•ä¾¡æŒ‡æ¨™: Recall@kã€Answer Accuracyã€Latencyæ¸¬å®š
  - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™: Re-rankingï¼ˆå†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
  - ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³: Citationï¼ˆå¼•ç”¨ï¼‰ã¨ä½æ¸©åº¦è¨­å®š
  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°: å¢—åˆ†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹+ãƒ•ã‚¡ã‚¤ãƒ«ã‚¦ã‚©ãƒƒãƒãƒ£ãƒ¼
  - ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: HNSW+ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- ãƒ™ã‚¯ãƒˆãƒ«DBã¯æ®µéšçš„ç§»è¡Œ: Chromaï¼ˆãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ï¼‰ â†’ Qdrantï¼ˆã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼‰ â†’ Pinecone/ã‚»ãƒ«ãƒ•ãƒ›ã‚¹ãƒˆï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼‰
- å®Ÿæ¸¬ã§æ¤œç´¢ç²¾åº¦85%ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹0.5ç§’ã€ã‚³ã‚¹ãƒˆ70%å‰Šæ¸›ã‚’é”æˆ

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
- æœ€å°æ§‹æˆã®RAGã‚·ã‚¹ãƒ†ãƒ ã‚’30åˆ†ã§å®Ÿè£…ã—ã¦ã¿ã‚‹ï¼ˆä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ä½¿ç”¨ï¼‰
- 50ä»¶ã®è©•ä¾¡ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦ã‚’æ¸¬å®š
- Qdrant Cloudç„¡æ–™æ ã§ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’æœ¬ç•ªãƒ¬ãƒ™ãƒ«ã«é€²åŒ–

## å‚è€ƒ

- [RAGã‚’ã‚¼ãƒ­ã‹ã‚‰å®Ÿè£…ã—ã¦ä»•çµ„ã¿ã‚’å­¦ã¶ã€2025å¹´ç‰ˆã€‘](https://zenn.dev/knowledgesense/articles/2619c6e5918d08)
- [RAGã®ç†æƒ³ã¨ç¾å®Ÿ ~å®Ÿè£…ç¾å ´ãŒç›´é¢ã™ã‚‹7ã¤ã®èª²é¡Œã¨ã€ãã®å…ˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ~](https://zenn.dev/nd_komosyu/articles/6540bbb4873fe5)
- [RAGã«é©ã—ãŸãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã¯ï¼ŸFAISSãƒ»Weaviateãƒ»Pineconeå¾¹åº•æ¯”è¼ƒï½œLLMå…¥é–€ 4.2](https://actionbridge.io/ja-JP/llmtutorial/p/llm-rag-chapter4-2-vector-search-engine)
- [ã€2025å¹´æ±ºå®šç‰ˆã€‘ãƒ™ã‚¯ãƒˆãƒ«DBå®Œå…¨æ¯”è¼ƒã¨RAGæœ€æ–°æ´»ç”¨](https://arpable.com/artificial-intelligence/rag/vector-database-rag/)
- [BM25 - Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25)

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
