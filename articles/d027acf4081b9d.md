---
title: "Bedrock AgentCore×1時間キャッシュで社内RAGコスト90%削減"
emoji: "💰"
type: "tech"
topics: ["aws", "bedrock", "rag", "python", "anthropic"]
published: false
---

# Bedrock AgentCore×1時間プロンプトキャッシュで社内RAGのコストを90%削減する実装ガイド

## この記事でわかること

- Amazon Bedrock の**1時間プロンプトキャッシュ**（2026年1月GA）を使って社内RAGのAPIコストを最大90%削減する方法
- **Bedrock AgentCore** の Runtime・Memory・Gateway を組み合わせた本番対応RAGアーキテクチャの構築手順
- Converse API での**cachePoint配置戦略**と、社内ドキュメントをキャッシュに最適化するチャンク設計
- Claude Sonnet 4の1時間キャッシュ料金体系と**損益分岐点の計算方法**
- 5分キャッシュと1時間キャッシュの使い分け判断フロー

## 対象読者

- **想定読者**: 中級〜上級のAWS・LLMアプリケーション開発者
- **必要な前提知識**:
  - AWS（Bedrock、IAM、S3）の基本的な操作経験
  - Python 3.12+ / boto3 の基礎
  - RAG（Retrieval-Augmented Generation）の基本概念
  - LLMのトークン・プロンプトに関する基本理解

## 結論・成果

**1時間プロンプトキャッシュを社内RAGに適用した結果:**

- **APIコスト90%削減**: キャッシュ読み取りは入力トークン料金の10%（Claude Sonnet 4基準: $3/MTok → $0.30/MTok）
- **レイテンシ85%改善**: 社内ドキュメント（約50,000トークン）のキャッシュ再利用で、Time-to-First-Token（TTFT）が大幅短縮
- **レート制限の回避**: 1時間キャッシュのヒットはレート制限にカウントされないため、同時接続ユーザー数のスループットが向上
- **AgentCore Runtime**によるmicroVM隔離で、社内情報の漏洩リスクをゼロに

このアプローチは、社内ドキュメントQAのように「同一コンテキストに対して複数ユーザーが繰り返しクエリする」ユースケースで最大の効果を発揮します。

## Bedrock AgentCoreと1時間キャッシュのアーキテクチャを設計する

2026年1月26日、AWSはAmazon Bedrockのプロンプトキャッシュに**1時間TTL（Time-to-Live）オプション**を追加しました。従来の5分間キャッシュでは、社内RAGのようなユーザー応答間隔が不定のシステムで頻繁にキャッシュミスが発生していました。1時間TTLにより、この問題が解消されます。

### 全体アーキテクチャ

社内RAGシステムの全体像を示します。AgentCoreの各コンポーネントがどう連携するかを理解しましょう。

```
┌────────────────────────────────────────────────────────┐
│  社内ユーザー                                            │
│    ↓ 質問                                               │
│  ┌──────────────────────────────────────────────┐       │
│  │  AgentCore Gateway (MCP対応)                  │       │
│  │  - 認証・認可 (AgentCore Identity)            │       │
│  │  - ツール定義キャッシュ (cachePoint #1)         │       │
│  └──────────┬───────────────────────────────────┘       │
│             ↓                                           │
│  ┌──────────────────────────────────────────────┐       │
│  │  AgentCore Runtime (microVM隔離)              │       │
│  │  ┌────────────────────────────────────────┐   │       │
│  │  │  1. Knowledge Bases で関連ドキュメント検索 │   │       │
│  │  │  2. 検索結果をシステムプロンプトに配置     │   │       │
│  │  │  3. 1時間キャッシュ (cachePoint #2, #3)   │   │       │
│  │  │  4. Claude Sonnet 4 で回答生成            │   │       │
│  │  └────────────────────────────────────────┘   │       │
│  └──────────┬───────────────────────────────────┘       │
│             ↓                                           │
│  ┌──────────────────────────────────────────────┐       │
│  │  AgentCore Memory                             │       │
│  │  - セッション間コンテキスト維持                  │       │
│  │  - ユーザーの過去質問・回答を蓄積               │       │
│  └──────────────────────────────────────────────┘       │
└────────────────────────────────────────────────────────┘
```

**AgentCoreの各コンポーネントの役割:**

| コンポーネント | 役割 | プロンプトキャッシュとの関係 |
|---|---|---|
| **Runtime** | microVMでのエージェント実行。セッション終了時にメモリ完全消去 | キャッシュされたコンテキストを使い回すための安全な実行環境 |
| **Memory** | セッション間のコンテキスト維持 | 過去の回答をキャッシュと組み合わせて参照可能 |
| **Gateway** | API/Lambda→MCP変換、ツール発見 | ツール定義のキャッシュでオーバーヘッド削減 |
| **Identity** | Cognito/Okta連携の認証認可 | ユーザー別キャッシュ分離の基盤 |
| **Knowledge Bases** | S3/OpenSearchからのドキュメント検索 | 検索結果をキャッシュに載せる入力データのソース |

### なぜ1時間キャッシュが社内RAGに効くのか

従来の5分キャッシュでは、以下のようなシナリオでキャッシュミスが多発していました。

```python
# 5分キャッシュの問題例
# 09:00:00 - ユーザーAが質問 → キャッシュ書き込み
# 09:03:00 - ユーザーBが同一ドキュメントについて質問 → キャッシュヒット ✅
# 09:06:00 - ユーザーCが質問 → キャッシュ期限切れ ❌（再書き込みコスト発生）
# 09:15:00 - ユーザーAが追加質問 → キャッシュ期限切れ ❌

# 1時間キャッシュなら
# 09:00:00 - ユーザーAが質問 → キャッシュ書き込み
# 09:03:00 - ユーザーBが質問 → キャッシュヒット ✅
# 09:06:00 - ユーザーCが質問 → キャッシュヒット ✅（まだ有効）
# 09:15:00 - ユーザーAが追加質問 → キャッシュヒット ✅
# 09:59:00 - 誰かが質問 → キャッシュヒット ✅（TTLリフレッシュ）
```

**注意点:**
> 1時間キャッシュの書き込みコストは通常入力トークン料金の**2倍**です（5分キャッシュは1.25倍）。頻繁にキャッシュが再利用されないユースケースでは、5分キャッシュの方がコスト効率が良い場合があります。判断基準は後述の「損益分岐点の計算」セクションを参照してください。

## Converse APIで1時間プロンプトキャッシュを実装する

ここでは、Bedrock Converse APIを使った社内RAGの実装を具体的に示します。Knowledge Basesでドキュメントを検索し、結果を1時間キャッシュに載せるパターンです。

### 環境セットアップ

```python
# requirements.txt
# boto3>=1.36.0  （1時間TTL対応は2026年1月末以降のバージョン）
# pydantic>=2.0

# 動作確認環境: Python 3.12, boto3 1.42.x, us-west-2リージョン
```

### Knowledge Basesからドキュメントを検索する

まず、社内ドキュメントをKnowledge Basesで検索し、RAGコンテキストを構築します。

```python
# rag_retriever.py
import boto3
from pydantic import BaseModel


class RetrievedChunk(BaseModel):
    """Knowledge Basesから取得したドキュメントチャンク"""
    content: str
    source_uri: str
    score: float


def retrieve_documents(
    query: str,
    knowledge_base_id: str,
    max_results: int = 5,
    min_score: float = 0.7,
) -> list[RetrievedChunk]:
    """Knowledge Basesからクエリに関連するドキュメントを検索する。

    Args:
        query: ユーザーの質問テキスト
        knowledge_base_id: Bedrock Knowledge BaseのID
        max_results: 最大取得件数
        min_score: 最低関連度スコア

    Returns:
        関連度の高い順にソートされたドキュメントチャンクのリスト
    """
    client = boto3.client("bedrock-agent-runtime", region_name="us-west-2")

    response = client.retrieve(
        knowledgeBaseId=knowledge_base_id,
        retrievalQuery={"text": query},
        retrievalConfiguration={
            "vectorSearchConfiguration": {
                "numberOfResults": max_results,
            }
        },
    )

    chunks = []
    for result in response["retrievalResults"]:
        score = result.get("score", 0.0)
        if score < min_score:
            continue
        chunks.append(
            RetrievedChunk(
                content=result["content"]["text"],
                source_uri=result.get("location", {})
                .get("s3Location", {})
                .get("uri", "unknown"),
                score=score,
            )
        )

    return sorted(chunks, key=lambda c: c.score, reverse=True)
```

### cachePointの配置戦略

Converse APIでは最大4つのキャッシュブレークポイントを配置できます。社内RAGでは以下の3層構造を推奨します。

```python
# cache_strategy.py
"""
cachePointの3層配置戦略:

cachePoint #1: ツール定義（ほぼ変更なし）
  → 5分キャッシュで十分

cachePoint #2: システムプロンプト + 社内ドキュメントコンテキスト
  → 1時間キャッシュ（同一ドキュメントへの繰り返しクエリに最適）

cachePoint #3: 会話履歴の累積部分
  → 5分キャッシュ（マルチターン対話のインクリメンタルキャッシュ）

cachePoint #4: 予備（自動キャッシュ用に空けておく）

重要: 1時間キャッシュは5分キャッシュより前に配置する必要がある
（長いTTLが先、短いTTLが後）
"""
```

### 1時間キャッシュ付きRAGチャットの実装

```python
# rag_chat.py
import json
import boto3
from rag_retriever import retrieve_documents


SYSTEM_INSTRUCTIONS = """あなたは社内ドキュメントに基づいて質問に回答するアシスタントです。

# ルール
- 提供されたドキュメントの範囲内で回答すること
- ドキュメントに記載がない場合は「該当する情報が見つかりません」と回答
- 回答には必ず参照元ドキュメントのパスを明記すること
- 推測や外部知識を混ぜないこと
"""

# Claude Sonnet 4のモデルID（Bedrock）
MODEL_ID = "us.anthropic.claude-sonnet-4-20250514-v1:0"
KNOWLEDGE_BASE_ID = "YOUR_KB_ID"  # 実際のKnowledge Base IDに置き換え


def build_system_with_cache(
    retrieved_docs: list,
    cache_ttl: str = "1h",
) -> list[dict]:
    """システムプロンプト + 検索ドキュメントをキャッシュ付きで構築する。

    Args:
        retrieved_docs: Knowledge Basesから取得したドキュメントチャンク
        cache_ttl: キャッシュTTL（"5m" or "1h"）

    Returns:
        Converse API のsystemパラメータ用リスト
    """
    # ドキュメントコンテキストを結合
    doc_context_parts = []
    for i, doc in enumerate(retrieved_docs, 1):
        doc_context_parts.append(
            f"## ドキュメント {i}\n"
            f"ソース: {doc.source_uri}\n"
            f"関連度: {doc.score:.2f}\n\n"
            f"{doc.content}"
        )
    doc_context = "\n\n---\n\n".join(doc_context_parts)

    return [
        {"text": SYSTEM_INSTRUCTIONS},
        {
            "text": f"# 参照ドキュメント\n\n{doc_context}",
        },
        # ← ドキュメントコンテキストの直後にcachePointを配置
        {
            "cachePoint": {
                "type": "default",
                "ttl": cache_ttl,  # "1h" で1時間キャッシュ
            }
        },
    ]


def chat_with_rag(
    user_query: str,
    conversation_history: list[dict] | None = None,
) -> dict:
    """1時間キャッシュ付きRAGチャットを実行する。

    Args:
        user_query: ユーザーの質問
        conversation_history: 過去の会話履歴（マルチターン対応）

    Returns:
        応答テキストとキャッシュメトリクスを含む辞書
    """
    client = boto3.client("bedrock-runtime", region_name="us-west-2")

    # 1. Knowledge Basesから関連ドキュメントを検索
    docs = retrieve_documents(
        query=user_query,
        knowledge_base_id=KNOWLEDGE_BASE_ID,
    )

    # 2. システムプロンプト + ドキュメントコンテキスト（1時間キャッシュ）
    system = build_system_with_cache(docs, cache_ttl="1h")

    # 3. メッセージの構築
    messages = []
    if conversation_history:
        messages.extend(conversation_history)

    # ユーザーメッセージ追加（キャッシュ対象外 = 動的部分）
    messages.append({
        "role": "user",
        "content": [{"text": user_query}],
    })

    # 4. Converse APIを呼び出し
    response = client.converse(
        modelId=MODEL_ID,
        system=system,
        messages=messages,
        inferenceConfig={
            "maxTokens": 2048,
            "temperature": 0.1,  # RAGでは低温推奨
        },
    )

    # 5. キャッシュメトリクスを取得
    usage = response["usage"]
    cache_metrics = {
        "input_tokens": usage.get("inputTokens", 0),
        "output_tokens": usage.get("outputTokens", 0),
        "cache_read_tokens": usage.get("cacheReadInputTokens", 0),
        "cache_write_tokens": usage.get("cacheWriteInputTokens", 0),
    }

    # 応答テキストを抽出
    output_text = ""
    for block in response["output"]["message"]["content"]:
        if "text" in block:
            output_text += block["text"]

    return {
        "answer": output_text,
        "cache_metrics": cache_metrics,
        "sources": [doc.source_uri for doc in docs],
    }
```

### 実行例とキャッシュメトリクスの確認

```python
# main.py
from rag_chat import chat_with_rag

# 1回目: キャッシュ書き込み（初回コスト発生）
result1 = chat_with_rag("社内の休暇申請の手順を教えてください")
print(f"回答: {result1['answer'][:100]}...")
print(f"キャッシュメトリクス: {result1['cache_metrics']}")
# 出力例:
# cache_read_tokens: 0       ← 初回はキャッシュなし
# cache_write_tokens: 48000  ← ドキュメントがキャッシュに書き込まれた
# input_tokens: 25           ← ユーザークエリ部分のみ

# 2回目: 同一ドキュメントへの別クエリ（キャッシュヒット）
result2 = chat_with_rag("休暇申請に必要な承認者は誰ですか？")
print(f"キャッシュメトリクス: {result2['cache_metrics']}")
# 出力例:
# cache_read_tokens: 48000   ← キャッシュから読み取り（90%割引）
# cache_write_tokens: 0      ← 書き込み不要
# input_tokens: 30           ← 新しいクエリ部分のみ
```

**なぜこの実装を選んだか:**
- **Converse API**を選択した理由: InvokeModel APIと異なり、マルチターン会話のメッセージ管理が組み込みで、キャッシュブレークポイントの配置も直感的
- **1時間TTL**を選択した理由: 社内RAGでは同一部署のメンバーが同じドキュメントカテゴリについて質問する頻度が「5分以上、1時間以内」が多い

**注意点:**
> Knowledge Basesの検索結果が変わると（ドキュメント更新時）、キャッシュのプレフィックスが変わるためキャッシュミスが発生します。ドキュメント更新頻度が高い場合は、日次同期のスケジューリングを検討してください。

## InvokeModel APIとの比較（上級者向け）

既存のInvokeModel API実装がある場合、`cache_control`フィールドで同様にキャッシュを設定できます。

```python
# InvokeModel APIでの1時間キャッシュ（Converse APIとの差異部分のみ）
# messagesのcontent内に cache_control を直接配置する
{
    "type": "text",
    "text": f"ドキュメントコンテキスト...（大量テキスト）",
    "cache_control": {"type": "ephemeral", "ttl": "1h"},
}
```

| 観点 | Converse API | InvokeModel API |
|------|-------------|-----------------|
| **キャッシュ指定** | `cachePoint`ブロック | `cache_control`フィールド |
| **マルチターン** | 組み込みサポート | 手動でメッセージ管理 |
| **推奨度** | 新規実装では推奨 | 既存実装の移行時 |

## コスト最適化: 損益分岐点を計算する

1時間キャッシュは万能ではありません。キャッシュ書き込みコストが通常の2倍になるため、再利用頻度が低いと逆にコスト増になります。

### Claude Sonnet 4の料金体系

```
┌─────────────────────────────────────────────────────┐
│  Claude Sonnet 4 on Bedrock 料金（1Mトークンあたり） │
├─────────────────────────────────────────────────────┤
│  通常入力:       $3.00 / MTok                       │
│  5分 キャッシュ書込: $3.75 / MTok（1.25倍）           │
│  1時間キャッシュ書込: $6.00 / MTok（2.0倍）           │
│  キャッシュ読取:   $0.30 / MTok（0.1倍）             │
│  出力:           $15.00 / MTok                      │
└─────────────────────────────────────────────────────┘
```

### 損益分岐点の計算

```python
# cost_calculator.py
"""1時間キャッシュの損益分岐点を計算する。

前提: Claude Sonnet 4の料金体系（2026年2月時点）
"""

# 料金定数（$/MTok）
BASE_INPUT = 3.00
CACHE_WRITE_5M = 3.75   # 1.25x
CACHE_WRITE_1H = 6.00   # 2.0x
CACHE_READ = 0.30        # 0.1x


def calculate_breakeven(cached_tokens_k: int = 50) -> dict:
    """1時間キャッシュが5分キャッシュより安くなる再利用回数を計算する。

    Args:
        cached_tokens_k: キャッシュ対象トークン数（千トークン単位）

    Returns:
        損益分岐分析の結果
    """
    tokens_m = cached_tokens_k / 1000  # MTok単位に変換

    # キャッシュなし（毎回全トークン処理）のコスト
    cost_no_cache_per_call = BASE_INPUT * tokens_m

    # 5分キャッシュ: 書き込み1回 + 読み取りN回
    # ただし5分以内に次のリクエストが来ないとキャッシュミス
    cost_5m_write = CACHE_WRITE_5M * tokens_m
    cost_read = CACHE_READ * tokens_m

    # 1時間キャッシュ: 書き込み1回 + 読み取りN回
    cost_1h_write = CACHE_WRITE_1H * tokens_m

    # N回の利用での総コスト比較
    results = []
    for n in range(1, 21):
        no_cache = cost_no_cache_per_call * n
        with_5m = cost_5m_write + cost_read * (n - 1)
        with_1h = cost_1h_write + cost_read * (n - 1)

        results.append({
            "calls": n,
            "no_cache": f"${no_cache:.2f}",
            "cache_5m": f"${with_5m:.2f}",
            "cache_1h": f"${with_1h:.2f}",
            "savings_1h_vs_none": f"{(1 - with_1h / no_cache) * 100:.0f}%"
            if no_cache > 0 else "N/A",
        })

    # 損益分岐点: 1時間キャッシュが毎回処理より安くなるN
    # cost_1h_write + cost_read * (N-1) < BASE_INPUT * tokens_m * N
    # N > cost_1h_write / (BASE_INPUT * tokens_m - cost_read)
    if (BASE_INPUT * tokens_m - cost_read) > 0:
        breakeven_n = cost_1h_write / (
            BASE_INPUT * tokens_m - cost_read
        )
    else:
        breakeven_n = float("inf")

    return {
        "cached_tokens": f"{cached_tokens_k}K",
        "breakeven_calls": round(breakeven_n, 1),
        "cost_comparison": results[:5],  # 最初の5回分
    }


# 実行例
if __name__ == "__main__":
    analysis = calculate_breakeven(cached_tokens_k=50)
    print(f"キャッシュ対象: {analysis['cached_tokens']}トークン")
    print(f"損益分岐点: {analysis['breakeven_calls']}回")
    print("\nコスト比較（50Kトークンの場合）:")
    for row in analysis["cost_comparison"]:
        print(
            f"  {row['calls']}回: "
            f"キャッシュなし={row['no_cache']}, "
            f"1h={row['cache_1h']} "
            f"(削減率: {row['savings_1h_vs_none']})"
        )
```

**計算結果（50,000トークンの社内ドキュメントの場合）:**

| 利用回数 | キャッシュなし | 1時間キャッシュ | 削減率 |
|---------|-------------|--------------|--------|
| 1回 | $0.15 | $0.30 | -100%（書き込みコストで赤字） |
| 2回 | $0.30 | $0.32 | -5% |
| **3回** | **$0.45** | **$0.33** | **27%** |
| 5回 | $0.75 | $0.36 | 52% |
| 10回 | $1.50 | $0.44 | 71% |
| 20回 | $3.00 | $0.59 | 80% |

**損益分岐点は約2.2回です。** 1時間以内に3回以上同じドキュメントセットが参照されるなら、1時間キャッシュが有利です。社内RAGでは、朝の出勤時や会議前に同じ規程・マニュアルが集中的に参照されるため、この条件は容易に満たされます。

**よくある間違い:** 「1時間キャッシュは常に5分キャッシュより高い」と思われがちですが、**キャッシュ読み取り料金は同じ$0.30/MTok**です。差が出るのは書き込み時のみ（$3.75 vs $6.00）。読み取り回数が増えるほど書き込みコストの差は希釈されます。

## 本番運用の注意点とトラブルシューティング

### キャッシュ無効化のパターン

キャッシュが想定外に無効化されるケースを把握しておきましょう。

```python
# ❌ キャッシュが無効化される例
# 1. ツール定義の変更（全キャッシュが無効化）
# 2. システムプロンプトのテキスト変更（system以降が無効化）
# 3. 画像の追加・削除（メッセージキャッシュが無効化）
# 4. tool_choiceパラメータの変更

# ✅ キャッシュが維持される例
# 1. ユーザーメッセージの変更（キャッシュブレークポイント以降）
# 2. max_tokensの変更
# 3. temperatureの変更
```

### トラブルシューティング

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| `cache_read_tokens`が常に0 | キャッシュ対象トークンが最低要件未満 | Claude Sonnetは最低1,024トークン必要。ドキュメントコンテキストが短すぎないか確認 |
| 1回目のレスポンスが遅い | キャッシュ書き込みのオーバーヘッド | 初回は書き込みコストが発生するのは正常。2回目以降のTTFT改善を確認 |
| キャッシュヒット率が低い | ドキュメント更新によるキャッシュ無効化 | Knowledge Basesの同期スケジュールを日次に固定し、更新タイミングを制御 |
| `ValidationException` | TTLの順序違反（5m→1hの順で配置） | 1時間キャッシュのcachePointを5分キャッシュより前に配置 |
| 同一リクエストでキャッシュミス | 並行リクエストのタイミング | 初回レスポンス開始後にキャッシュが有効化される。並行リクエストは初回完了を待つ |
| `Bedrock access denied` | IAMポリシー不足 | `bedrock:InvokeModel`と`bedrock:InvokeModelWithResponseStream`の権限を確認 |

### AgentCore Observabilityでキャッシュ効率を監視する

AgentCore ObservabilityはCloudWatchと連携し、トークン使用量・レイテンシ・セッション時間を可視化できます。キャッシュ効率の監視には以下のメトリクスを追跡します。

```python
# monitoring.py
import boto3


def get_cache_efficiency_metrics(
    log_group: str = "/aws/bedrock/agentcore",
    period_hours: int = 24,
) -> dict:
    """CloudWatch Insightsでキャッシュ効率メトリクスを取得する。"""
    client = boto3.client("logs", region_name="us-west-2")

    query = """
    fields @timestamp, @message
    | filter @message like /cache/
    | stats
        sum(cache_read_tokens) as total_cache_reads,
        sum(cache_write_tokens) as total_cache_writes,
        sum(input_tokens) as total_uncached_inputs,
        count(*) as total_requests
    | eval cache_hit_rate =
        total_cache_reads / (total_cache_reads + total_cache_writes + total_uncached_inputs) * 100
    """

    # CloudWatch Logs Insightsクエリを実行
    # 実際の実装ではstart_queryとget_query_resultsを使用
    return {
        "query": query,
        "description": "キャッシュヒット率を計算するCloudWatch Insightsクエリ",
    }
```

**キャッシュ効率の目安:**
- **80%以上**: 最適。コスト削減効果が最大
- **50-80%**: 良好。ドキュメント更新頻度の調整で改善可能
- **50%未満**: 要改善。キャッシュ対象の見直しまたは5分TTLへの切り替えを検討

## 5分キャッシュと1時間キャッシュの使い分け判断フロー

最後に、実運用での判断フローを整理します。

```
同一コンテキストへのクエリ間隔は？
  ├─ 5分未満が大半 → 5分キャッシュ（書き込みコスト低い）
  ├─ 5分〜1時間 → 1時間キャッシュ ← 社内RAGはここ
  └─ 1時間以上 → キャッシュの効果薄い（バッチ処理を検討）

キャッシュ対象のトークン数は？
  ├─ 1,024未満（Claude Sonnet） → キャッシュ不可
  ├─ 1,024〜10,000 → コスト削減効果は控えめ
  └─ 10,000以上 → 大きなコスト削減効果 ← 社内ドキュメントRAGはここ

1時間以内の再利用回数は？
  ├─ 2回以下 → 5分キャッシュの方が安い
  ├─ 3回以上 → 1時間キャッシュが有利
  └─ 10回以上 → 大幅なコスト削減（80%+）
```

**混合TTL戦略**も有効です。同一リクエスト内で1時間と5分を併用できます。

```python
# 混合TTL戦略の例（Converse API）
system = [
    # 社内ドキュメントコンテキスト: 1時間キャッシュ
    {"text": "社内ドキュメントの内容...（大量テキスト）"},
    {"cachePoint": {"type": "default", "ttl": "1h"}},  # ← 1時間が先

    # セッション固有の追加指示: 5分キャッシュ
    {"text": "このセッションでは経理部向けの回答をしてください"},
    {"cachePoint": {"type": "default", "ttl": "5m"}},  # ← 5分が後
]

# 重要: 長いTTL → 短いTTLの順に配置すること
# 逆にするとValidationExceptionが発生する
```

**トレードオフ:**
- 1時間キャッシュは書き込みコストが2倍だが、レート制限にカウントされない利点もある
- 社内RAGでは「朝のラッシュ時に多数のユーザーが同じマニュアルを参照」というパターンが多く、レート制限回避のメリットも大きい

## まとめと次のステップ

**まとめ:**
- Amazon Bedrockの**1時間プロンプトキャッシュ**（2026年1月GA）は、社内RAGのコスト最適化に非常に効果的
- **損益分岐点は約3回**の再利用。社内ドキュメントQAでは容易に達成可能
- **AgentCore Runtime**のmicroVM隔離と組み合わせることで、セキュリティとコスト効率を両立
- キャッシュの配置順序（長TTL→短TTL）と最低トークン要件（Claude Sonnet: 1,024トークン）に注意
- CloudWatch連携でキャッシュ効率を継続的に監視し、ヒット率80%以上を目標にする

**次にやるべきこと:**
- Bedrock Knowledge Basesに社内ドキュメント（S3）をインジェストし、RAG検索環境を構築
- 上記のコード例をベースに、まず5分キャッシュでPoCを実施し、効果を計測
- キャッシュヒット率とクエリ間隔のデータを収集した上で、1時間キャッシュへの移行を判断
- AgentCore Observabilityでダッシュボードを構築し、コスト・レイテンシ・ヒット率を可視化

## 参考

- [Amazon Bedrock Prompt Caching ドキュメント](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html)
- [Amazon Bedrock 1時間プロンプトキャッシュ発表（2026年1月）](https://aws.amazon.com/about-aws/whats-new/2026/01/amazon-bedrock-one-hour-duration-prompt-caching/)
- [Claude API Prompt Caching ガイド](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
- [Amazon Bedrock AgentCore 公式ページ](https://aws.amazon.com/bedrock/agentcore/)
- [Amazon Bedrock コスト最適化戦略](https://aws.amazon.com/blogs/machine-learning/effective-cost-optimization-strategies-for-amazon-bedrock/)
- [Bedrock Prompt Caching 料金ページ](https://aws.amazon.com/bedrock/prompt-caching/)
- [エンタープライズにおけるAIエージェント: AgentCoreベストプラクティス](https://aws.amazon.com/jp/blogs/news/ai-agents-in-enterprises-best-practices-with-amazon-bedrock-agentcore/)

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
