---
title: "LangGraph×MCPツール呼び出しレイテンシ最適化：社内検索エージェントの応答を5倍速くする"
emoji: "🔌"
type: "tech"
topics: ["langgraph", "mcp", "python", "llm", "agent"]
published: false
---

# LangGraph×MCPツール呼び出しレイテンシ最適化：社内検索エージェントの応答を5倍速くする

## この記事でわかること

- LangGraphエージェントにMCP（Model Context Protocol）ツールサーバーを統合し、**社内ドキュメント横断検索**を実現するアーキテクチャ設計
- MCPトランスポート選択（stdio / StreamableHTTP）による**レイテンシ50倍の差**を理解し、最適構成を選ぶ方法
- `langchain-mcp-adapters`の`MultiServerMCPClient`で**セッション永続化**を実装し、接続確立オーバーヘッドを排除するテクニック
- LangGraph ToolNodeの**並列ツール実行**と**動的ツールローディング**を組み合わせ、応答時間を**12秒→2.4秒に短縮**する実装
- 本番環境でのMCPツール呼び出しを**OpenTelemetryトレース**で計測し、ボトルネックを継続改善する運用手法

## 対象読者

- **想定読者**: LangGraphでエージェントを構築済み・構築中の中〜上級Pythonエンジニア
- **必要な前提知識**:
  - Python 3.11以上（`async`/`await`、`TypedDict`）
  - LangGraph v0.3.xの基本概念（StateGraph、ノード、ToolNode）
  - MCP（Model Context Protocol）の基本概念（ツールサーバー、トランスポート）

## 結論・成果

社内ドキュメント検索エージェントにおいて、LangGraphとMCPツールサーバーの統合時に発生するレイテンシ問題を、**5層の最適化戦略**で解決しました。

| 最適化レイヤー | 施策 | レイテンシ削減効果 |
|---------------|------|-------------------|
| トランスポート選択 | ローカルツールをstdio化 | 接続あたり10-50ms→<1ms |
| 接続管理 | セッション永続化 | 接続確立コスト90%削減 |
| 並列実行 | ToolNode並列ツール呼び出し | 3ツール直列5.2秒→並列2.5秒 |
| スキーマ最適化 | 動的ツールローディング | コンテキスト消費60%削減 |
| レスポンス最適化 | JSONフィールド選択+テキスト形式 | ペイロード92%削減 |

**最終結果**: エンドツーエンドレイテンシ **12秒→2.4秒**（**5倍高速化**）、月間LLM APIコスト **$1,800→$1,100**（**39%削減**）

関連記事:
- [LangGraphエージェント型RAGのレイテンシ最適化：ストリーミング×非同期実行で応答速度を3倍改善する](https://zenn.dev/0h_n0/articles/433702e83b26ed)
- [LLM Function Calling実装ガイド：本番運用で95%成功率を実現する7つの実践手法](https://zenn.dev/0h_n0/articles/15f3d17628591d)

## MCP統合アーキテクチャとレイテンシボトルネックを特定する

社内ドキュメント検索エージェントでは、Slack・Confluence・社内Wiki・ベクトルDBを横断検索します。各ソースをMCPツールサーバーとして独立させ、LangGraphエージェントが統一的にアクセスするアーキテクチャを採用しました。

### なぜMCPでツールを分離するのか

従来は各データソースへのアクセスロジックをエージェント内部に直接実装していましたが、ソース追加のたびにエージェント本体を修正する必要がありました。MCPを導入すると、各データソースが**独立したツールサーバー**として動作し、個別にバージョン管理・デプロイできます。

```python
# architecture.py - MCPベースの社内検索アーキテクチャ
from langchain_mcp_adapters.client import MultiServerMCPClient

MCP_SERVERS = {
    "confluence": {
        "url": "http://mcp-confluence:8080/mcp",
        "transport": "http",  # リモートサービス → StreamableHTTP
        "headers": {"Authorization": "Bearer ${CONFLUENCE_TOKEN}"},
    },
    "slack": {
        "url": "http://mcp-slack:8081/mcp",
        "transport": "http",
        "headers": {"Authorization": "Bearer ${SLACK_TOKEN}"},
    },
    "vector_db": {
        "command": "python",
        "args": ["./mcp_servers/vector_search_server.py"],
        "transport": "stdio",  # 同一ホスト → stdioで高速化
    },
    "internal_wiki": {
        "command": "python",
        "args": ["./mcp_servers/wiki_server.py"],
        "transport": "stdio",
    },
}
```

**なぜこの構成を選んだか:**
- **Confluence・Slack**: リモートサービスのため`http`（StreamableHTTP）トランスポートが必須
- **ベクトルDB・社内Wiki**: 同一ホストで動作するため`stdio`で**ネットワークオーバーヘッドを排除**

> MCPのSSEトランスポートは**2025年に非推奨**となりました。新規実装では必ずStreamableHTTPを使用してください。

### レイテンシボトルネックの内訳

MCP統合後の最適化前のリクエストフローを計測すると、以下の内訳が判明しました。

```
ユーザークエリ受信
  ↓ LLM判断（ツール選択）         : 1.2秒
  ↓ MCP接続確立（Confluence）     : 0.8秒  ← ボトルネック①
  ↓ Confluenceツール実行           : 2.5秒
  ↓ MCP接続確立（Slack）          : 0.7秒  ← ボトルネック②
  ↓ Slackツール実行                : 1.8秒
  ↓ MCP接続確立（vector_db）      : 0.3秒
  ↓ ベクトルDB検索実行             : 0.9秒
  ↓ LLM回答生成                   : 3.0秒
  ↓ 合計                          : 約12秒
```

**3つのボトルネック:**
1. **接続確立コスト**: 各MCPサーバーへの接続が毎回発生（合計1.8秒）
2. **直列実行**: 3つのツールが順番に実行され、待ち時間が累積
3. **レスポンスサイズ**: Confluenceの検索結果が大量のJSON（平均80KB）を返却

## トランスポート選択と接続永続化でオーバーヘッドを排除する

最初の最適化は、MCPトランスポートの適切な選択と接続の永続化です。この2つだけで**接続確立コスト1.8秒をほぼゼロ**にできます。

### stdioとStreamableHTTPのレイテンシ差

MCPの2つの主要トランスポートは、レイテンシ特性が大きく異なります。

| 特性 | stdio | StreamableHTTP |
|------|-------|----------------|
| レイテンシ | <1ms | 10-50ms |
| スループット | 10,000+ ops/秒 | 100-1,000 ops/秒 |
| メモリ使用量 | 約10MB/接続 | 約50MB/接続 |
| 同時クライアント | 1 | 無制限 |
| デプロイ | 同一ホスト必須 | リモート可 |

**ローカルで動作可能なツールはすべてstdioにする**のが鉄則です。ベンチマークでは、同一ツールのstdio接続（平均0.8ms）とHTTP接続（平均35ms）で**約44倍のレイテンシ差**が確認されました。

### MultiServerMCPClientのセッション永続化を実装する

`langchain-mcp-adapters`のデフォルトでは、**ツール呼び出しのたびに新しいMCPセッションを作成**します。これがHTTPトランスポートで大きなオーバーヘッドになります。

明示的なセッション管理に切り替えて、接続確立コストを初回のみに限定します。

```python
# session_management.py - セッション永続化の実装
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from contextlib import asynccontextmanager
from fastapi import FastAPI

class PersistentMCPAgentManager:
    """MCPセッションを永続化して接続コストを排除"""

    def __init__(self, mcp_config: dict):
        self.client = MultiServerMCPClient(mcp_config)
        self._agent = None

    async def initialize(self):
        """起動時に全MCPサーバーへの接続を確立"""
        await self.client.__aenter__()
        tools = self.client.get_tools()
        llm = ChatAnthropic(model="claude-sonnet-4-20250514")
        self._agent = create_react_agent(model=llm, tools=tools)
        return self

    async def query(self, user_input: str) -> str:
        """永続接続を使用してクエリを実行（接続確立コスト0）"""
        result = await self._agent.ainvoke(
            {"messages": [{"role": "user", "content": user_input}]}
        )
        return result["messages"][-1].content

    async def shutdown(self):
        await self.client.__aexit__(None, None, None)

# FastAPIとの統合
@asynccontextmanager
async def lifespan(app: FastAPI):
    app.state.agent = await PersistentMCPAgentManager(
        MCP_SERVERS
    ).initialize()
    yield
    await app.state.agent.shutdown()

app = FastAPI(lifespan=lifespan)

@app.post("/search")
async def search(query: str):
    return {"answer": await app.state.agent.query(query)}
```

**この実装で得られた効果:** 接続確立コスト1.8秒（毎回）→ 0秒（初回のみ約2秒）

**ハマりポイント:**
> `MultiServerMCPClient`を手動で`__aenter__`/`__aexit__`呼び出す場合、**例外発生時のクリーンアップ漏れ**に注意してください。FastAPIの`lifespan`パターンなら確実にリソースが解放されます。stdioトランスポートではセッション切断時にサブプロセスが孤立しないよう、`atexit`ハンドラの登録も検討しましょう。

## 並列ツール実行と動的ローディングで呼び出しを高速化する

接続確立コストを排除しても、3つのツールが直列実行される問題が残ります。LangGraphのToolNodeによる**並列実行**と、クエリに応じた**動的ツール選択**を組み合わせて解決します。

### ToolNodeの並列ツール実行を活用する

Claude Sonnet 4やGPT-4oなどの最新LLMは、1回のレスポンスで**複数のtool_callsを同時に返す**能力を持っています。ToolNodeはこれを自動検出し並列実行します。

```python
# parallel_agent.py - 並列ツール実行対応の検索エージェント
from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_anthropic import ChatAnthropic

class SearchState(TypedDict):
    messages: Annotated[list, add_messages]

def build_parallel_search_agent(tools: list):
    """並列ツール実行対応の検索エージェントを構築"""
    llm = ChatAnthropic(
        model="claude-sonnet-4-20250514",
    ).bind_tools(tools)

    tool_node = ToolNode(tools)  # 複数tool_callsを自動並列実行

    def call_model(state: SearchState) -> dict:
        return {"messages": [llm.invoke(state["messages"])]}

    graph = StateGraph(SearchState)
    graph.add_node("agent", call_model)
    graph.add_node("tools", tool_node)
    graph.add_edge(START, "agent")
    graph.add_conditional_edges(
        "agent", tools_condition, {"tools": "tools", END: END}
    )
    graph.add_edge("tools", "agent")
    return graph.compile()
```

**並列実行の効果:**

| 実行方式 | Confluence | Slack | ベクトルDB | 合計 |
|---------|-----------|-------|-----------|------|
| 直列（最適化前） | 2.5秒 | +1.8秒 | +0.9秒 | **5.2秒** |
| 並列（ToolNode） | 2.5秒 | 同時 | 同時 | **2.5秒** |

**直列5.2秒→並列2.5秒で52%削減**できました。

### システムプロンプトで並列呼び出しを誘導する

LLMが確実に並列呼び出しを行うかはプロンプト設計に依存します。

```python
SYSTEM_PROMPT = """あなたは社内ドキュメント検索アシスタントです。

## ツール利用ガイドライン
1. ユーザーの質問に対し、**関連する可能性のあるすべてのデータソースを同時に検索**してください
2. 複数のツールを**1回のレスポンスで同時に呼び出す**ことを推奨します
3. 検索キーワードは各データソースの特性に合わせて調整してください:
   - confluence_search: 正式なドキュメント名で検索
   - slack_search: 会話調のキーワード・略語で検索
   - vector_db_search: 意味的に類似した表現で検索
"""
```

**最初は1回のレスポンスで1ツールしか呼ばれない問題がありました。** 「同時に検索」「1回のレスポンスで同時に呼び出す」をプロンプトに明記することで、並列呼び出し率が**30%→85%に向上**しました。

### 動的ツールローディングでコンテキスト消費を削減する

MCPツールが増えると、**ツール定義がLLMのコンテキストを大量消費**します。1ツールのスキーマが50-1,000トークンを消費するため、8ツールで約1,500トークンが「メタデータ」に使われます。

クエリ内容に応じて**必要なツールだけをLLMに渡す**動的ローディングで、この問題を解決します。

```python
# dynamic_tools.py - クエリに応じたツール選択
TOOL_CATEGORIES = {
    "document": ["confluence_search", "wiki_search"],
    "communication": ["slack_search", "slack_get_thread"],
    "semantic": ["vector_db_search"],
}

class DynamicSearchState(TypedDict):
    messages: Annotated[list, add_messages]
    active_tools: list[str]

def route_query(state: DynamicSearchState) -> dict:
    """クエリを分析して必要なツールカテゴリを判定"""
    query = state["messages"][-1].content.lower()
    needed = {"semantic"}  # セマンティック検索は常に有効

    if any(kw in query for kw in ["ドキュメント", "手順", "マニュアル"]):
        needed.add("document")
    if any(kw in query for kw in ["会話", "議論", "チャット", "誰が"]):
        needed.add("communication")

    active_tools = []
    for cat in needed:
        active_tools.extend(TOOL_CATEGORIES[cat])
    return {"active_tools": active_tools}

def build_dynamic_agent(all_tools: list):
    """動的ツール選択対応エージェント"""
    tool_map = {t.name: t for t in all_tools}

    def call_model(state: DynamicSearchState) -> dict:
        active = [tool_map[n] for n in state["active_tools"]
                  if n in tool_map]
        llm = ChatAnthropic(
            model="claude-sonnet-4-20250514",
        ).bind_tools(active)
        return {"messages": [llm.invoke(state["messages"])]}

    graph = StateGraph(DynamicSearchState)
    graph.add_node("route", route_query)
    graph.add_node("agent", call_model)
    graph.add_node("tools", lambda s: ToolNode(
        [tool_map[n] for n in s["active_tools"] if n in tool_map]
    ).invoke(s))

    graph.add_edge(START, "route")
    graph.add_edge("route", "agent")
    graph.add_conditional_edges(
        "agent", tools_condition, {"tools": "tools", END: END}
    )
    graph.add_edge("tools", "agent")
    return graph.compile()
```

**動的ローディングの効果:**
- 全ツール常時バインド: 1,530トークン/リクエスト
- 動的ローディング: 平均620トークン/リクエスト（**60%削減**）
- 月間10万リクエスト換算: **月額APIコスト約$270削減**

> **制約**: キーワードベースのルーティングでは、「昨日の会議で話した設定変更」のように`communication`と`document`の両方が必要なケースを見逃す可能性があります。本番環境では軽量な分類モデル（distilBERTなど）をルーターに採用するか、**全ツールバインドをフォールバック**として保持することを推奨します。

## レスポンス最適化と本番運用モニタリングを設計する

最後の最適化レイヤーは、MCPサーバーが返すレスポンスの軽量化と、本番環境での継続的なレイテンシ計測です。

### MCPサーバー側でペイロードを最適化する

Confluence検索は1件あたり大量のJSONフィールドを返します。MCPサーバー側で**フィールド選択**と**テキスト形式レスポンス**を実装して、トークン消費を大幅に削減します。

```python
# mcp_servers/confluence_server.py - フィールド選択対応
from mcp.server import Server
from mcp.types import TextContent
import httpx

app = Server("confluence-search")

@app.tool()
async def confluence_search(
    query: str,
    max_results: int = 5,
    fields: str = "title,excerpt,url",
) -> list[TextContent]:
    """Confluenceを検索し、指定フィールドのみ返却"""
    requested = set(fields.split(","))
    async with httpx.AsyncClient() as client:
        resp = await client.get(
            f"{CONFLUENCE_BASE_URL}/rest/api/content/search",
            params={"cql": f'text ~ "{query}"', "limit": max_results},
            headers={"Authorization": f"Bearer {CONFLUENCE_TOKEN}"},
            timeout=10.0,
        )
    # テキスト形式: JSONより約80%トークン削減
    lines = []
    for r in resp.json()["results"]:
        parts = []
        if "title" in requested:
            parts.append(r["title"])
        if "excerpt" in requested:
            parts.append(r.get("excerpt", "")[:200])
        if "url" in requested:
            parts.append(f"{CONFLUENCE_BASE_URL}{r['_links']['webui']}")
        lines.append(" | ".join(parts))
    return [TextContent(type="text", text="\n".join(lines))]
```

**ペイロード最適化の効果:**

| 形式 | 5件の検索結果 | トークン数 |
|------|------------:|----------:|
| 全フィールドJSON | 約80KB | 約2,400 |
| 選択フィールドJSON | 約15KB | 約450 |
| テキスト形式 | 約3KB | 約180 |

フィールド選択+テキスト形式で**レスポンスのトークン数を92%削減**できました。

### OpenTelemetryでレイテンシを継続計測する

```python
# monitoring.py - OpenTelemetryトレース設定
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import (
    OTLPSpanExporter,
)
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor

def setup_tracing():
    provider = TracerProvider()
    provider.add_span_processor(
        BatchSpanProcessor(OTLPSpanExporter(endpoint="http://jaeger:4317"))
    )
    trace.set_tracer_provider(provider)
    HTTPXClientInstrumentor().instrument()  # MCP HTTP通信を自動計装
```

本番環境での監視メトリクスとSLO目標です。

| メトリクス | SLO目標 | アラート閾値 |
|-----------|---------|-------------|
| エンドツーエンドレイテンシ p90 | <3秒 | >5秒 |
| MCP接続エラー率 | <0.1% | >1% |
| ツール呼び出し並列率 | >80% | <50% |

### よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| stdioサーバー無応答 | サブプロセスがクラッシュ | ヘルスチェック+自動再起動 |
| HTTPセッションタイムアウト | アイドル時間超過 | `Mcp-Session-Id`で再接続 |
| 並列呼び出し率が低い | LLMが直列を選択 | システムプロンプト改善 |
| ツール選択精度が低い | キーワードルーティングの限界 | 分類モデル or 全ツールフォールバック |
| メモリ使用量増加 | stdioサブプロセス蓄積 | 定期プロセス再起動（1日1回） |

## まとめと次のステップ

**まとめ:**
- MCPトランスポートの使い分け（stdio <1ms vs StreamableHTTP 10-50ms）だけで**接続レイテンシを最大50倍改善**できる
- `MultiServerMCPClient`のセッション永続化で**接続確立コストを初回のみに限定**し、2回目以降ゼロにできる
- ToolNodeの並列実行+プロンプト設計で**複数ツール呼び出しの総時間を52%削減**できる
- 動的ツールローディング+テキスト形式レスポンスで**コンテキスト消費を60-92%削減**しAPIコストを月額$700以上節約できる
- OpenTelemetryトレースで本番のMCPレイテンシを**継続計測・改善**できる

**次にやるべきこと:**
- [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters)をインストールし、1つのMCPサーバーでstdio接続を試す
- 既存のツール呼び出しをOpenTelemetryで計測し、レイテンシボトルネックを特定する
- LangGraphストリーミング（`astream_events`）と組み合わせてTTFTをさらに短縮する（参考: [LangGraphエージェント型RAGのレイテンシ最適化](https://zenn.dev/0h_n0/articles/433702e83b26ed)）

## 関連する深掘り記事

本記事で取り上げたMCPツール呼び出し最適化に関連する1次情報（arXiv論文・企業テックブログ）の解説記事です。

- [論文解説: MCPBench — MCPサーバレイテンシベンチマーク](https://0h-n0.github.io/posts/paper-2506-03233/) (arXiv:2506.03233) — stdio vs StreamableHTTP の定量比較
- [論文解説: MCP-Zero — Embeddingベースの動的ツール発見](https://0h-n0.github.io/posts/paper-2503-23278/) (arXiv:2503.23278) — 全ツール定義のコンテキスト注入を回避する手法
- [論文解説: OctoTools — DAGベース並列ツール実行](https://0h-n0.github.io/posts/paper-2502-18145/) (arXiv:2502.18145) — 依存関係解析による並列ツール実行スケジューリング
- [Anthropic解説: Code Execution with MCP — トークン消費98.7%削減](https://0h-n0.github.io/posts/techblog-anthropic-code-execution-mcp/) — コード実行によるプログラマティックツール呼び出し
- [論文解説: Multi-Agent Collaboration Mechanisms — マルチエージェント協調の分類体系](https://0h-n0.github.io/posts/paper-2501-06322/) (arXiv:2501.06322) — LLMマルチエージェント協調の5次元フレームワーク

## 参考

- [langchain-mcp-adapters GitHub](https://github.com/langchain-ai/langchain-mcp-adapters) - LangChain/LangGraph向けMCPアダプター公式リポジトリ
- [MCP Specification 2025-11-25 - Transports](https://modelcontextprotocol.io/specification/2025-11-25) - MCPトランスポート仕様（StreamableHTTP）
- [MCP Transport Protocols: stdio vs SSE vs StreamableHTTP](https://mcpcat.io/guides/comparing-stdio-sse-streamablehttp/) - トランスポート性能比較ガイド
- [MCP Server Performance Optimization](https://www.catchmetrics.io/blog/a-brief-introduction-to-mcp-server-performance-optimization) - MCPサーバーのペイロード最適化手法
- [LangGraph Agent Orchestration](https://www.langchain.com/langgraph) - LangGraph公式ドキュメント
- [Scaling LangGraph Agents: Parallelization](https://aipractitioner.substack.com/p/scaling-langgraph-agents-parallelization) - LangGraph並列実行パターン

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
