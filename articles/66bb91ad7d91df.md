---
title: "AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡æŒ‡æ¨™è¨­è¨ˆï¼šæœ¬ç•ªé‹ç”¨ã§æˆåŠŸç‡95%ã‚’å®Ÿç¾ã™ã‚‹æ¸¬å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"
emoji: "ğŸ“Š"
type: "tech"
topics: ["ai", "agent", "evaluation", "observability", "metrics"]
published: false
---

# AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡æŒ‡æ¨™è¨­è¨ˆï¼šæœ¬ç•ªé‹ç”¨ã§æˆåŠŸç‡95%ã‚’å®Ÿç¾ã™ã‚‹æ¸¬å®šãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã®3å±¤æ§‹é€ ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ã¨å„å±¤ã®å…·ä½“çš„æŒ‡æ¨™
- å¯è¦³æ¸¬æ€§89% vs è©•ä¾¡52%ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ä½“ç³»çš„è©•ä¾¡æ‰‹æ³•
- Microsoftãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§å®Ÿç¾ã™ã‚‹ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã®çµ±åˆ
- æœ¬ç•ªç’°å¢ƒã§95%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’é”æˆã™ã‚‹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- Maxim AIã€Langfuseã€Braintrustãªã©ä¸»è¦ãƒ„ãƒ¼ãƒ«7ç¨®ã®ç‰¹å¾´æ¯”è¼ƒã¨é¸å®šåŸºæº–

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æœ¬ç•ªç’°å¢ƒã«å°å…¥ä¸­ã®ä¸­ç´šè€…ã€œä¸Šç´šè€…
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - LLMãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºçµŒé¨“
  - Python 3.10+ã¾ãŸã¯TypeScriptã§ã®APIé–‹ç™º
  - åŸºæœ¬çš„ãªå¯è¦³æ¸¬æ€§æ¦‚å¿µï¼ˆãƒ­ã‚°ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ï¼‰ã®ç†è§£
  - CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åŸºæœ¬çŸ¥è­˜

## çµè«–ãƒ»æˆæœ

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ¬ç•ªé‹ç”¨ã«ãŠã„ã¦ã€**ä½“ç³»çš„ãªè©•ä¾¡æŒ‡æ¨™è¨­è¨ˆã«ã‚ˆã‚ŠæˆåŠŸç‡ã‚’95%ä»¥ä¸Šã«å‘ä¸Š**ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚LangChainã®èª¿æŸ»ã§ã¯ã€å¯è¦³æ¸¬æ€§ã‚’å®Ÿè£…æ¸ˆã¿ã®çµ„ç¹”ãŒ89%ã«é”ã™ã‚‹ä¸€æ–¹ã€ä½“ç³»çš„ãªã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã®ã¯52.4%ã«ç•™ã¾ã£ã¦ã„ã¾ã™ã€‚ã“ã®å·®ã‚’åŸ‹ã‚ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®æˆæœãŒå¾—ã‚‰ã‚Œã¾ã™:

- **å“è³ªå‘ä¸Š**: ã‚¿ã‚¹ã‚¯å®Œäº†ç‡ã‚’75% â†’ 95%ã«æ”¹å–„ï¼ˆMicrosoftäº‹ä¾‹ï¼‰
- **ã‚³ã‚¹ãƒˆå‰Šæ¸›**: ç›£è¦–ã‚³ã‚¹ãƒˆã‚’97%å‰Šæ¸›ï¼ˆGalileo Luna-2ãƒ¢ãƒ‡ãƒ«æ´»ç”¨ï¼‰
- **ä¿¡é ¼æ€§ç¢ºä¿**: P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’300msä»¥ä¸‹ã«ç¶­æŒã€ã‚¨ãƒ©ãƒ¼ç‡ã‚’2%æœªæº€ã«æŠ‘åˆ¶
- **ãƒ“ã‚¸ãƒã‚¹ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ**: åˆå›è§£æ±ºç‡ï¼ˆFCRï¼‰ã‚’70% â†’ 85%ã«å‘ä¸Š

æœ¬è¨˜äº‹ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®3å±¤è©•ä¾¡æ§‹é€ ã€ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã®çµ±åˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ãã—ã¦å®Ÿè£…å¯èƒ½ãªãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³èª¬ã—ã¾ã™ã€‚

## AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã®3å±¤æ§‹é€ 

AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®è¤‡é›‘ã•ã«å¿œã˜ã¦**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«ãƒ»RAGãƒ¬ãƒ™ãƒ«ãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®3å±¤**ã«åˆ†é¡ã•ã‚Œã¾ã™ã€‚å„å±¤ã§ç•°ãªã‚‹æŒ‡æ¨™ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€åŒ…æ‹¬çš„ãªå“è³ªç®¡ç†ãŒå®Ÿç¾ã—ã¾ã™ã€‚

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«è©•ä¾¡

å˜ä¸€ã‚¿ãƒ¼ãƒ³ã®LLMå‘¼ã³å‡ºã—ã‚’å¯¾è±¡ã¨ã—ãŸåŸºç¤çš„ãªè©•ä¾¡å±¤ã§ã™ã€‚

**ä¸»è¦æŒ‡æ¨™**:
- **å‡ºåŠ›å½¢å¼ã®æ­£ç¢ºæ€§**: JSON/XMLç­‰ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãç”Ÿæˆã•ã‚Œã‚‹ã‹
- **ãƒˆãƒ¼ãƒ³ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã®ä¸€è²«æ€§**: ãƒ“ã‚¸ãƒã‚¹æ–‡æ›¸ãƒ»ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå¯¾è©±ç­‰ã®æŒ‡å®šéµå®ˆç‡
- **å‘½ä»¤éµå®ˆç‡**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æŒ‡å®šã—ãŸåˆ¶ç´„æ¡ä»¶ã®å……è¶³åº¦

**å®Ÿè£…ä¾‹ï¼ˆPython + Pydanticï¼‰**:

```python
from pydantic import BaseModel, Field
from openai import OpenAI

class StructuredOutput(BaseModel):
    """å‡ºåŠ›å½¢å¼ã‚’å³å¯†ã«å®šç¾©"""
    summary: str = Field(description="è¦ç´„ï¼ˆ100æ–‡å­—ä»¥å†…ï¼‰")
    sentiment: str = Field(description="ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–/ä¸­ç«‹")
    confidence: float = Field(ge=0.0, le=1.0, description="ä¿¡é ¼åº¦")

client = OpenAI()

def evaluate_prompt_level(prompt: str, expected_format: type[BaseModel]):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«è©•ä¾¡"""
    response = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[{"role": "user", "content": prompt}],
        response_format=expected_format,
    )

    # 1. å‡ºåŠ›å½¢å¼ã®æ¤œè¨¼ï¼ˆPydanticãŒè‡ªå‹•å®Ÿæ–½ï¼‰
    parsed = response.choices[0].message.parsed

    # 2. ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã®æ¸¬å®š
    usage = response.usage
    cost = (usage.prompt_tokens * 0.0025 + usage.completion_tokens * 0.01) / 1000

    # 3. è©•ä¾¡çµæœã‚’è¨˜éŒ²
    return {
        "format_valid": True,  # Pydanticæ¤œè¨¼æˆåŠŸ
        "token_count": usage.total_tokens,
        "cost_usd": cost,
        "latency_ms": response.response_time_ms,
    }
```

**ãªãœã“ã®å®Ÿè£…ã‹**:
- Pydanticã®å‹æ¤œè¨¼ã«ã‚ˆã‚Šå‡ºåŠ›å½¢å¼ã®æ­£ç¢ºæ€§ã‚’ä¿è¨¼
- ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã‚’è‡ªå‹•æ¸¬å®šã—ã€çµŒæ¸ˆæ€§ã‚’å¯è¦–åŒ–
- å¤±æ•—æ™‚ã¯ `ValidationError` ã§åŸå› ã‚’ç‰¹å®šå¯èƒ½

**æ³¨æ„ç‚¹**:
> ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«è©•ä¾¡ã¯å˜ä¸€ã‚¿ãƒ¼ãƒ³é™å®šã§ã™ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚„ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’å«ã‚€ã‚±ãƒ¼ã‚¹ã§ã¯ã€æ¬¡ã®RAGãƒ¬ãƒ™ãƒ«ãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«è©•ä¾¡ãŒå¿…è¦ã§ã™ã€‚

### RAGãƒ¬ãƒ™ãƒ«è©•ä¾¡

æ¤œç´¢æ‹¡å¼µç”Ÿæˆï¼ˆRAGï¼‰ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã€**æ¤œç´¢å“è³ª**ã¨**ç”Ÿæˆç²¾åº¦**ã‚’ç‹¬ç«‹ã—ã¦è©•ä¾¡ã—ã¾ã™ã€‚

**æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºã®æŒ‡æ¨™**:

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|------|--------|----------|
| **Precision@K** | 0.8ä»¥ä¸Š | ä¸Šä½Kä»¶ä¸­ã®é–¢é€£æ–‡æ›¸å‰²åˆ |
| **Recall@K** | 0.7ä»¥ä¸Š | å…¨é–¢é€£æ–‡æ›¸ä¸­ã®æ¤œç´¢æˆåŠŸç‡ |
| **NDCG@K** | 0.75ä»¥ä¸Š | ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ªï¼ˆæ­£è¦åŒ–DCGï¼‰ |
| **Context Relevance** | 0.85ä»¥ä¸Š | æ¤œç´¢ãƒãƒ£ãƒ³ã‚¯ãŒè³ªå•ã«é–¢é€£ã™ã‚‹åº¦åˆã„ |

**ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã®æŒ‡æ¨™**:

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|------|--------|----------|
| **Answer Grounding** | 0.9ä»¥ä¸Š | å¿œç­”ãŒæ¤œç´¢æ–‡æ›¸ã«æ ¹æ‹ ã‚’æŒã¤å‰²åˆ |
| **Hallucination Rate** | 5%æœªæº€ | æ¤œç´¢æ–‡æ›¸ã«å­˜åœ¨ã—ãªã„æƒ…å ±ã®ç”Ÿæˆç‡ |
| **Completeness** | 0.85ä»¥ä¸Š | è³ªå•ã«å¯¾ã™ã‚‹ç¶²ç¾…æ€§ |

**å®Ÿè£…ä¾‹ï¼ˆRagas + LangChainï¼‰**:

```python
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
)
from datasets import Dataset

# RAGè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
eval_dataset = Dataset.from_dict({
    "question": ["LLMã®è©•ä¾¡æŒ‡æ¨™ã¯ï¼Ÿ"],
    "answer": ["ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ãƒ™ãƒ«ã€RAGãƒ¬ãƒ™ãƒ«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®3å±¤ã§ã™"],
    "contexts": [["LLMè©•ä¾¡ã¯3å±¤æ§‹é€ ã§å®Ÿæ–½ã•ã‚Œã‚‹ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè©•ä¾¡ã€RAGè©•ä¾¡ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã€‚"]],
    "ground_truth": ["3å±¤æ§‹é€ : ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€RAGã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"],
})

# Ragasã§è‡ªå‹•è©•ä¾¡
result = evaluate(
    eval_dataset,
    metrics=[
        context_precision,  # æ¤œç´¢ç²¾åº¦
        context_recall,     # æ¤œç´¢ç¶²ç¾…æ€§
        faithfulness,       # ç”Ÿæˆã®å¿ å®Ÿæ€§ï¼ˆå¹»è¦šæ¤œå‡ºï¼‰
        answer_relevancy,   # å¿œç­”ã®é–¢é€£æ€§
    ],
)

print(f"Context Precision: {result['context_precision']:.2f}")
print(f"Faithfulness: {result['faithfulness']:.2f}")
```

**ãªãœRagasã‚’é¸ã¶ã‹**:
- LLM-as-judgeã‚’æ´»ç”¨ã—ã€äººæ‰‹è©•ä¾¡ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›
- æ¤œç´¢ã¨ç”Ÿæˆã‚’ç‹¬ç«‹è©•ä¾¡ã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šå¯èƒ½
- LangChainãƒ»LlamaIndexã¨çµ±åˆå¯èƒ½

**è½ã¨ã—ç©´**:
> Ragasã®è©•ä¾¡ã‚¹ã‚³ã‚¢ã¯è©•ä¾¡ç”¨LLMã®å“è³ªã«ä¾å­˜ã—ã¾ã™ã€‚GPT-4oç­‰ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡å™¨ã¨ã—ã¦ä½¿ç”¨ã—ã€å®šæœŸçš„ã«äººé–“è©•ä¾¡ã§è¼ƒæ­£ã—ã¦ãã ã•ã„ã€‚

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«è©•ä¾¡

è¤‡æ•°ã®LLMå‘¼ã³å‡ºã—ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³çŠ¶æ…‹ã‚’æŒã¤ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ã€**ã‚¿ã‚¹ã‚¯é”æˆèƒ½åŠ›**ã¨**æ¨è«–å“è³ª**ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

**ä¸»è¦æŒ‡æ¨™**:

| æŒ‡æ¨™ | å®šç¾© | ç›®æ¨™å€¤ |
|------|------|--------|
| **Task Success Rate** | ã‚¿ã‚¹ã‚¯å®Œäº†ã®æˆåŠŸå‰²åˆ | 95%ä»¥ä¸Š |
| **Tool Selection Accuracy** | é©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã—ãŸå‰²åˆ | 90%ä»¥ä¸Š |
| **Reasoning Quality** | æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã®è«–ç†æ€§ï¼ˆLLM-as-judgeï¼‰ | 0.85ä»¥ä¸Š |
| **Error Recovery Rate** | ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å›å¾©æˆåŠŸç‡ | 80%ä»¥ä¸Š |
| **Average Turns to Completion** | ã‚¿ã‚¹ã‚¯å®Œäº†ã¾ã§ã®å¹³å‡ã‚¿ãƒ¼ãƒ³æ•° | 3ã‚¿ãƒ¼ãƒ³ä»¥ä¸‹ |

**å®Ÿè£…ä¾‹ï¼ˆLangSmithè©•ä¾¡ï¼‰**:

```python
from langsmith import Client, evaluate

client = Client()

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡é–¢æ•°
def task_success_evaluator(run, example):
    """ã‚¿ã‚¹ã‚¯æˆåŠŸåˆ¤å®š"""
    agent_output = run.outputs.get("output", "")
    expected = example.outputs.get("expected_result", "")

    # LLM-as-judgeã§æˆåŠŸåˆ¤å®š
    judge_prompt = f"""
    Agent Output: {agent_output}
    Expected Result: {expected}

    Did the agent successfully complete the task? Answer 'yes' or 'no'.
    """

    judge_response = client.chat.completions.create(
        model="gpt-4o-2024-08-06",
        messages=[{"role": "user", "content": judge_prompt}],
    )

    success = judge_response.choices[0].message.content.lower() == "yes"
    return {"score": 1.0 if success else 0.0}

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è©•ä¾¡
results = evaluate(
    target=my_agent_function,  # è©•ä¾¡å¯¾è±¡ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    data="agent-test-dataset",  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
    evaluators=[task_success_evaluator],
    experiment_prefix="agent-eval-v1",
)

print(f"Task Success Rate: {results['task_success_evaluator']['mean']:.1%}")
```

**ãªãœLangSmithã‹**:
- LangChainçµ±åˆã§ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã‚’è‡ªå‹•è¿½è·¡
- å„ã‚¿ãƒ¼ãƒ³ã§ã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã¨æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’å¯è¦–åŒ–
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆå¯èƒ½

**å®Ÿå‹™ã§ã®èª²é¡Œ**:
> ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã®æœ€å¤§ã®èª²é¡Œã¯ã€Œã‚¿ã‚¹ã‚¯æˆåŠŸã®å®šç¾©ã€ã§ã™ã€‚æ›–æ˜§ãªç›®æ¨™ï¼ˆã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’æº€è¶³ã•ã›ã‚‹ã€ï¼‰ã§ã¯ãªãã€æ¸¬å®šå¯èƒ½ãªåŸºæº–ï¼ˆã€Œ3ã‚¿ãƒ¼ãƒ³ä»¥å†…ã«æ­£ç¢ºãªå›ç­”ã‚’æä¾›ã€ï¼‰ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚

## ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã®çµ±åˆ

MicrosoftãŒæå”±ã™ã‚‹è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€**Understandingï¼ˆç†è§£ï¼‰ã€Reasoningï¼ˆæ¨è«–ï¼‰ã€Response Qualityï¼ˆå¿œç­”å“è³ªï¼‰**ã®3è»¸ã§ãƒ“ã‚¸ãƒã‚¹æˆæœã¨ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã‚’çµ±åˆã—ã¾ã™ã€‚

### 3è»¸è©•ä¾¡ãƒ¢ãƒ‡ãƒ«

#### 1. Understandingï¼ˆç†è§£ï¼‰

**ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™**:
- åˆå›ç†è§£æˆåŠŸç‡ï¼ˆFirst Intent Resolutionï¼‰: 85%ä»¥ä¸Š
- ãƒ¦ãƒ¼ã‚¶ãƒ¼å†è³ªå•ç‡: 15%æœªæº€

**ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™**:
- ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºç²¾åº¦: 0.9ä»¥ä¸Š
- ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ç²¾åº¦: 0.92ä»¥ä¸Š

**æ¸¬å®šæ–¹æ³•**:

```python
def evaluate_understanding(user_query: str, agent_interpretation: dict):
    """ç†è§£ç²¾åº¦ã®è©•ä¾¡"""
    # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã®æ­£ç¢ºæ€§
    expected_entities = {"product": "iPhone 15", "action": "è³¼å…¥"}
    extracted = agent_interpretation.get("entities", {})
    entity_accuracy = len(set(expected_entities) & set(extracted)) / len(expected_entities)

    # 2. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†é¡ã®æ­£ç¢ºæ€§
    expected_intent = "purchase_intent"
    predicted_intent = agent_interpretation.get("intent")
    intent_correct = expected_intent == predicted_intent

    return {
        "entity_accuracy": entity_accuracy,
        "intent_correct": int(intent_correct),
        "understanding_score": (entity_accuracy + int(intent_correct)) / 2,
    }
```

#### 2. Reasoningï¼ˆæ¨è«–ï¼‰

**ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™**:
- è«–ç†çš„ä¸€è²«æ€§: äººé–“è©•ä¾¡ã§4.0/5.0ä»¥ä¸Š
- å•é¡Œè§£æ±ºåŠ¹ç‡: å¹³å‡3ã‚¹ãƒ†ãƒƒãƒ—ä»¥å†…

**ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™**:
- æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã®å®Œå…¨æ€§: å…¨ã‚¹ãƒ†ãƒƒãƒ—ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹
- ãƒ„ãƒ¼ãƒ«é¸æŠã®æœ€é©æ€§: ä¸è¦ãªãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒç„¡ã„ã‹

**ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹å¯è¦–åŒ–**:

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor

# OpenTelemetryã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°è¨­å®š
provider = TracerProvider()
processor = SimpleSpanProcessor(ConsoleSpanExporter())
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

def agent_reasoning_step(step_name: str, context: dict):
    """æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹"""
    with tracer.start_as_current_span(step_name) as span:
        span.set_attribute("reasoning.context", str(context))
        span.set_attribute("reasoning.step_number", context.get("step", 0))

        # æ¨è«–å®Ÿè¡Œ
        result = execute_reasoning_logic(context)

        span.set_attribute("reasoning.result", str(result))
        span.set_attribute("reasoning.success", result.get("success", False))

        return result

# æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ
with tracer.start_as_current_span("agent_task") as root_span:
    root_span.set_attribute("task.id", "task-12345")

    step1 = agent_reasoning_step("analyze_query", {"query": "..."})
    step2 = agent_reasoning_step("select_tool", {"analysis": step1})
    step3 = agent_reasoning_step("execute_tool", {"tool": step2})
```

**ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®åˆ©ç‚¹**:
- å„æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã—ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®š
- ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®æ ¹æœ¬åŸå› ã‚’ç‰¹å®šï¼ˆã©ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å¤±æ•—ã—ãŸã‹ï¼‰
- åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’å¯è¦–åŒ–

#### 3. Response Qualityï¼ˆå¿œç­”å“è³ªï¼‰

**ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™**:
- é¡§å®¢æº€è¶³åº¦ï¼ˆCSATï¼‰: 4.5/5.0ä»¥ä¸Š
- åˆå›è§£æ±ºç‡ï¼ˆFCRï¼‰: 85%ä»¥ä¸Š

**ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™**:
- å¿œç­”ã®å®Œå…¨æ€§: è³ªå•ã®å…¨è¦ç´ ã«ç­”ãˆã¦ã„ã‚‹ã‹ï¼ˆ0.9ä»¥ä¸Šï¼‰
- å¿œç­”ã®ç°¡æ½”æ€§: ä¸è¦ãªæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ï¼ˆ0.85ä»¥ä¸Šï¼‰
- å®‰å…¨æ€§: æœ‰å®³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºç‡ï¼ˆ0%ç›®æ¨™ï¼‰

**ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«å®Ÿè£…ä¾‹**:

```python
from guardrails import Guard
from guardrails.validators import ValidLength, ToxicLanguage

# å¿œç­”å“è³ªã®ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«
guard = Guard.from_string(
    validators=[
        ValidLength(min=50, max=500, on_fail="reask"),  # å¿œç­”é•·ã•åˆ¶ç´„
        ToxicLanguage(threshold=0.8, on_fail="exception"),  # æœ‰å®³æ€§ãƒã‚§ãƒƒã‚¯
    ],
    description="å¿œç­”å“è³ªã®æ¤œè¨¼",
)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”ã‚’æ¤œè¨¼
validated_response = guard.parse(
    llm_output=agent_response,
    metadata={"user_id": "user-123"},
)

if validated_response.validation_passed:
    print("å¿œç­”å“è³ªOK")
else:
    print(f"æ¤œè¨¼å¤±æ•—: {validated_response.error}")
```

### çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­è¨ˆ

ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã‚’å˜ä¸€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç›£è¦–ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³:

```python
# Datadogãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡ä¾‹
from datadog import initialize, statsd

initialize(api_key="YOUR_API_KEY", app_key="YOUR_APP_KEY")

def report_agent_metrics(
    understanding_score: float,
    reasoning_steps: int,
    response_quality: float,
    user_satisfaction: float,
):
    """ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã‚’çµ±åˆé€ä¿¡"""
    # ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™
    statsd.gauge("agent.understanding.score", understanding_score)
    statsd.gauge("agent.reasoning.steps", reasoning_steps)
    statsd.gauge("agent.response.quality", response_quality)

    # ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™
    statsd.gauge("agent.user.satisfaction", user_satisfaction)

    # è¤‡åˆæŒ‡æ¨™ï¼ˆUnderstanding Ã— Reasoning Ã— Response Qualityï¼‰
    composite_score = understanding_score * (1 / reasoning_steps) * response_quality
    statsd.gauge("agent.composite.score", composite_score)
```

**ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç›£è¦–ã™ã¹ãKPI**:

| æŒ‡æ¨™ã‚«ãƒ†ã‚´ãƒª | å…·ä½“çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤ |
|------------|---------------|------------|
| **ä¿¡é ¼æ€§** | P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | 300msè¶… |
| **ä¿¡é ¼æ€§** | ã‚¨ãƒ©ãƒ¼ç‡ | 2%è¶… |
| **å“è³ª** | ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ | 95%æœªæº€ |
| **å“è³ª** | å¹»è¦šæ¤œå‡ºç‡ | 5%è¶… |
| **ã‚³ã‚¹ãƒˆ** | ãƒˆãƒ¼ã‚¯ãƒ³/ãƒªã‚¯ã‚¨ã‚¹ãƒˆ | äºˆç®—ã®120%è¶… |
| **ãƒ“ã‚¸ãƒã‚¹** | é¡§å®¢æº€è¶³åº¦ï¼ˆCSATï¼‰ | 4.5/5.0æœªæº€ |

## å¯è¦³æ¸¬æ€§ã¨è©•ä¾¡ã®çµ±åˆå®Ÿè£…

LangChainã®èª¿æŸ»ã§æ˜ã‚‰ã‹ã«ãªã£ãŸã€Œå¯è¦³æ¸¬æ€§89% vs è©•ä¾¡52%ã€ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ã«ã¯ã€**Observability-by-Design**ã®åŸå‰‡ã§æœ€åˆã‹ã‚‰è©•ä¾¡ã‚’çµ„ã¿è¾¼ã¿ã¾ã™ã€‚

### å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°é§†å‹•è©•ä¾¡

**4ã¤ã®ã‚³ã‚¢è¦ç´ **:

1. **ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°**: å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜éŒ²
2. **ãƒ­ã‚°**: æ§‹é€ åŒ–ã•ã‚ŒãŸJSONå½¢å¼ã§ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¨˜éŒ²
3. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: æ•°å€¤æŒ‡æ¨™ï¼ˆãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€ã‚³ã‚¹ãƒˆï¼‰ã‚’é›†è¨ˆ
4. **è©•ä¾¡**: è‡ªå‹•ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆLLM-as-judge + ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ï¼‰

**å®Ÿè£…ä¾‹ï¼ˆBraintrustçµ±åˆï¼‰**:

```python
from braintrust import init_logger, current_span

# Braintrustãƒ­ã‚¬ãƒ¼åˆæœŸåŒ–
logger = init_logger(project="agent-evaluation", api_key="YOUR_API_KEY")

def agent_with_observability(user_query: str):
    """å¯è¦³æ¸¬æ€§ã‚’çµ„ã¿è¾¼ã‚“ã ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ"""
    with logger.start_span(name="agent_task", input={"query": user_query}) as span:
        # 1. ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°: ã‚¯ã‚¨ãƒªåˆ†æ
        with logger.start_span(name="analyze_query") as analysis_span:
            analysis = analyze_user_query(user_query)
            analysis_span.log(output=analysis)

        # 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹: ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
        span.log(metrics={"tokens_used": 150, "cost_usd": 0.0045})

        # 3. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚’ãƒˆãƒ¬ãƒ¼ã‚¹
        with logger.start_span(name="tool_execution") as tool_span:
            tool_result = execute_tool(analysis["selected_tool"])
            tool_span.log(output=tool_result)

        # 4. æœ€çµ‚å¿œç­”ç”Ÿæˆ
        response = generate_response(analysis, tool_result)

        # 5. è‡ªå‹•è©•ä¾¡ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã«ä»˜ä¸
        with logger.start_span(name="evaluation") as eval_span:
            eval_result = evaluate_response(user_query, response)
            eval_span.log(
                scores={
                    "task_success": eval_result["success"],
                    "response_quality": eval_result["quality_score"],
                }
            )

        span.log(output={"response": response})
        return response

# å®Ÿè¡Œ
result = agent_with_observability("æœ€æ–°ã®iPhoneã‚’è³¼å…¥ã—ãŸã„")
```

**ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ©ç‚¹**:
- ãƒˆãƒ¬ãƒ¼ã‚¹ã¨è©•ä¾¡ã‚’åŒã˜ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ç®¡ç†
- æœ¬ç•ªç’°å¢ƒã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è‡ªå‹•çš„ã«ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¤‰æ›
- CI/CDã§åŒã˜è©•ä¾¡åŸºæº–ã‚’é©ç”¨å¯èƒ½

### å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³2: CI/CDçµ±åˆè©•ä¾¡

**ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã®è‡ªå‹•è©•ä¾¡**:

```yaml
# .github/workflows/agent-evaluation.yml
name: Agent Evaluation

on:
  pull_request:
    branches: [main]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run Agent Evaluation
        run: |
          python scripts/evaluate_agent.py \
            --dataset tests/agent_test_cases.json \
            --baseline-threshold 0.90

      - name: Upload Evaluation Report
        uses: actions/upload-artifact@v3
        with:
          name: evaluation-report
          path: evaluation_results.json
```

**evaluate_agent.pyã®å®Ÿè£…**:

```python
# scripts/evaluate_agent.py
import json
from pathlib import Path
from langsmith import Client, evaluate

def load_test_cases(dataset_path: str):
    """ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹èª­ã¿è¾¼ã¿"""
    return json.loads(Path(dataset_path).read_text())

def run_evaluation(dataset_path: str, baseline_threshold: float):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã‚’å®Ÿè¡Œ"""
    client = Client()

    # LangSmithã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡
    results = evaluate(
        target=my_production_agent,
        data=dataset_path,
        evaluators=[task_success_evaluator, quality_evaluator],
        experiment_prefix="pr-evaluation",
    )

    # æˆåŠŸç‡ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸‹å›ã£ãŸã‚‰CIå¤±æ•—
    task_success = results["task_success_evaluator"]["mean"]
    if task_success < baseline_threshold:
        raise ValueError(
            f"Task success rate {task_success:.1%} below threshold {baseline_threshold:.1%}"
        )

    print(f"âœ… Evaluation passed: {task_success:.1%} success rate")
    return results

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", required=True)
    parser.add_argument("--baseline-threshold", type=float, default=0.90)
    args = parser.parse_args()

    run_evaluation(args.dataset, args.baseline_threshold)
```

**CI/CDçµ±åˆã®åŠ¹æœ**:
- ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«å“è³ªã‚’ä¿è¨¼
- ãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ï¼ˆå“è³ªåŠ£åŒ–ï¼‰ã‚’è‡ªå‹•æ¤œå‡º
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ã®å½±éŸ¿ã‚’å®šé‡è©•ä¾¡

### å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³3: OpenTelemetryæ¨™æº–åŒ–

**ãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³å›é¿**:

```python
from opentelemetry import trace, metrics
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader

# OTLPã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼è¨­å®šï¼ˆDatadog/Grafana/Jaegeräº’æ›ï¼‰
trace_provider = TracerProvider()
trace_provider.add_span_processor(
    BatchSpanProcessor(OTLPSpanExporter(endpoint="http://localhost:4317"))
)
trace.set_tracer_provider(trace_provider)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
metric_reader = PeriodicExportingMetricReader(
    OTLPSpanExporter(endpoint="http://localhost:4317"),
    export_interval_millis=5000,
)
metrics.set_meter_provider(MeterProvider(metric_readers=[metric_reader]))

# ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ã¨ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å–å¾—
tracer = trace.get_tracer("agent.tracer")
meter = metrics.get_meter("agent.meter")

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
task_success_counter = meter.create_counter(
    name="agent.task.success",
    description="Number of successful task completions",
)
latency_histogram = meter.create_histogram(
    name="agent.task.latency",
    description="Task completion latency in milliseconds",
    unit="ms",
)

def agent_with_otel(query: str):
    """OpenTelemetryæ¨™æº–åŒ–ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    with tracer.start_as_current_span("agent_execution") as span:
        span.set_attribute("query", query)

        start_time = time.time()
        result = execute_agent_logic(query)
        latency_ms = (time.time() - start_time) * 1000

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        task_success_counter.add(1, {"status": "success"})
        latency_histogram.record(latency_ms, {"model": "gpt-4o"})

        span.set_attribute("latency_ms", latency_ms)
        return result
```

**OpenTelemetryæ¡ç”¨ã®åˆ©ç‚¹**:
- è¤‡æ•°ã®å¯è¦³æ¸¬æ€§ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ï¼ˆDatadogã€Grafanaã€Jaegerï¼‰ã§å…±é€šãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨
- ãƒ„ãƒ¼ãƒ«å¤‰æ›´æ™‚ã®ãƒ­ãƒƒã‚¯ã‚¤ãƒ³è§£æ¶ˆ
- æ¥­ç•Œæ¨™æº–ã«æº–æ‹ 

## ä¸»è¦ãƒ„ãƒ¼ãƒ«æ¯”è¼ƒã¨é¸å®šåŸºæº–

2026å¹´ã®ä¸»è¦ãªAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ãƒ»å¯è¦³æ¸¬æ€§ãƒ„ãƒ¼ãƒ«ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### ãƒ„ãƒ¼ãƒ«ä¸€è¦§ã¨ç‰¹å¾´

| ãƒ„ãƒ¼ãƒ« | ä¸»è¦æ©Ÿèƒ½ | æœ€é©ãªç”¨é€” | ä¾¡æ ¼å¸¯ |
|--------|----------|-----------|--------|
| **Maxim AI** | ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€è©•ä¾¡ã€å¯è¦³æ¸¬æ€§ã®çµ±åˆ | ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç®¡ç† | å•†ç”¨ï¼ˆè¦å•åˆã›ï¼‰ |
| **Langfuse** | ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å¯è¦³æ¸¬æ€§ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç† | ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«é‡è¦–ã€è‡ªç¤¾ãƒ›ã‚¹ãƒˆ | ç„¡æ–™ï¼ˆOSSï¼‰ |
| **Braintrust** | è©•ä¾¡ä¸»å°ã€CI/CDçµ±åˆã€è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åŒ– | é–‹ç™ºâ†’æœ¬ç•ªã®ä¸€è²«è©•ä¾¡ | å•†ç”¨ï¼ˆä½¿ç”¨é‡èª²é‡‘ï¼‰ |
| **LangSmith** | LangChainçµ±åˆã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³è©•ä¾¡ | LangChainå°‚ç”¨é–‹ç™º | å•†ç”¨ï¼ˆæœˆé¡$39ã€œï¼‰ |
| **Arize** | MLå¯è¦³æ¸¬æ€§ã€ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥ | ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®é•·æœŸç›£è¦– | å•†ç”¨ï¼ˆè¦å•åˆã›ï¼‰ |
| **Galileo** | Luna-2å°è¨€èªãƒ¢ãƒ‡ãƒ«ã€ä½ã‚³ã‚¹ãƒˆç›£è¦– | ã‚³ã‚¹ãƒˆæœ€é©åŒ–é‡è¦– | å•†ç”¨ï¼ˆ97%å‰Šæ¸›å®Ÿç¸¾ï¼‰ |
| **Datadog LLM Obs** | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºçµ±åˆã€æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ©é€£æº | å¤§è¦æ¨¡çµ„ç¹”ã€æ—¢å­˜Datadogåˆ©ç”¨ | å•†ç”¨ï¼ˆæ—¢å­˜å¥‘ç´„ã«è¿½åŠ ï¼‰ |

### é¸å®šåŸºæº–

**ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹åˆ¥ã®æ¨å¥¨**:

1. **LangChainå°‚ç”¨é–‹ç™º** â†’ **LangSmith**
   - ãƒã‚¤ãƒ†ã‚£ãƒ–çµ±åˆã§è¨­å®šä¸è¦
   - ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ã®è‡ªå‹•ãƒˆãƒ¬ãƒ¼ã‚¹

2. **ãƒ‡ãƒ¼ã‚¿æ‰€æœ‰ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é‡è¦–** â†’ **Langfuse**
   - è‡ªç¤¾ãƒ›ã‚¹ãƒˆå¯èƒ½ï¼ˆã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹å±•é–‹ï¼‰
   - å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«

3. **CI/CDçµ±åˆã¨å›å¸°ãƒ†ã‚¹ãƒˆ** â†’ **Braintrust**
   - æœ¬ç•ªãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹åŒ–
   - GitHub Actionsçµ±åˆ

4. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–** â†’ **Galileo**
   - Luna-2å°è¨€èªãƒ¢ãƒ‡ãƒ«ã§ç›£è¦–ã‚³ã‚¹ãƒˆ97%å‰Šæ¸›
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯

5. **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ»æ—¢å­˜ã‚¤ãƒ³ãƒ•ãƒ©çµ±åˆ** â†’ **Datadog LLM Observability**
   - æ—¢å­˜APMãƒ»ãƒ­ã‚°ã¨çµ±åˆ
   - SLAç®¡ç†ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆåŸºç›¤

6. **ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ç®¡ç†** â†’ **Maxim AI**
   - ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€è©•ä¾¡ã€å¯è¦³æ¸¬æ€§ã‚’çµ±åˆ
   - ãƒãƒ¼ã‚³ãƒ¼ãƒ‰UI

### å®Ÿè£…ä¾‹: è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã®ä½µç”¨

**é–‹ç™ºç’°å¢ƒ: Langfuseï¼ˆã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ï¼‰**
**æœ¬ç•ªç’°å¢ƒ: Datadog LLM Observabilityï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼‰**

```python
import os

# ç’°å¢ƒå¤‰æ•°ã§åˆ‡ã‚Šæ›¿ãˆ
if os.getenv("ENV") == "production":
    # Datadogçµ±åˆ
    from ddtrace.llmobs import LLMObs
    LLMObs.enable(
        ml_app="agent-production",
        api_key=os.getenv("DATADOG_API_KEY"),
    )
    observability_backend = "datadog"
else:
    # Langfuseçµ±åˆï¼ˆé–‹ç™ºç’°å¢ƒï¼‰
    from langfuse import Langfuse
    langfuse = Langfuse(
        public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
        secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
    )
    observability_backend = "langfuse"

def trace_agent_execution(query: str):
    """ç’°å¢ƒã«å¿œã˜ãŸå¯è¦³æ¸¬æ€§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰"""
    if observability_backend == "datadog":
        # Datadog APMãƒˆãƒ¬ãƒ¼ã‚¹
        import ddtrace
        with ddtrace.tracer.trace("agent.task", service="agent") as span:
            span.set_tag("query", query)
            result = run_agent(query)
            return result
    else:
        # Langfuseãƒˆãƒ¬ãƒ¼ã‚¹
        trace = langfuse.trace(name="agent_task", input={"query": query})
        result = run_agent(query)
        trace.update(output={"result": result})
        return result
```

## ã‚ˆãã‚ã‚‹è½ã¨ã—ç©´ã¨è§£æ±ºç­–

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| **è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒä¸å®‰å®š** | LLM-as-judgeã®éæ±ºå®šæ€§ | GPT-4oç­‰ã®é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã€temperature=0ã«è¨­å®šã€‚è¤‡æ•°å›è©•ä¾¡ã®å¹³å‡ã‚’æ¡ç”¨ã€‚ |
| **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸è¶³** | æ‰‹å‹•ä½œæˆã®ã‚³ã‚¹ãƒˆ | æœ¬ç•ªãƒˆãƒ¬ãƒ¼ã‚¹ã‹ã‚‰è‡ªå‹•ç”Ÿæˆï¼ˆBraintrustç­‰ï¼‰ã€‚åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆMaxim AIï¼‰ã€‚ |
| **å¯è¦³æ¸¬æ€§ã‚³ã‚¹ãƒˆã®çˆ†ç™º** | å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ | ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç‡ã‚’è¨­å®šï¼ˆ1%ã€œ10%ï¼‰ã€‚é‡è¦ãªã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹ã¯100%ãƒˆãƒ¬ãƒ¼ã‚¹ã€‚ |
| **è©•ä¾¡æŒ‡æ¨™ã®å®šç¾©æ›–æ˜§** | ãƒ“ã‚¸ãƒã‚¹ç›®æ¨™ã®ä¸æ˜ç¢ºåŒ– | SMARTåŸºæº–ï¼ˆSpecific, Measurable, Achievable, Relevant, Time-boundï¼‰ã§å®šç¾©ã€‚ |
| **äººé–“è©•ä¾¡ã¨ã®ä¹–é›¢** | LLM-as-judgeã®éä¿¡ | å®šæœŸçš„ã«äººé–“è©•ä¾¡ã§è¼ƒæ­£ï¼ˆæœˆæ¬¡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã€‚ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ã¯**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»RAGãƒ»ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®3å±¤æ§‹é€ **ã§å®Ÿæ–½ã—ã€å„å±¤ã§ç•°ãªã‚‹æŒ‡æ¨™ã‚’é©ç”¨
- **å¯è¦³æ¸¬æ€§89% vs è©•ä¾¡52%ã®ã‚®ãƒ£ãƒƒãƒ—**ã‚’åŸ‹ã‚ã‚‹ã«ã¯ã€Observability-by-Designã§æœ€åˆã‹ã‚‰è©•ä¾¡ã‚’çµ„ã¿è¾¼ã‚€
- **Microsoftã®3è»¸ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**ï¼ˆUnderstandingãƒ»Reasoningãƒ»Response Qualityï¼‰ã§ãƒ“ã‚¸ãƒã‚¹æŒ‡æ¨™ã¨ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™ã‚’çµ±åˆ
- **OpenTelemetryæ¨™æº–åŒ–**ã«ã‚ˆã‚Šãƒ™ãƒ³ãƒ€ãƒ¼ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ã‚’å›é¿ã—ã€è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã‚’æŸ”è»Ÿã«çµ„ã¿åˆã‚ã›ã‚‹
- LangChainèª¿æŸ»ã§ã¯**57.3%ã®çµ„ç¹”ãŒæœ¬ç•ªé‹ç”¨ä¸­**ã€å“è³ªãŒæœ€å¤§ã®èª²é¡Œï¼ˆ33%ï¼‰

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

1. **ç¾çŠ¶è©•ä¾¡**: ç¾åœ¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ã‚’æ¸¬å®šï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹ï¼‰
2. **è©•ä¾¡æŒ‡æ¨™ã®å®šç¾©**: ãƒ“ã‚¸ãƒã‚¹ç›®æ¨™ã«åŸºã¥ã„ãŸSMARTåŸºæº–ã®æŒ‡æ¨™è¨­å®š
3. **æœ€å°é™ã®ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…**: OpenTelemetryã§åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ¼ã‚¹è¨˜éŒ²ã‚’é–‹å§‹
4. **CI/CDçµ±åˆ**: LangSmithã¾ãŸã¯Braintrustã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’è‡ªå‹•åŒ–
5. **æœ¬ç•ªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°**: Datadog/Langfuseã§å¯è¦³æ¸¬æ€§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ§‹ç¯‰

## å‚è€ƒ

- [Top 5 AI Agent Evaluation Tools in 2026 | Maxim AI](https://www.getmaxim.ai/articles/top-5-ai-agent-evaluation-tools-in-2026/)
- [The Complete Guide to LLM & AI Agent Evaluation in 2026 | Adaline](https://www.adaline.ai/blog/complete-guide-llm-ai-agent-evaluation-2026)
- [The complete guide to LLM observability for 2026 | Portkey](https://portkey.ai/blog/the-complete-guide-to-llm-observability/)
- [AI Agent Performance Measurement: Redefining Excellence | Microsoft](https://www.microsoft.com/en-us/dynamics-365/blog/it-professional/2026/02/04/ai-agent-performance-measurement/)
- [State of AI Agents | LangChain](https://www.langchain.com/state-of-agent-engineering)
- [5 best AI agent observability tools for agent reliability in 2026 | Braintrust](https://www.braintrust.dev/articles/best-ai-agent-observability-tools-2026)
- [AI Agent Monitoring: Best Practices, Tools, and Metrics for 2026 | UptimeRobot](https://uptimerobot.com/knowledge-hub/monitoring/ai-agent-monitoring-best-practices-tools-and-metrics/)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #52](https://github.com/0h-n0/zen-auto-create-article/issues/52) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
