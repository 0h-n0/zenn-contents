---
title: "AWS re:Invent 2025速報：AI時代のインフラ革新を読み解く"
emoji: "🚀"
type: "tech"
topics: ["aws", "reinvent", "ai", "machinelearning", "cloud"]
published: true
---

## 導入

2025年12月1〜5日に開催されたAWS re:Invent 2025は、AI実装を大幅に加速させるインフラとサービスの発表が相次ぎました。特に注目すべきは、**Amazon Nova 2モデルファミリーの発表、Trainium3 UltraServersによる4.4倍の性能向上、S3のオブジェクトサイズ上限10倍拡大（50TB）、そしてS3 Vectorsの一般提供開始**です。

これらのアップデートは、AI開発のボトルネックとなっていた「推論コスト」「学習速度」「ベクトルデータベースの運用コスト」を一挙に解消する可能性を秘めています。

この記事では、re:Invent 2025の主要発表を実務視点で整理し、2026年のAI実装でどのように活用できるかを解説します。

## Amazon Nova 2: AWS独自の推論モデル

### モデルファミリー概要

Amazon Nova 2は、推論能力・マルチモーダル処理・コード生成において業界トップクラスの価格性能比を実現したAWS独自の基盤モデルです。最大の特徴は、**3つの思考強度レベル（低・中・高）を備えた段階的推論機能**と、**100万トークンのコンテキストウィンドウ**です。

### モデルラインナップ

**Nova 2 Lite（一般提供）:**
高速で費用対効果の高い推論モデルです。カスタマーサービスのチャットボット、ドキュメント処理、ビジネスプロセスの自動化に適しています。先行アクセスを利用している企業は、既存のGPT-3.5ベースのシステムから移行することで、**コストを40〜60%削減しながら応答品質を維持**できたと報告されています。

**Nova 2 Pro（プレビュー）:**
マルチドキュメント分析、動画推論、ソフトウェア移行などの非常に複雑なエージェントタスクに対応します。特にコードマイグレーション案件では、レガシーシステムのコードベース全体（数十万行）を一度に分析できる100万トークンのコンテキストが威力を発揮します。

**Nova 2 Omni（プレビュー）:**
業界初の**マルチモーダル推論モデル**です。テキスト・画像・動画・音声を入力として受け取り、テキストと画像を出力します。製造業の品質検査（動画から欠陥検出）や医療画像診断など、視覚情報を扱うエージェントタスクでの利用が期待されます。

### 実装のポイント

Nova 2モデルはAmazon Bedrockを通じて利用可能で、コードインタープリターやウェブグラウンディングなどの組み込みツールに加え、**リモートMCP（Model Context Protocol）ツール**もサポートしています。これにより、外部APIやデータベースとの連携が容易になり、エージェントの実装工数を大幅に削減できます。

```python
import boto3

# Amazon Bedrock経由でNova 2 Liteを利用
bedrock = boto3.client('bedrock-runtime')

response = bedrock.invoke_model(
    modelId='amazon.nova-2-lite-v1:0',
    body={
        'inputText': 'このドキュメントから重要な契約条項を抽出してください',
        'inferenceConfig': {
            'thinkingIntensity': 'medium'  # 推論強度を指定
        }
    }
)
```

## Trainium3 UltraServers: AI学習の4.4倍高速化

### 性能スペック

Trainium3は、AWS独自開発の機械学習専用チップです。各チップは**2.52 PFLOPs（ペタフロップス）のFP8コンピュート性能**を持ち、Trainium2と比較して以下の改善を実現しています：

- **メモリ容量**: 1.5倍（144GB HBM3e）
- **メモリ帯域幅**: 1.7倍（4.9TB/s）
- **チップあたりスループット**: 3倍
- **レスポンスタイム**: 4倍高速

### UltraServerの統合システム

Trn3 UltraServersは**最大144個のTrainium3チップ**を統合でき、合計362 PFLOPsの演算能力を提供します。完全構成では以下のスペックになります：

- **総メモリ**: 20.7TB HBM3e
- **総メモリ帯域幅**: 706TB/s
- **ネットワーク**: NeuronLink-v4による2TB/s/チップのオールツーオールファブリック

### コスト効率とエネルギー効率

Trainium2 UltraServersと比較して、Trn3は：

- **性能**: 4.4倍向上
- **メモリ帯域幅**: 3.9倍向上
- **ワットあたり性能**: 4倍改善

この性能向上により、大規模言語モデルの学習コストが実質的に**1/4以下**になる計算です。データセンターの電力消費も40%削減されるため、環境負荷の低減にも貢献します。

### 実務での活用シーン

GPT-OSSモデルを使用したベンチマークでは、顧客はTrainium2に比べて**チップあたり3倍のスループット**を達成しました。これは、同じ予算でモデル学習の試行回数を3倍に増やせることを意味します。

スタートアップや研究機関にとって、この高速化は競争力を大きく左右します。A/Bテストのサイクルを週単位から日単位に短縮できれば、モデル改善のイテレーション速度が飛躍的に向上します。

## Amazon S3の大幅アップデート

### 50TBオブジェクトのサポート

これまでS3の単一オブジェクトサイズ上限は5TBでしたが、re:Invent 2025で**50TBに拡大**されました。これにより、以下のユースケースが容易になります：

- **大規模動画ファイル**: 4K/8K動画の長時間収録データを分割せずに保存
- **AI学習データセット**: ImageNetやCommon Crawlクラスの大規模データセットを単一オブジェクトとして管理
- **シミュレーション結果**: 科学技術計算や気象シミュレーションの結果ファイル

従来は5TBを超えるファイルをマルチパートアップロードで分割管理する必要がありましたが、その運用負荷が解消されます。

### S3 Vectors: ベクトルストレージの革新

S3 Vectorsは、**クラウドストレージで初めてベクトルデータをネイティブサポート**する機能です。一般提供開始により、以下の仕様が実現しました：

- **インデックスあたり20億ベクトル**をサポート
- **バケットあたり10,000インデックス**まで拡張可能
- **専用ベクトルDBと比較して最大90%のコスト削減**
- **クエリレイテンシ100ミリ秒以下**（頻繁なクエリの場合）

### 実装例

```python
import boto3

s3_client = boto3.client('s3')

# ベクトルインデックスの作成
s3_client.create_vector_index(
    Bucket='my-vector-bucket',
    IndexName='product-embeddings',
    Dimensions=768,  # ベクトル次元数
    SimilarityMetric='COSINE'
)

# ベクトルの登録
s3_client.put_vector(
    Bucket='my-vector-bucket',
    IndexName='product-embeddings',
    VectorId='product-001',
    Vector=[0.123, 0.456, ...],  # 768次元のembedding
    Metadata={'product_name': 'Sample Product'}
)

# 類似ベクトル検索
response = s3_client.query_vectors(
    Bucket='my-vector-bucket',
    IndexName='product-embeddings',
    QueryVector=[0.111, 0.222, ...],
    TopK=10
)
```

### Amazon Bedrock Knowledge Basesとの統合

S3 Vectorsは**Amazon Bedrock Knowledge Bases**とネイティブ統合されており、RAG（Retrieval-Augmented Generation）アプリケーションのインフラコストを大幅に削減できます。

従来、PineconeやWeaviateなどの専用ベクトルDBを別途運用する必要がありましたが、S3 Vectorsを使えばストレージとベクトル検索を統合管理できます。運用負荷とコストの両面でメリットがあります。

## AWS AI Factories: オンプレミスへのAIインフラ展開

AWS AI Factoriesは、**顧客の既存データセンターにAWS専用インフラを設置**する新サービスです。以下の構成が提供されます：

- **コンピュート**: Nvidia GPU、AWS Trainium3チップ
- **ネットワーク**: 高速相互接続
- **ストレージ**: S3互換ストレージ
- **マネージドサービス**: Amazon Bedrock、SageMaker

### 利用シーン

金融機関や医療機関など、**データを外部クラウドに送信できない規制要件**がある組織向けです。オンプレミスでAWSクラウドと同等のAI開発環境を構築できるため、セキュリティとパフォーマンスを両立できます。

## まとめ

AWS re:Invent 2025の発表は、AI実装の3大ボトルネックを解消するものでした：

- **推論コスト**: Nova 2 Liteで40〜60%削減
- **学習速度**: Trainium3で4.4倍高速化
- **ベクトルDB運用**: S3 Vectorsで90%コスト削減

特に、Nova 2の100万トークンコンテキストとS3 Vectorsの20億ベクトル対応は、**大規模RAGシステムの実用化を後押し**します。2026年は、これらの新機能を活用したAIエージェントの実装が本格化する年になるでしょう。

次のステップとして、まずはNova 2 LiteをAmazon Bedrockで試し、既存のOpenAI APIベースのアプリケーションと性能比較することをお勧めします。コスト削減効果を実測できれば、本格的な移行判断がしやすくなります。

## 参考

- [AWS re:Invent 2025 での注目の発表](https://aws.amazon.com/jp/blogs/news/top-announcements-of-aws-reinvent-2025/)
- [【AWS re:Invent 2025】AI関連のサービスアップデートまとめ](https://iret.media/177677)
- [【re:Invent 2025】Amazon Nova 2 に関するアップデートの一覧化とまとめ](https://blog.serverworks.co.jp/amazon-nova-2-summary)
- [Trainium3 UltraServer delivers faster AI training at lower cost](https://www.aboutamazon.com/news/aws/trainium-3-ultraserver-faster-ai-training-lower-cost)
- [Amazon S3 Vectors がスケールとパフォーマンスを向上させて一般提供開始](https://aws.amazon.com/jp/blogs/news/amazon-s3-vectors-now-generally-available-with-increased-scale-and-performance/)

詳細なリサーチ内容は Issue（作成予定）を参照してください。

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
