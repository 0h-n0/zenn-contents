---
title: "LLMã‚¢ãƒ—ãƒªã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼š40ä¸‡GPUç¨¼åƒã®çŸ¥è¦‹ã‹ã‚‰å­¦ã¶æœ¬ç•ªé‹ç”¨æˆ¦ç•¥"
emoji: "ğŸš€"
type: "tech"
topics: ["llm", "scaling", "kubernetes", "gpu", "infrastructure"]
published: false
---

# LLMã‚¢ãƒ—ãƒªã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè·µã‚¬ã‚¤ãƒ‰ï¼š40ä¸‡GPUç¨¼åƒã®çŸ¥è¦‹ã‹ã‚‰å­¦ã¶æœ¬ç•ªé‹ç”¨æˆ¦ç•¥

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LLMæ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼ˆã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ´»ç”¨ï¼‰
- vLLMã€SGLangã€TensorRT-LLMã®æ€§èƒ½æ¯”è¼ƒã¨é¸å®šåŸºæº–ï¼ˆ180-220 req/s vs 120-160 req/sï¼‰
- æœ¬ç•ªç’°å¢ƒã§40ä¸‡GPUç¨¼åƒã‚’å®Ÿç¾ã™ã‚‹SGLangã®RadixAttentionæŠ€è¡“
- Kubernetes HPAã«ã‚ˆã‚‹æ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- Prefill/Decodeåˆ†é›¢ã«ã‚ˆã‚‹ç•°ç¨®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åŠ¹ç‡çš„æ´»ç”¨

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šè€…ä»¥ä¸Šã®LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºè€…ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Kubernetesï¼ˆHPAã€Podã€Deploymentï¼‰ã®åŸºæœ¬æ“ä½œ
  - Python/FastAPIç­‰ã§ã®APIå®Ÿè£…çµŒé¨“
  - Docker/ã‚³ãƒ³ãƒ†ãƒŠã®åŸºç¤ç†è§£
  - LLMæ¨è«–ã®åŸºæœ¬æ¦‚å¿µï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒˆãƒ¼ã‚¯ãƒ³ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼‰

## çµè«–ãƒ»æˆæœ

LLMæ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯ã€**å¾“æ¥ã®Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯ç•°ãªã‚‹æˆ¦ç•¥**ãŒå¿…è¦ã§ã™ã€‚æœ¬è¨˜äº‹ã§è§£èª¬ã™ã‚‹æ‰‹æ³•ã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®æˆæœãŒæœŸå¾…ã§ãã¾ã™ï¼š

- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·äºˆæ¸¬ç²¾åº¦90%ä»¥ä¸Š**: ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ1.5å€å‘ä¸Š**: TensorRT-LLMæ¡ç”¨æ™‚ï¼ˆvLLMæ¯”è¼ƒï¼‰
- **GPUç¨¼åƒç‡40%æ”¹å–„**: Prefill/Decodeåˆ†é›¢ã«ã‚ˆã‚‹ç•°ç¨®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ´»ç”¨
- **ã‚¤ãƒ³ãƒ•ãƒ©ã‚³ã‚¹ãƒˆ20-40%å‰Šæ¸›**: Kubernetesã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° + ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ´»ç”¨

2026å¹´ç¾åœ¨ã€**SGLangãŒæœ¬ç•ªç’°å¢ƒã§400,000ä»¥ä¸Šã®GPU**ã‚’ç¨¼åƒã•ã›ã¦ãŠã‚Šã€vLLMãŒã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æˆç†Ÿåº¦ã§å„ªä½ã«ç«‹ã¤ã¨ã„ã†çŠ¶æ³ã§ã™ã€‚ã“ã®è¨˜äº‹ã§ã¯ã€ã“ã‚Œã‚‰ã®æœ€æ–°å‹•å‘ã‚’è¸ã¾ãˆãŸå®Ÿè·µçš„ãªå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚

## LLMæ¨è«–ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°èª²é¡Œ

### å¾“æ¥ã®Webã‚¢ãƒ—ãƒªã¨ã®3ã¤ã®é•ã„

LLMæ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã¯ã€ä¸€èˆ¬çš„ãªWebã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯ç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¡ã¾ã™ã€‚

| ç‰¹æ€§ | å¾“æ¥ã®Webã‚¢ãƒ—ãƒª | LLMæ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ |
|------|---------------|-------------------|
| **ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“** | ã€œ100msï¼ˆå®šæ•°æ™‚é–“ï¼‰ | æ•°ç§’ã€œæ•°åç§’ï¼ˆå¯å¤‰ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ä¾å­˜ï¼‰ |
| **ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»** | CPU/ãƒ¡ãƒ¢ãƒªï¼ˆäºˆæ¸¬å¯èƒ½ï¼‰ | GPU/VRAMï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºãƒ»ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ä¾å­˜ï¼‰ |
| **ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ‡æ¨™** | CPUä½¿ç”¨ç‡ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° | **ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã€ãƒãƒƒãƒã‚µã‚¤ã‚º**ãŒé‡è¦ |

å¾“æ¥ã®ã€ŒCPUä½¿ç”¨ç‡ã§ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ã¨ã„ã†æ‰‹æ³•ã¯ã€LLMæ¨è«–ã«ã¯é©ã—ã¾ã›ã‚“ã€‚ç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

**GPUä½¿ç”¨ç‡ã®è½ã¨ã—ç©´:**
> GPUä½¿ç”¨ç‡ã¯ã€Œç¨¼åƒæ™‚é–“ã®å‰²åˆã‚’æ¸¬å®šã€ã—ã¾ã™ãŒã€ã€Œå®Ÿè¡Œä¸­ã«ã©ã®ç¨‹åº¦ã®ä½œæ¥­ãŒè¡Œã‚ã‚Œã¦ã„ã‚‹ã‹ã‚’æ¸¬å®šã—ãªã„ã€ãŸã‚ã€æ¨è«–ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆç›®æ¨™ã¨ç›´çµã§ãã¾ã›ã‚“ã€‚ç‰¹ã«vLLMã®ã‚ˆã†ãªãƒ¡ãƒ¢ãƒªäº‹å‰å‰²ã‚Šå½“ã¦ï¼ˆPagedAttentionï¼‰ã‚’æ¡ç”¨ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã¯ã€GPUä½¿ç”¨ç‡ãŒå¸¸ã«é«˜ã„çŠ¶æ…‹ã¨ãªã‚Šã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãŒæ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚

### æœ€é©ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼šã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º

**Google Cloud GKEã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**ã§ã¯ã€ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãŒæœ€ã‚‚æ¨å¥¨ã•ã‚Œã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦æŒ™ã’ã‚‰ã‚Œã¦ã„ã¾ã™ï¼š

```yaml
# Horizontal Pod Autoscalerè¨­å®šä¾‹ï¼ˆã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ï¼‰
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-inference
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: vllm_queue_size  # â† ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã‚’ç›£è¦–
      target:
        type: AverageValue
        averageValue: "5"  # â† åˆæœŸå€¤3-5ã‹ã‚‰é–‹å§‹ã€æ®µéšçš„ã«èª¿æ•´
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5åˆ†é–“å®‰å®šåŒ–
    scaleUp:
      stabilizationWindowSeconds: 0    # å³åº§ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
```

**ãªãœã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãŒå„ªã‚Œã¦ã„ã‚‹ã‹:**
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã¨ã®ç›´çµ**: ã‚­ãƒ¥ãƒ¼ã«å¾…æ©Ÿä¸­ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ç›´æ¥åæ˜ 
- **æ—©æœŸè­¦å‘Š**: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å¤‰å‹•ã‚’äº‹å‰ã«ã‚·ã‚°ãƒŠãƒ«åŒ–
- **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€å¤§åŒ–**: ãƒ¢ãƒ‡ãƒ«ã‚µãƒ¼ãƒãƒ¼ã®è¨­å®šï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰ã‚’å°Šé‡

**æ¨å¥¨è¨­å®šãƒ—ãƒ­ã‚»ã‚¹:**
1. åˆæœŸå€¤ã‚’3ã€œ5ã«è¨­å®š
2. è² è·ãƒ†ã‚¹ãƒˆã§å¥½é©ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆä¾‹: 1ç§’ä»¥å†…ï¼‰ã‚’ç¢ºèª
3. æ®µéšçš„ã«é–¾å€¤ã‚’å¢—åŠ ï¼ˆ5 â†’ 7 â†’ 10ï¼‰
4. ã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ãƒãƒ©ãƒ³ã‚¹ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹

## æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³é¸å®šï¼š2026å¹´ã®å‹¢åŠ›å›³

### ä¸»è¦5ã‚¨ãƒ³ã‚¸ãƒ³ã®ç‰¹å¾´

2026å¹´ç¾åœ¨ã€æœ¬ç•ªç’°å¢ƒã§ã¯ä»¥ä¸‹ã®5ã¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒä¸»æµã§ã™ã€‚

| ã‚¨ãƒ³ã‚¸ãƒ³ | å¼·ã¿ | å¼±ã¿ | æ¨å¥¨ç”¨é€” |
|---------|------|------|----------|
| **SGLang** | 400,000+ GPUç¨¼åƒå®Ÿç¸¾ã€RadixAttention | ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ãŒç™ºå±•é€”ä¸Š | **æœ¬ç•ªé‹ç”¨ï¼ˆæœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå„ªå…ˆï¼‰** |
| **vLLM** | 218ãƒ¢ãƒ‡ãƒ«å¯¾å¿œã€ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ æˆç†Ÿ | SGLangã‚ˆã‚Šæ€§èƒ½åŠ£ã‚‹ | **æœ¬ç•ªé‹ç”¨ï¼ˆæ‹¡å¼µæ€§å„ªå…ˆï¼‰** |
| **TensorRT-LLM** | NVIDIA GPUæœ€é©åŒ–ï¼ˆ1.5å€é«˜é€Ÿï¼‰ | ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—1-2é€±é–“ | **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºï¼ˆæ€§èƒ½å„ªå…ˆï¼‰** |
| **Triton** | Pythonãƒ©ã‚¤ã‚¯DSLã€ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ« | ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆé™å®šçš„ | **ã‚«ã‚¹ã‚¿ãƒ æœ€é©åŒ–ãŒå¿…è¦ãªå ´åˆ** |
| **Ollama** | ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç°¡æ˜“ | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°éå¯¾å¿œ | **é–‹ç™ºãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°** |

### vLLM vs TensorRT-LLM: æ€§èƒ½æ¯”è¼ƒ

å®Ÿæ¸¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆ2026å¹´ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰:

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | vLLM | TensorRT-LLM | å·®åˆ† |
|-----------|------|--------------|------|
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | 120-160 req/s | 180-220 req/s | **+50%** |
| **TTFT (Time to First Token)** | 50-80ms | 35-50ms | **-37%** |
| **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ™‚é–“** | 1æ—¥ | 1-2é€±é–“ | - |
| **ãƒ¢ãƒ‡ãƒ«å¯¾å¿œæ•°** | 218+ | é™å®šçš„ï¼ˆNVIDIAæœ€é©åŒ–æ¸ˆã¿ã®ã¿ï¼‰ | - |

**é¸å®šåŸºæº–:**
```python
# æ„æ€æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ï¼‰
if is_nvidia_hardware and can_invest_setup_time(weeks=2):
    use_tensorrt_llm  # æœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¿½æ±‚
elif need_broad_model_support or quick_deployment:
    use_vllm  # æŸ”è»Ÿæ€§ã¨ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 
elif need_max_throughput_now:
    use_sglang  # RadixAttentionæ´»ç”¨
else:
    use_ollama  # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™º
```

### SGLangã®RadixAttentionæŠ€è¡“

**RadixAttentionã®ä»•çµ„ã¿:**

SGLangã¯ã€Œå…±é€šãƒ—ãƒªãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€æ–‡è„ˆï¼‰ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ã™ã‚‹ã“ã¨ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

```python
# å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆvLLMï¼‰
# å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¯å›ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†
request_1 = "You are a helpful assistant.\n\nUser: è³ªå•1"
request_2 = "You are a helpful assistant.\n\nUser: è³ªå•2"
# â†’ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ãŒé‡è¤‡å‡¦ç†ã•ã‚Œã‚‹

# RadixAttentionã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆSGLangï¼‰
# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€åº¦ã ã‘å‡¦ç†ã€ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å†åˆ©ç”¨
common_prefix = "You are a helpful assistant.\n\n"  # â† ã‚­ãƒ£ãƒƒã‚·ãƒ¥
request_1 = common_prefix + "User: è³ªå•1"
request_2 = common_prefix + "User: è³ªå•2"  # â† ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
# â†’ å‡¦ç†æ™‚é–“ãŒ40-60%çŸ­ç¸®ï¼ˆå®Ÿæ¸¬å€¤ï¼‰
```

**æœ¬ç•ªç’°å¢ƒã§ã®æˆæœ:**
- **400,000ä»¥ä¸Šã®GPU**ã‚’æœ¬ç•ªç¨¼åƒï¼ˆ2026å¹´æ™‚ç‚¹ï¼‰
- ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã§**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·40-60%å‰Šæ¸›**
- 35ä»¥ä¸Šã®ãƒŸã‚­ã‚·ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§æŸ”è»Ÿãªæ‹¡å¼µ

## Kubernetesæ°´å¹³ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿè£…

### vLLMã®ãƒ‡ãƒ—ãƒ­ã‚¤ã¨HPAè¨­å®š

ä»¥ä¸‹ã¯ã€GKEä¸Šã§vLLMã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè£…ã™ã‚‹å®Œå…¨ãªä¾‹ã§ã™ã€‚

**1. vLLMã‚³ãƒ³ãƒ†ãƒŠã®ãƒ‡ãƒ—ãƒ­ã‚¤:**

```yaml
# vllm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-inference
  labels:
    app: vllm
spec:
  replicas: 2  # åˆæœŸãƒ¬ãƒ—ãƒªã‚«æ•°
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:v0.5.0
        args:
          - --model=meta-llama/Llama-3.1-8B-Instruct
          - --dtype=half
          - --max-model-len=4096
          - --gpu-memory-utilization=0.9
        resources:
          requests:
            nvidia.com/gpu: 1
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
          name: http
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  selector:
    app: vllm
  ports:
  - protocol: TCP
    port: 8000
    targetPort: 8000
  type: LoadBalancer
```

**2. Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†:**

```yaml
# prometheus-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-metrics
spec:
  selector:
    matchLabels:
      app: vllm
  endpoints:
  - port: http
    path: /metrics
    interval: 15s
```

**3. ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹HPA:**

```yaml
# vllm-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-inference
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Pods
    pods:
      metric:
        name: vllm_queue_size
      target:
        type: AverageValue
        averageValue: "5"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
```

**æ³¨æ„ç‚¹:**
> ã“ã®å®Ÿè£…ã¯ã€Œã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º = 5ã€ã‚’ãƒˆãƒªã‚¬ãƒ¼ã¨ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®æœ¬ç•ªç’°å¢ƒã§ã¯è² è·ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç›®æ¨™ï¼ˆä¾‹: 1ç§’ä»¥å†…ï¼‰ã‚’é”æˆã™ã‚‹é–¾å€¤ã‚’è¦‹ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãŸã€vLLMã®ãƒ¡ãƒ¢ãƒªäº‹å‰å‰²ã‚Šå½“ã¦ç‰¹æ€§ã«ã‚ˆã‚Šã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãŒé…å»¶ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€`stabilizationWindowSeconds: 300`ï¼ˆ5åˆ†é–“ï¼‰ã®èª¿æ•´ãŒé‡è¦ã§ã™ã€‚

### TensorRT-LLMã¨Tritonã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒãƒ¼ãƒ‰æ§‹æˆ

é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒå¿…è¦ãªã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç’°å¢ƒã§ã¯ã€TensorRT-LLMã¨Triton Inference Serverã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒ«ãƒãƒãƒ¼ãƒ‰æ§‹æˆãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚

```yaml
# tensorrt-triton-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-tensorrt-llm
spec:
  replicas: 4  # ãƒãƒ«ãƒãƒãƒ¼ãƒ‰æ§‹æˆ
  template:
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:26.01-trtllm-python-py3
        args:
          - tritonserver
          - --model-repository=/models
          - --backend-config=tensorrt,max-queue-delay-microseconds=100
        resources:
          requests:
            nvidia.com/gpu: 2  # ãƒãƒ¼ãƒ‰ã‚ãŸã‚Š2 GPU
          limits:
            nvidia.com/gpu: 2
        volumeMounts:
        - name: model-repo
          mountPath: /models
      volumes:
      - name: model-repo
        persistentVolumeClaim:
          claimName: tensorrt-models-pvc
```

**ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †:**
1. TensorRT-LLMç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆ1-2é€±é–“ï¼‰
2. Triton Model Repositoryã«é…ç½®
3. Kubernetes Deploymentã¨HPAã‚’è¨­å®š
4. è² è·ãƒ†ã‚¹ãƒˆã§æ€§èƒ½æ¤œè¨¼ï¼ˆ180-220 req/sã‚’ç¢ºèªï¼‰

## Prefill/Decodeåˆ†é›¢ã«ã‚ˆã‚‹æœ€é©åŒ–

### åˆ†é›¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ä»•çµ„ã¿

æœ€æ–°ã®ç ”ç©¶ï¼ˆ2025å¹´ï¼‰ã§ã¯ã€**Prefillï¼ˆåˆæœŸå‡¦ç†ï¼‰ã¨Decodeï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼‰ã‚’åˆ¥ã€…ã®ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œ**ã™ã‚‹ã“ã¨ã§ã€ç•°ç¨®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®åŠ¹ç‡çš„æ´»ç”¨ãŒå¯èƒ½ã«ãªã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚                     â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Prefillãƒãƒ¼ãƒ‰ â”‚      â”‚ Decodeãƒãƒ¼ãƒ‰ â”‚
â”‚ (GPU: A100)  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚ (GPU: T4)   â”‚
â”‚ é«˜è¨ˆç®—èƒ½åŠ›    â”‚      â”‚ é«˜ãƒ¡ãƒ¢ãƒªå¸¯åŸŸ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å½¹å‰²åˆ†æ‹…:**

| ãƒ•ã‚§ãƒ¼ã‚º | ç‰¹æ€§ | æœ€é©ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ | ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ |
|---------|------|----------------|-------------|
| **Prefill** | è¨ˆç®—é›†ç´„çš„ã€ä¸¦åˆ—å‡¦ç†å¯èƒ½ | A100ã€H100ï¼ˆé«˜FLOPSï¼‰ | é«˜è¨ˆç®—èƒ½åŠ› |
| **Decode** | ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¾‹é€Ÿã€é€æ¬¡å‡¦ç† | T4ã€L4ï¼ˆé«˜ãƒ¡ãƒ¢ãƒªå¸¯åŸŸï¼‰ | é«˜ãƒ¡ãƒ¢ãƒªå¸¯åŸŸ |

**å®Ÿè£…ä¾‹ï¼ˆKubernetesãƒãƒ¼ãƒ‰ã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£ï¼‰:**

```yaml
# prefill-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-prefill
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - a100  # â† Prefillã¯é«˜è¨ˆç®—èƒ½åŠ›GPU
      containers:
      - name: vllm-prefill
        args:
          - --disable-log-requests
          - --enable-prefix-caching
        resources:
          limits:
            nvidia.com/gpu: 1
---
# decode-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-decode
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: gpu-type
                operator: In
                values:
                - t4  # â† Decodeã¯é«˜ãƒ¡ãƒ¢ãƒªå¸¯åŸŸGPU
      containers:
      - name: vllm-decode
        args:
          - --disable-log-requests
        resources:
          limits:
            nvidia.com/gpu: 1
```

**æˆæœ:**
- **GPUç¨¼åƒç‡40%å‘ä¸Š**: å„GPUã‚¿ã‚¤ãƒ—ã®å¼·ã¿ã‚’æ´»ã‹ã—ãŸé…ç½®
- **ã‚¤ãƒ³ãƒ•ãƒ©ã‚³ã‚¹ãƒˆ20-30%å‰Šæ¸›**: T4ç­‰ã®ä½ä¾¡æ ¼GPUã‚’Decodeå‡¦ç†ã«æ´»ç”¨
- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ”¹å–„**: Prefillã®ä¸¦åˆ—å‡¦ç†ã§TTFTçŸ­ç¸®

## å®Ÿè£…æ™‚ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

æœ¬ç•ªç’°å¢ƒã§ç›´é¢ã™ã‚‹å…¸å‹çš„ãªå•é¡Œã¨è§£æ±ºç­–ã‚’ä»¥ä¸‹ã«ã¾ã¨ã‚ã¾ã™ã€‚

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| **ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãŒæ©Ÿèƒ½ã—ãªã„** | vLLMã®ãƒ¡ãƒ¢ãƒªäº‹å‰å‰²ã‚Šå½“ã¦ã§GPUä½¿ç”¨ç‡ãŒå¸¸ã«é«˜ã„ | HPAãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’GPUä½¿ç”¨ç‡ã‹ã‚‰ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã«å¤‰æ›´ |
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒä¸å®‰å®š** | ãƒãƒƒãƒã‚µã‚¤ã‚ºã®è‡ªå‹•èª¿æ•´ãŒè£ç›®ã«å‡ºã¦ã„ã‚‹ | `--max-num-seqs`ã§å›ºå®šãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š |
| **OOMï¼ˆOut of Memoryï¼‰ã‚¨ãƒ©ãƒ¼** | ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·è¶…é | `--max-model-len`ã§ä¸Šé™è¨­å®šã€ã¾ãŸã¯é•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´ |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒæœŸå¾…å€¤ã«é”ã—ãªã„** | TensorRT-LLMã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–ä¸è¶³ | GPUä¸–ä»£åˆ¥ã«å†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆAmpereã€Hopperç­‰ï¼‰ |
| **ã‚³ã‚¹ãƒˆè¶…é** | å¸¸æ™‚æœ€å¤§ãƒ¬ãƒ—ãƒªã‚«ã§ç¨¼åƒ | HPAã®`maxReplicas`ã‚’èª¿æ•´ã€ã‚¹ãƒãƒƒãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ´»ç”¨ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- LLMæ¨è«–ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯**ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãƒ¡ãƒˆãƒªã‚¯ã‚¹**ãŒæœ€é©æŒ‡æ¨™ï¼ˆGPUä½¿ç”¨ç‡ã¯ä¸é©ï¼‰
- æœ¬ç•ªé‹ç”¨ã§ã¯**SGLangï¼ˆæœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼‰vs vLLMï¼ˆæ‹¡å¼µæ€§ï¼‰**ã®äºŒæŠ
- TensorRT-LLMã¯1.5å€é«˜é€Ÿã ãŒã€**ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«1-2é€±é–“å¿…è¦**ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘ï¼‰
- Prefill/Decodeåˆ†é›¢ã§**GPUç¨¼åƒç‡40%å‘ä¸Šã€ã‚³ã‚¹ãƒˆ20-30%å‰Šæ¸›**
- Kubernetes HPAã®å®‰å®šåŒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦èª¿æ•´ãŒæˆåŠŸã®éµ

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
1. **è² è·ãƒ†ã‚¹ãƒˆå®Ÿæ–½**: è‡ªç¤¾ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºé–¾å€¤ï¼ˆ3-5ã‹ã‚‰é–‹å§‹ï¼‰ã‚’æœ€é©åŒ–
2. **æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³é¸å®š**: vLLM/SGLang/TensorRT-LLMã®å®Ÿæ©Ÿãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆæœ¬è¨˜äº‹ã®æ€§èƒ½ç›®å®‰ã‚’å‚è€ƒï¼‰
3. **Kubernetesç’°å¢ƒæ§‹ç¯‰**: GKE/EKS/AKSã§HPA + Prometheusã‚’è¨­å®š
4. **Prefill/Decodeåˆ†é›¢æ¤œè¨¼**: ç•°ç¨®GPUç’°å¢ƒã§ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’æ¸¬å®š
5. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–**: Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€ã‚³ã‚¹ãƒˆã‚’å¯è¦–åŒ–

## å‚è€ƒ

- [Best practices for autoscaling LLM inference workloads with GPUs on GKE | Google Cloud](https://docs.cloud.google.com/kubernetes-engine/docs/best-practices/machine-learning/inference/autoscaling)
- [The State of LLM Serving in 2026: Ollama, SGLang, TensorRT, Triton, and vLLM | Canteen](https://thecanteenapp.com/analysis/2026/01/03/inference-serving-landscape.html)
- [vLLM vs TensorRT-LLM: Key differences, performance, and how to run them | Northflank](https://northflank.com/blog/vllm-vs-tensorrt-llm-and-how-to-run-them)
- [Scaling LLMs with NVIDIA Triton and TensorRT-LLM Using Kubernetes | NVIDIA Technical Blog](https://developer.nvidia.com/blog/scaling-llms-with-nvidia-triton-and-nvidia-tensorrt-llm-using-kubernetes/)
- [GPU Autoscaling for Large Language Models | Medium](https://medium.com/@vishal_95877/gpu-autoscaling-for-large-language-models-d51060be1e8e)
- [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference | arXiv](https://arxiv.org/html/2508.19559v1)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #40](https://github.com/0h-n0/zen-auto-create-article/issues/40) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
