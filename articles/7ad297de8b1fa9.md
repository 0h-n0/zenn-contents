---
title: "LangGraph×Claude Sonnet 4.6のKVキャッシュ最適化でRAGコスト90%削減"
emoji: "⚡"
type: "tech"
topics: ["langgraph", "claude", "rag", "llm", "python"]
published: false
---

# LangGraph×Claude Sonnet 4.6のKVキャッシュ最適化でRAGコスト90%削減

## この記事でわかること

- LLM推論におけるKVキャッシュの仕組みと、長文脈処理でメモリの70%を消費する問題の背景
- Anthropic Prompt Cachingを活用してAgentic RAGの入力トークンコストを90%削減する実装方法
- LangGraphの`AnthropicPromptCachingMiddleware`による自動キャッシュ管理の設定手順
- RocketKV・ChunkKVなどモデルレベルのKVキャッシュ圧縮技術の動向と、vLLMでのFP8量子化の適用方法
- APIキャッシュとモデルレベル圧縮を組み合わせた2層最適化アーキテクチャの設計

## 対象読者

- **想定読者**: 中級〜上級のPythonエンジニアで、LLMアプリケーションの運用コスト削減に取り組んでいる方
- **必要な前提知識**:
  - Python 3.11以上の基礎文法
  - LangGraph v0.4+（StateGraph、ToolNode）の基本的な使い方
  - RAG（Retrieval-Augmented Generation）の基本概念
  - Anthropic Claude APIの基本的な利用経験

## 結論・成果

Anthropic Prompt Cachingの導入により、Agentic RAGの**キャッシュヒット時の入力トークンコストが90%削減**（$3.00/MTok → $0.30/MTok）されることが公式ドキュメントで示されています。さらに、セルフホスティング環境ではvLLMのFP8 KVキャッシュ量子化により**メモリ使用量を約50%削減**し、RocketKVの研究では**最大400倍のKVキャッシュ圧縮**と**3.7倍の推論高速化**が報告されています（ICML 2025）。本記事では、これらの最適化手法をLangGraph Agentic RAGに統合する実装パターンを解説します。

## KVキャッシュがLLM推論のボトルネックになる仕組みを理解する

Transformerベースの大規模言語モデルでは、Self-Attention機構において過去のトークンのKey-Value（KV）ペアをメモリに保持します。これが**KVキャッシュ**です。推論時に毎回全トークンのKVペアを再計算するのを避けるため、一度計算したKV表現をGPUメモリ上にキャッシュして再利用する仕組みになっています。

### KVキャッシュのメモリ消費が問題になる理由

KVキャッシュのメモリ消費量は、以下の要素に比例して増大します。

```python
# KVキャッシュのメモリ消費量の概算
def estimate_kv_cache_memory(
    batch_size: int,
    seq_length: int,
    num_layers: int,
    num_heads: int,
    head_dim: int,
    dtype_bytes: int = 2,  # FP16 = 2bytes, FP8 = 1byte
) -> float:
    """KVキャッシュのメモリ使用量をGB単位で推定する"""
    # Key + Value の2つ分
    kv_cache_bytes = (
        2 * batch_size * seq_length * num_layers * num_heads * head_dim * dtype_bytes
    )
    return kv_cache_bytes / (1024**3)  # GB変換


# Claude Sonnet 4.6相当のモデルパラメータでの推定例
memory_gb = estimate_kv_cache_memory(
    batch_size=1,
    seq_length=200_000,  # 200Kトークン
    num_layers=80,
    num_heads=64,
    head_dim=128,
    dtype_bytes=2,
)
print(f"KVキャッシュ推定メモリ: {memory_gb:.1f} GB")
# KVキャッシュ推定メモリ: 約48.8 GB（FP16の場合）
```

128Kトークンを超える長文脈処理では、**KVキャッシュがGPUメモリの最大70%を消費する**と報告されています。Agentic RAGでは、システムプロンプト・ツール定義・検索結果・会話履歴が繰り返し入力されるため、この問題が顕著になります。

**ここで注意すべき点があります。** KVキャッシュの「最適化」には2つの異なるレイヤーが存在します。

| レイヤー | 手法 | 対象 | 削減効果 |
|---------|------|------|---------|
| **APIレベル** | Anthropic Prompt Caching | APIコスト・レイテンシ | コスト90%削減 |
| **モデルレベル** | RocketKV、ChunkKV、FP8量子化 | GPUメモリ・推論速度 | メモリ50%〜最大400倍圧縮 |

本記事ではまずAPIレベルの最適化（Prompt Caching）をLangGraph Agentic RAGに実装し、次にモデルレベルの圧縮技術を概説します。

## Anthropic Prompt CachingをLangGraph Agentic RAGに実装する

Anthropic Prompt Cachingは、APIリクエスト間で共通するプロンプトのKVキャッシュ表現をAnthropicのサーバー側に保持し、再利用する仕組みです。Agentic RAGでは、システムプロンプトやツール定義が各ターンで繰り返されるため、キャッシュの効果が大きくなります。

### 基本構成: StateGraphとPrompt Cachingの統合

まず、LangGraphのAgentic RAGにAnthropicPromptCachingMiddlewareを統合する基本実装を見ていきましょう。

```python
# agentic_rag_cached.py
from langchain_anthropic import ChatAnthropic
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.messages import HumanMessage
from langgraph.graph import MessagesState, StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain.tools import tool
from pydantic import BaseModel, Field
from typing import Literal


# 1. ベクトルストア・リトリーバの準備
def create_retriever(documents: list[str]):
    """ドキュメントからリトリーバを作成する"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100,
    )
    splits = text_splitter.create_documents(documents)
    vectorstore = InMemoryVectorStore.from_documents(
        documents=splits,
        embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
    )
    return vectorstore.as_retriever(search_kwargs={"k": 5})


retriever = create_retriever(["ドキュメント群をここに渡す"])


# 2. リトリーバツール定義
@tool
def search_knowledge_base(query: str) -> str:
    """社内ナレッジベースを検索し、関連ドキュメントを返す"""
    docs = retriever.invoke(query)
    return "\n\n---\n\n".join(
        [f"[Source: {d.metadata.get('source', 'unknown')}]\n{d.page_content}" for d in docs]
    )


# 3. Claude Sonnet 4.6 + Prompt Cachingの設定
llm = ChatAnthropic(
    model="claude-sonnet-4-6-20250929",
    temperature=0,
    max_tokens=4096,
)

SYSTEM_PROMPT = """あなたは社内ナレッジベースを検索して質問に回答するRAGアシスタントです。

## 回答ルール
- 必ずsearch_knowledge_baseツールで検索してから回答してください
- 検索結果に基づいて回答し、情報がない場合は「該当する情報が見つかりませんでした」と回答してください
- 出典を明示してください
- 回答は日本語で、簡潔かつ正確に行ってください
"""
```

**なぜChatAnthropicを直接使うのか:** `create_agent`のAPI（LangGraph v1.0）では、現時点でAnthropicの`cache_control`付き構造化システムメッセージを直接サポートしていないという制約があります（[GitHub Issue #33635](https://github.com/langchain-ai/langchain/issues/33635)で議論中）。そのため、StateGraphを直接構築するアプローチを採用しています。

### グラフの構築とキャッシュ戦略

次に、検索→評価→回答生成の循環グラフを構築します。

```python
# 4. ドキュメント評価（Grading）
class GradeDocuments(BaseModel):
    """検索結果の関連性を評価するスキーマ"""
    binary_score: Literal["yes", "no"] = Field(
        description="検索結果が質問に関連する場合は'yes'、そうでない場合は'no'"
    )


grader_llm = ChatAnthropic(
    model="claude-sonnet-4-6-20250929",
    temperature=0,
    max_tokens=256,
)


# 5. ノード定義
def generate_query_or_respond(state: MessagesState):
    """質問に対して検索するかそのまま回答するかを判断する"""
    response = llm.bind_tools([search_knowledge_base]).invoke(
        [{"role": "system", "content": SYSTEM_PROMPT}] + state["messages"]
    )
    return {"messages": [response]}


def grade_documents(
    state: MessagesState,
) -> Literal["generate_answer", "rewrite_question"]:
    """検索結果の関連性を評価し、次のステップを決定する"""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    grade_prompt = (
        f"以下の検索結果がユーザーの質問に関連しているか評価してください。\n\n"
        f"質問: {question}\n\n検索結果: {context}"
    )
    result = grader_llm.with_structured_output(GradeDocuments).invoke(
        [{"role": "user", "content": grade_prompt}]
    )
    return "generate_answer" if result.binary_score == "yes" else "rewrite_question"


def rewrite_question(state: MessagesState):
    """検索結果が不十分な場合、質問を書き直す"""
    question = state["messages"][0].content
    rewrite_prompt = (
        f"以下の質問をより具体的に書き直してください。\n"
        f"元の質問: {question}\n改善された質問:"
    )
    response = llm.invoke([{"role": "user", "content": rewrite_prompt}])
    return {"messages": [HumanMessage(content=response.content)]}


def generate_answer(state: MessagesState):
    """検索結果に基づいて回答を生成する"""
    question = state["messages"][0].content
    context = state["messages"][-1].content

    answer_prompt = (
        f"以下の検索結果に基づいて質問に回答してください。\n\n"
        f"質問: {question}\n\n検索結果:\n{context}\n\n"
        f"検索結果に基づいた正確な回答を日本語で提供してください。"
    )
    response = llm.invoke(
        [{"role": "system", "content": SYSTEM_PROMPT},
         {"role": "user", "content": answer_prompt}]
    )
    return {"messages": [response]}


# 6. StateGraphの組み立て
workflow = StateGraph(MessagesState)
workflow.add_node(generate_query_or_respond)
workflow.add_node("retrieve", ToolNode([search_knowledge_base]))
workflow.add_node(rewrite_question)
workflow.add_node(generate_answer)

workflow.add_edge(START, "generate_query_or_respond")
workflow.add_conditional_edges(
    "generate_query_or_respond",
    tools_condition,
    {"tools": "retrieve", END: END},
)
workflow.add_conditional_edges("retrieve", grade_documents)
workflow.add_edge("generate_answer", END)
workflow.add_edge("rewrite_question", "generate_query_or_respond")

# MemorySaverで会話履歴を永続化
graph = workflow.compile(checkpointer=MemorySaver())
```

### Prompt Cachingの明示的な適用

APIレベルのPrompt Cachingを最大限活用するために、`cache_control`を明示的に設定するパターンを見ていきましょう。

```python
# 7. 明示的キャッシュ制御を使った呼び出し
import anthropic

client = anthropic.Anthropic()


def invoke_with_explicit_cache(
    question: str,
    retrieved_context: str,
    conversation_history: list[dict] | None = None,
) -> str:
    """明示的なキャッシュブレークポイントを設定してAPI呼び出しを行う

    最大4つのキャッシュブレークポイントを戦略的に配置:
    1. ツール定義の末尾
    2. システムプロンプトの末尾
    3. 検索コンテキストの末尾
    4. 会話履歴の末尾
    """
    tools = [
        {
            "name": "search_knowledge_base",
            "description": "社内ナレッジベースを検索する",
            "input_schema": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "検索クエリ"}
                },
                "required": ["query"],
            },
            # ← キャッシュブレークポイント1: ツール定義
            "cache_control": {"type": "ephemeral"},
        }
    ]

    system = [
        {
            "type": "text",
            "text": SYSTEM_PROMPT,
        },
        {
            "type": "text",
            "text": f"## 検索済みコンテキスト\n\n{retrieved_context}",
            # ← キャッシュブレークポイント2: システムプロンプト+コンテキスト
            "cache_control": {"type": "ephemeral"},
        },
    ]

    messages = conversation_history or []
    messages.append({"role": "user", "content": question})

    response = client.messages.create(
        model="claude-sonnet-4-6-20250929",
        max_tokens=4096,
        tools=tools,
        system=system,
        messages=messages,
    )

    # キャッシュ統計の出力
    usage = response.usage
    print(f"入力トークン: {usage.input_tokens}")
    print(f"キャッシュ書込: {getattr(usage, 'cache_creation_input_tokens', 0)}")
    print(f"キャッシュ読取: {getattr(usage, 'cache_read_input_tokens', 0)}")

    return response.content[0].text
```

**注意点:**

> Prompt Cachingの最小キャッシュ可能トークン数は、Claude Sonnet 4.6の場合**1024トークン**です。これより短いプロンプトは`cache_control`を指定してもキャッシュされません。短いシステムプロンプトの場合は、検索コンテキストと結合してキャッシュすることを検討してください。

### 自動キャッシュ: AnthropicPromptCachingMiddleware

LangGraphの`AnthropicPromptCachingMiddleware`を使うと、キャッシュブレークポイントの管理が自動化されます。

```python
# 8. Middlewareによる自動キャッシュ管理
from langchain_anthropic.middleware import AnthropicPromptCachingMiddleware
from langchain.agents import create_agent

# マルチターン会話でのキャッシュ自動管理
cached_agent = create_agent(
    model=ChatAnthropic(model="claude-sonnet-4-6-20250929"),
    tools=[search_knowledge_base],
    system_prompt=SYSTEM_PROMPT,
    middleware=[
        AnthropicPromptCachingMiddleware(
            ttl="5m",  # 5分間のキャッシュ有効期間
        )
    ],
    checkpointer=MemorySaver(),
)

# 使用例
config = {"configurable": {"thread_id": "user-session-001"}}

# 1回目: キャッシュ書き込み
result1 = cached_agent.invoke(
    {"messages": [HumanMessage("RAGの検索精度を改善する方法は？")]},
    config=config,
)

# 2回目以降: キャッシュヒット（コスト90%削減）
result2 = cached_agent.invoke(
    {"messages": [HumanMessage("具体的なリランキングの実装方法を教えて")]},
    config=config,
)
```

**Middlewareの動作原理:** リクエスト内の最後のキャッシュ可能ブロックに自動的にキャッシュブレークポイントを配置します。マルチターン会話では、前のターンまでの内容がキャッシュから読み込まれ、新しいメッセージのみが書き込まれます。

### コスト削減効果の具体的な試算

以下は、Agentic RAGの典型的な利用パターンでのコスト試算です。

| 項目 | キャッシュなし | キャッシュあり | 削減率 |
|------|-------------|-------------|--------|
| システムプロンプト（2Kトークン） | $0.006/回 | 初回$0.0075、以降$0.0006/回 | 90% |
| ツール定義（1Kトークン） | $0.003/回 | 初回$0.00375、以降$0.0003/回 | 90% |
| 検索コンテキスト（10Kトークン） | $0.03/回 | 初回$0.0375、以降$0.003/回 | 90% |
| **10ターン会話の合計** | **$0.39** | **$0.0858** | **78%** |

（Anthropic公式料金に基づく試算: Claude Sonnet 4.6 入力$3/MTok、キャッシュヒット$0.30/MTok、キャッシュ書込$3.75/MTok）

> このコスト試算は、全ターンでキャッシュヒットが発生する理想的なケースです。実際の運用では、5分間のTTL内にリクエストがない場合はキャッシュが失効し、再書き込みコストが発生します。高頻度なユーザーセッションほど削減効果が大きくなります。

## モデルレベルのKVキャッシュ圧縮技術を把握する

APIレベルのPrompt Cachingに加え、モデルレベルでのKVキャッシュ圧縮技術も急速に進化しています。セルフホスティング環境でLLMを運用する場合、これらの技術が重要になります。

### RocketKV: 2段階圧縮による400倍のKV削減

RocketKV（ICML 2025、NVlabs）は、**トレーニング不要**の2段階KVキャッシュ圧縮手法です。

```
Stage 1（粗粒度）: 入力シーケンスのKVキャッシュを永続的に削除
         ↓
Stage 2（細粒度）: クエリに応じたTop-K選択でハイブリッド疎アテンション
```

著者らの実験では、NVIDIA A100 GPUにおいて以下の結果が報告されています。

| メトリクス | 数値 |
|-----------|------|
| KVキャッシュ圧縮率 | 最大400倍 |
| 推論速度向上 | 最大3.7倍（エンドツーエンド） |
| ピークメモリ削減 | 最大32.6%（デコードフェーズ） |
| 精度劣化 | 長文脈タスクでほぼなし |

評価対象モデルはLlama3.1-8B-Instruct、Mistral-7B-Instruct-v0.2、LongChat-7B-v1.5-32kです。

### ChunkKV: セマンティックチャンク単位の圧縮

ChunkKV（NeurIPS 2025）は、従来のトークン単位ではなく**セマンティックチャンク**を圧縮単位とすることで、言語構造を保持したまま圧縮を実現します。

従来手法との違いは、個々のトークンを独立に評価して削除するのではなく、意味的にまとまった単位（文節・フレーズ）で圧縮するため、文脈の整合性が維持される点です。著者らの報告によると、レイヤー間のインデックス再利用技術により**スループットが26.5%改善**されています。

**制約条件:** RocketKV・ChunkKVともにオープンソースモデル向けの手法であり、Claude等のAPI提供モデルには直接適用できません。セルフホスティング環境（vLLM、TGI等）でオープンソースモデルを運用する場合に有効です。

### vLLMでのFP8 KVキャッシュ量子化を設定する

セルフホスティング環境で即座に導入可能な最適化として、vLLMのFP8 KVキャッシュ量子化があります。

```bash
# vLLMサーバーの起動（FP8 KVキャッシュ量子化 + Prefix Caching有効）
vllm serve meta-llama/Llama-3.1-8B-Instruct \
    --kv-cache-dtype fp8 \
    --calculate-kv-scales \
    --enable-prefix-caching \
    --max-model-len 131072 \
    --gpu-memory-utilization 0.9
```

```python
# vLLM Python APIでの設定
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3.1-8B-Instruct",
    kv_cache_dtype="fp8",          # FP16→FP8で約50%メモリ削減
    calculate_kv_scales=True,       # 自動スケール計算
    enable_prefix_caching=True,     # Prefix Cachingで共通プロンプトを再利用
    max_model_len=131072,
    gpu_memory_utilization=0.9,
)

sampling_params = SamplingParams(temperature=0, max_tokens=2048)

# 共通プロンプトはPrefix Cachingにより自動キャッシュ
outputs = llm.generate(
    ["あなたはRAGアシスタントです。\n\n質問: RAGの精度を改善するには？"],
    sampling_params,
)
```

**ハードウェア要件:** FP8 KVキャッシュはHopper世代以降のGPU（H100、RTX 4090/6000 Ada、AMD MI300）でサポートされています。Ampere世代（A100、RTX 3090）ではFP8 KVキャッシュは利用できません。

## 2層最適化アーキテクチャを設計する

ここまでの内容を統合し、APIレベルとモデルレベルのKVキャッシュ最適化を組み合わせたアーキテクチャを設計してみましょう。

### アーキテクチャ概要

```
ユーザークエリ
    ↓
[LangGraph Agentic RAG]
    ├── Layer 1: API-Level Cache（Anthropic Prompt Caching）
    │   ├── システムプロンプト → cache_control: ephemeral
    │   ├── ツール定義 → cache_control: ephemeral
    │   └── 検索コンテキスト → cache_control: ephemeral (ttl: 1h)
    │
    └── Layer 2: Model-Level Cache（セルフホスティング時）
        ├── vLLM FP8 KV Cache Quantization
        ├── vLLM Prefix Caching
        └── RocketKV/ChunkKV（実験的）
```

### 実装パターン: ハイブリッドキャッシュ戦略

```python
# hybrid_cache_strategy.py
from dataclasses import dataclass
from enum import Enum


class CacheLayer(Enum):
    API = "api"         # Anthropic Prompt Caching
    MODEL = "model"     # vLLM Prefix Caching / FP8 KV Cache
    BOTH = "both"       # 両方（セルフホスティング + API Proxy構成）


@dataclass
class CacheConfig:
    """KVキャッシュ最適化の設定"""
    layer: CacheLayer
    api_ttl: str = "5m"               # "5m" or "1h"
    api_min_tokens: int = 1024        # 最小キャッシュ可能トークン数
    model_kv_dtype: str = "fp8"       # "fp8" or "fp16"
    model_prefix_caching: bool = True
    max_cache_breakpoints: int = 4    # Anthropic APIの上限


def build_optimized_rag(config: CacheConfig):
    """最適化設定に基づいてRAGパイプラインを構築する"""
    if config.layer in (CacheLayer.API, CacheLayer.BOTH):
        # APIレベルキャッシュ: Anthropic Prompt Caching
        llm = ChatAnthropic(
            model="claude-sonnet-4-6-20250929",
            temperature=0,
            max_tokens=4096,
        )

        # 1時間TTLが必要な場合（セッション間隔が5分超）
        middleware_config = {"ttl": config.api_ttl}
        if config.api_ttl == "1h":
            # 1時間キャッシュ: コスト2倍だがTTL長い
            # $6/MTok書込、$0.30/MTok読取（Sonnet 4.6）
            middleware_config["ttl"] = "1h"

        print(f"API Cache: TTL={config.api_ttl}, "
              f"MinTokens={config.api_min_tokens}")

    if config.layer in (CacheLayer.MODEL, CacheLayer.BOTH):
        # モデルレベルキャッシュ: vLLM設定
        print(f"Model Cache: KV dtype={config.model_kv_dtype}, "
              f"Prefix={config.model_prefix_caching}")

    return llm


# 使用例: API-onlyパターン（最も一般的）
api_config = CacheConfig(
    layer=CacheLayer.API,
    api_ttl="5m",
)
rag_llm = build_optimized_rag(api_config)
```

### よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| キャッシュヒットが発生しない | プロンプトが1024トークン未満 | システムプロンプトと検索コンテキストを結合して1024トークン以上にする |
| TTL切れで毎回キャッシュミス | ユーザーセッション間隔が5分超 | `ttl: "1h"`に変更（書込コスト2倍、ただし読取は同じ$0.30/MTok） |
| `create_agent`でcache_controlが使えない | LangGraph v1.0の制約 | StateGraphを直接構築するか、`AnthropicPromptCachingMiddleware`を使用 |
| FP8 KVキャッシュでエラー | Ampere世代GPU（A100等）を使用 | Hopper世代以降のGPU（H100、RTX 4090）にアップグレード、またはFP16のまま運用 |
| 並列リクエストでキャッシュミス | キャッシュエントリは最初のレスポンス開始後に利用可能になる | 最初のリクエストのレスポンスを待ってから並列リクエストを送信 |

## まとめと次のステップ

**まとめ:**

- LLM推論におけるKVキャッシュはGPUメモリの最大70%を消費し、Agentic RAGでは繰り返し入力によりコストが増大します
- Anthropic Prompt Cachingにより、**キャッシュヒット時の入力トークンコストが90%削減**されます（$3.00→$0.30/MTok）
- LangGraphの`AnthropicPromptCachingMiddleware`を使えば、マルチターン会話でのキャッシュ管理を自動化できます
- セルフホスティング環境では、vLLMのFP8 KVキャッシュ量子化が即座に導入可能で、メモリ使用量を約50%削減できます
- RocketKV（ICML 2025）やChunkKV（NeurIPS 2025）など、モデルレベルの圧縮技術は研究段階ですが、最大400倍の圧縮が報告されています

**次にやるべきこと:**

- 既存のAgentic RAGパイプラインに`cache_control`を追加し、`cache_read_input_tokens`をモニタリングして実際のキャッシュヒット率を計測する
- 利用パターンに応じたTTL（5分 vs 1時間）を選択し、コスト試算を行う
- セルフホスティング環境がある場合は、vLLMの`--kv-cache-dtype fp8`オプションを検証する

## 参考

- [Anthropic Prompt Caching 公式ドキュメント](https://platform.claude.com/docs/en/build-with-claude/prompt-caching)
- [LangGraph Agentic RAG チュートリアル](https://docs.langchain.com/oss/python/langgraph/agentic-rag)
- [LangChain Anthropic Middleware](https://docs.langchain.com/oss/python/integrations/middleware/anthropic)
- [RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression（ICML 2025）](https://arxiv.org/abs/2502.14051)
- [ChunkKV: Semantic-Preserving KV Cache Compression（NeurIPS 2025）](https://arxiv.org/abs/2502.00299)
- [vLLM Quantized KV Cache ドキュメント](https://docs.vllm.ai/en/latest/features/quantization/quantized_kvcache/)
- [Awesome KV Cache Compression（論文リスト）](https://github.com/October2001/Awesome-KV-Cache-Compression)

関連記事: [LangGraph×Claude Sonnet 4.6で実装する階層的Agentic RAG検索パイプライン](https://zenn.dev/0h_n0/articles/a4cd3a7f1cf4ce)
関連記事: [LangGraphエージェント型RAGのレイテンシ最適化：ストリーミング×非同期実行で応答速度を3倍改善する](https://zenn.dev/0h_n0/articles/433702e83b26ed)

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
