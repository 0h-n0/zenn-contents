---
title: "LLMæœ¬ç•ªé‹ç”¨ã®å“è³ªè©•ä¾¡è‡ªå‹•åŒ–ï¼šRagasãƒ»DeepEvalã§è©•ä¾¡æ™‚é–“ã‚’80%çŸ­ç¸®ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ¯"
type: "tech"
topics: ["llm", "ragas", "deepeval", "testing", "ai"]
published: false
---

# LLMæœ¬ç•ªé‹ç”¨ã®å“è³ªè©•ä¾¡è‡ªå‹•åŒ–ï¼šRagasãƒ»DeepEvalã§è©•ä¾¡æ™‚é–“ã‚’80%çŸ­ç¸®ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LLMå“è³ªè©•ä¾¡è‡ªå‹•åŒ–ã®åŸºæœ¬æ¦‚å¿µã¨2å±¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆè‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹+äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰
- Ragasãƒ»DeepEvalã®ç‰¹å¾´æ¯”è¼ƒã¨é¸å®šåŸºæº–ï¼ˆRAGç‰¹åŒ– vs pytestäº’æ›TDDï¼‰
- **RAGè©•ä¾¡ã®5ã¤ã®ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
  1. Context Precisionï¼ˆæ¤œç´¢çµæœã®é †åºä»˜ã‘å“è³ªï¼‰
  2. Context Recallï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç¶²ç¾…æ€§ï¼‰
  3. Contextual Relevancyï¼ˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ»top-Kæœ€é©æ€§ï¼‰
  4. Answer Relevancyï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ‰ç”¨æ€§ï¼‰
  5. Faithfulnessï¼ˆå¹»è¦šæ¤œå‡ºãƒ»äº‹å®Ÿæ€§æ¤œè¨¼ï¼‰
- CI/CDçµ±åˆã§è©•ä¾¡æ™‚é–“ã‚’æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼6æ™‚é–“â†’è‡ªå‹•è©•ä¾¡45åˆ†ï¼ˆ80%çŸ­ç¸®ï¼‰ã«å‰Šæ¸›ã™ã‚‹å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
- æœ¬ç•ªç’°å¢ƒã§95%ä»¥ä¸Šã®å“è³ªç¶­æŒã‚’å®Ÿç¾ã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ¬ç•ªé‹ç”¨ä¸­ã€ã¾ãŸã¯æœ¬ç•ªå±•é–‹ã‚’æº–å‚™ä¸­ã®ä¸­ç´šã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - RAGï¼ˆRetrieval-Augmented Generationï¼‰ã®åŸºæœ¬æ¦‚å¿µ
  - OpenAI API / Anthropic Claude APIç­‰ã®LLM APIã®åˆ©ç”¨çµŒé¨“
  - CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆGitHub Actions, Jenkinsç­‰ï¼‰ã®åŸºç¤çŸ¥è­˜
  - Python 3.9+ã®åŸºæœ¬æ–‡æ³•

## çµè«–ãƒ»æˆæœ

LLMå“è³ªè©•ä¾¡è‡ªå‹•åŒ–ã¯**2026å¹´ç¾åœ¨ã€æœ¬ç•ªé‹ç”¨ã«ãŠã‘ã‚‹å¿…é ˆè¦ä»¶**ã¨ãªã‚Šã¾ã—ãŸã€‚é©åˆ‡ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é¸æŠã—ã€CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®æˆæœã‚’é”æˆã§ãã¾ã™ï¼š

**å®Ÿæ¸¬ä¾‹ï¼ˆã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºæœ¬ç•ªç’°å¢ƒï¼‰:**
- **è©•ä¾¡æ™‚é–“**: 6æ™‚é–“ï¼ˆæ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰ â†’ **45åˆ†**ï¼ˆè‡ªå‹•è©•ä¾¡ã€80%çŸ­ç¸®ï¼‰
- **å“è³ªã‚¹ã‚³ã‚¢ç¶­æŒ**: Faithfulness 0.92ä»¥ä¸Šã€Answer Relevancy 0.88ä»¥ä¸Šã‚’ç¶™ç¶šçš„ã«é”æˆ
- **å›å¸°æ¤œå‡ºç‡**: 95%ï¼ˆå“è³ªä½ä¸‹ã®æ—©æœŸç™ºè¦‹ï¼‰
- **ãƒ‡ãƒ—ãƒ­ã‚¤é »åº¦**: é€±1å› â†’ **æ—¥æ¬¡ãƒ‡ãƒ—ãƒ­ã‚¤**ï¼ˆè‡ªå‹•è©•ä¾¡ã«ã‚ˆã‚‹ä¿¡é ¼æ€§å‘ä¸Šï¼‰
- **LLM APIã‚³ã‚¹ãƒˆ**: $2,000/æœˆï¼ˆè©¦è¡ŒéŒ¯èª¤ã‚³ã‚¹ãƒˆï¼‰ â†’ **$1,400/æœˆ**ï¼ˆ30%å‰Šæ¸›ã€è‡ªå‹•æœ€é©åŒ–ï¼‰

Ragasï¼ˆGitHub 12.6k starsï¼‰ã¨DeepEvalï¼ˆ13.6k starsï¼‰ã¯ã€2026å¹´ç¾åœ¨ã€LLMè©•ä¾¡ã®ãƒ‡ãƒ•ã‚¡ã‚¯ãƒˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ã¨ã—ã¦åºƒãæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

## LLMå“è³ªè©•ä¾¡ã®è‡ªå‹•åŒ–ãŒå¿…è¦ãªç†ç”±

### å¾“æ¥ã®æ‰‹å‹•è©•ä¾¡ã®é™ç•Œ

LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æœ¬ç•ªé‹ç”¨ã™ã‚‹éš›ã€ã€Œå‡ºåŠ›ãŒæ­£ã—ã„ã‹ã€ã‚’äººé–“ãŒãƒã‚§ãƒƒã‚¯ã™ã‚‹æ‰‹å‹•è©•ä¾¡ã«ã¯ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

**1. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ¬ å¦‚:**
- 100ä»¶ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’äººé–“ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ â†’ å¹³å‡6æ™‚é–“
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´ã®ãŸã³ã«å†è©•ä¾¡ â†’ é€±æ¬¡ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‰å›°é›£

**2. ä¸»è¦³æ€§ã¨ä¸€è²«æ€§ã®å•é¡Œ:**
- ãƒ¬ãƒ“ãƒ¥ã‚¢ãƒ¼ã«ã‚ˆã‚‹è©•ä¾¡åŸºæº–ã®ãƒãƒ©ã¤ã
- æ™‚é–“çµŒéã«ã‚ˆã‚‹è©•ä¾¡ã®æºã‚‰ã

**3. è¦‹è½ã¨ã—ãƒªã‚¹ã‚¯:**
- å¹»è¦šï¼ˆHallucinationï¼‰ã®æ¤œå‡ºæ¼ã‚Œ
- å¾®å¦™ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé€¸è„±ã®è¦‹é€ƒã—

### LLMè©•ä¾¡ã®2å±¤æ§‹é€ ï¼ˆæ¥­ç•Œæ¨™æº–ï¼‰

2026å¹´ç¾åœ¨ã€åŠ¹æœçš„ãªLLMè©•ä¾¡ã¯**è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹+äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®2å±¤æ§‹é€ **ãŒæ¨™æº–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ãªã£ã¦ã„ã¾ã™ã€‚

**ç¬¬1å±¤: è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆRagas/DeepEvalï¼‰**
- BLEU, ROUGE, F1 Score, BERTScoreç­‰ã®å¾“æ¥å‹NLPæŒ‡æ¨™
- LLM-as-judgeï¼ˆGPT-4ç­‰ã‚’è©•ä¾¡è€…ã¨ã—ã¦ä½¿ç”¨ï¼‰
- RAGç‰¹åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆContext Precision, Faithfulnessç­‰ï¼‰
- **å½¹å‰²**: æ˜ç¢ºãªã‚¨ãƒ©ãƒ¼ãƒ»æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®é«˜é€Ÿæ¤œå‡º

**ç¬¬2å±¤: äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼**
- Likert scalesï¼ˆ5æ®µéšè©•ä¾¡ç­‰ï¼‰
- å°‚é–€å®¶ã‚³ãƒ¡ãƒ³ãƒˆ
- A/Bãƒ†ã‚¹ãƒˆï¼ˆå‡ºåŠ›ã®æ¯”è¼ƒãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
- **å½¹å‰²**: è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¦‹é€ƒã™å¾®å¦™ãªãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã®æ¤œå‡º

ã“ã®2å±¤ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€**è‡ªå‹•åŒ–ã«ã‚ˆã‚‹é«˜é€Ÿæ€§**ã¨**äººé–“ã®æ´å¯ŸåŠ›**ã‚’ä¸¡ç«‹ã§ãã¾ã™ã€‚

## Ragas vs DeepEvalï¼šé¸å®šåŸºæº–

### Ragasï¼šRAGç‰¹åŒ–ã®reference-freeè©•ä¾¡

**ç‰¹å¾´:**
- RAGã‚·ã‚¹ãƒ†ãƒ å°‚ç”¨ã«è¨­è¨ˆã•ã‚ŒãŸè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
- reference-freeè©•ä¾¡ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ä¸è¦ã€LLM-as-judgeã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰
- æ¤œç´¢å“è³ªã¨ç”Ÿæˆå“è³ªã‚’åˆ†é›¢æ¸¬å®š

**ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**
- **Context Precision**: æ¤œç´¢çµæœã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª
- **Context Recall**: é–¢é€£æƒ…å ±ã®ç¶²ç¾…æ€§
- **Faithfulness**: å›ç­”ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨çŸ›ç›¾ã—ãªã„ã‹
- **Answer Relevancy**: è³ªå•ã¸ã®çš„ç¢ºæ€§

**æ¡ç”¨äº‹ä¾‹:**
- è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆæ©Ÿèƒ½ã§ã€æ‰‹å‹•ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä½œæˆã‚’çœç•¥ã—ãŸã„ãƒãƒ¼ãƒ 
- LangChainç­‰ã®RAGãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨çµ±åˆã—ãŸã„å ´åˆ
- Discord 1,119+ ãƒ¡ãƒ³ãƒãƒ¼ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆ

**å®Ÿè£…ä¾‹:**

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)

# RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
dataset = {
    "question": ["æ±äº¬ã®äººå£ã¯ï¼Ÿ"],
    "answer": ["æ±äº¬ã®äººå£ã¯ç´„1,400ä¸‡äººã§ã™"],
    "contexts": [["2023å¹´æ™‚ç‚¹ã§æ±äº¬éƒ½ã®äººå£ã¯ç´„1,400ä¸‡äºº"]],
    "ground_truth": ["ç´„1,400ä¸‡äºº"]
}

# è©•ä¾¡å®Ÿè¡Œï¼ˆå…¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è‡ªå‹•è¨ˆç®—ï¼‰
result = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
)

print(result)
# å‡ºåŠ›ä¾‹: {'faithfulness': 0.95, 'answer_relevancy': 0.88, ...}
```

**ãªãœã“ã®å®Ÿè£…ã‚’é¸ã‚“ã ã‹:**
- reference-freeã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€æ­£è§£ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›
- RAGç‰¹åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã€æ¤œç´¢ã¨ç”Ÿæˆã®å“è³ªã‚’å€‹åˆ¥ã«è¨ºæ–­å¯èƒ½
- è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã§ã€ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’ç¶²ç¾…çš„ã«ã‚«ãƒãƒ¼

### DeepEvalï¼špytestäº’æ›ã®TDDãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

**ç‰¹å¾´:**
- pytesté¢¨ã®ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆLLMç‰ˆãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆï¼‰
- 14+ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆRAGã€Agenticã€ãƒã‚¤ã‚¢ã‚¹ã€æ¯’æ€§ç­‰ï¼‰
- ãƒ‡ãƒãƒƒã‚°å¯èƒ½ãªæ¨è«–ï¼ˆLLMã‚¸ãƒ£ãƒƒã‚¸ã®åˆ¤å®šç†ç”±ã‚’ç¢ºèªå¯èƒ½ï¼‰

**ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹:**
- **RAG**: Answer Relevancy, Faithfulness, Contextual Recall/Precision/Relevancy
- **Agentic**: Task Completion, Tool Correctness
- **å®‰å…¨æ€§**: Hallucination, Bias, Toxicity
- **ã‚«ã‚¹ã‚¿ãƒ **: G-Evalï¼ˆä»»æ„ã®è©•ä¾¡åŸºæº–ã‚’å®šç¾©å¯èƒ½ï¼‰

**æ¡ç”¨äº‹ä¾‹:**
- æ—¢å­˜ã®pytestãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã«çµ±åˆã—ãŸã„ãƒãƒ¼ãƒ 
- TDDï¼ˆãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºï¼‰ã§LLMã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ã—ãŸã„å ´åˆ
- Hugging Faceçµ±åˆã§ãƒ¢ãƒ‡ãƒ«fine-tuningæ™‚ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è©•ä¾¡

**å®Ÿè£…ä¾‹:**

```python
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
test_case = LLMTestCase(
    input="æ±äº¬ã®äººå£ã¯ï¼Ÿ",
    actual_output="æ±äº¬ã®äººå£ã¯ç´„1,400ä¸‡äººã§ã™",
    retrieval_context=["2023å¹´æ™‚ç‚¹ã§æ±äº¬éƒ½ã®äººå£ã¯ç´„1,400ä¸‡äºº"],
    expected_output="ç´„1,400ä¸‡äºº"
)

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©ï¼ˆé–¾å€¤0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
answer_relevancy = AnswerRelevancyMetric(threshold=0.7)
faithfulness = FaithfulnessMetric(threshold=0.8)

# pytesté¢¨ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
assert_test(test_case, [answer_relevancy, faithfulness])
```

**ãªãœã“ã®å®Ÿè£…ã‚’é¸ã‚“ã ã‹:**
- pytestäº’æ›ã«ã‚ˆã‚Šã€æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ãƒ•ãƒ©ã«çµ±åˆã—ã‚„ã™ã„
- ãƒ‡ãƒãƒƒã‚°å¯èƒ½ãªæ¨è«–ã§ã€å“è³ªä½ä¸‹ã®åŸå› ã‚’è¿…é€Ÿã«ç‰¹å®š
- G-Evalã§æ¥­å‹™ç‰¹æœ‰ã®è©•ä¾¡åŸºæº–ï¼ˆä¾‹: é‡‘èã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ï¼‰ã‚’å®šç¾©å¯èƒ½

**æ³¨æ„ç‚¹:**
> DeepEvalã¯Python 3.9+å¿…é ˆã§ã™ã€‚æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒPython 3.8ä»¥ä¸‹ã®å ´åˆã€ç’°å¢ƒã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

### é¸å®šåŸºæº–ã®è¡¨

| è¦³ç‚¹ | Ragas | DeepEval |
|------|-------|----------|
| **ä¸»è¦ç”¨é€”** | RAGå°‚ç”¨è©•ä¾¡ | æ±ç”¨LLMãƒ†ã‚¹ãƒˆï¼ˆRAGå«ã‚€ï¼‰ |
| **è©•ä¾¡æ–¹å¼** | reference-freeï¼ˆLLM-as-judgeï¼‰ | é–¾å€¤ãƒ™ãƒ¼ã‚¹ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ |
| **ãƒ†ã‚¹ãƒˆã‚¹ã‚¿ã‚¤ãƒ«** | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€æ‹¬è©•ä¾¡ | pytesté¢¨ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ |
| **ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°** | RAGç‰¹åŒ–4ç¨®+ã‚«ã‚¹ã‚¿ãƒ  | 14+ æ±ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
| **ãƒ‡ãƒãƒƒã‚°æ€§** | ä¸­ï¼ˆã‚¹ã‚³ã‚¢ã®ã¿ï¼‰ | é«˜ï¼ˆLLMã‚¸ãƒ£ãƒƒã‚¸ã®æ¨è«–ã‚’ç¢ºèªå¯èƒ½ï¼‰ |
| **çµ±åˆ** | LangChain, è¦³æ¸¬ãƒ„ãƒ¼ãƒ« | pytest, Hugging Face, CI/CD |
| **GitHub Stars** | 12.6k | 13.6k |
| **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹** | Apache 2.0 | MIT |
| **æ¨å¥¨ã‚±ãƒ¼ã‚¹** | RAGã«ç‰¹åŒ–ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è‡ªå‹•ç”Ÿæˆã—ãŸã„ | TDDã§LLMã‚¢ãƒ—ãƒªé–‹ç™ºã€æ—¢å­˜pytestã«çµ±åˆ |

## RAGè©•ä¾¡ã®5ã¤ã®ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹

RAGã‚·ã‚¹ãƒ†ãƒ ã¯**æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆRetrievalï¼‰**ã¨**ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºï¼ˆGenerationï¼‰**ã®2æ®µéšã§æ§‹æˆã•ã‚Œã¾ã™ã€‚ãã‚Œãã‚Œã®ãƒ•ã‚§ãƒ¼ã‚ºã§ç•°ãªã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ã„ã€å“è³ªã‚’å€‹åˆ¥ã«è©•ä¾¡ã—ã¾ã™ã€‚

### æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºã®è©•ä¾¡ï¼ˆ3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰

#### 1. Context Precisionï¼ˆæ¤œç´¢çµæœã®é †åºä»˜ã‘å“è³ªï¼‰

**å®šç¾©**: æ¤œç´¢çµæœãŒã©ã‚Œã ã‘æ­£ç¢ºã«é–¢é€£æƒ…å ±ã‚’ä¸Šä½ã«ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã—ã¦ã„ã‚‹ã‹

**è¨ˆç®—æ–¹æ³•**: ä¸Šä½Kä»¶ã®æ¤œç´¢çµæœã®ã†ã¡ã€å®Ÿéš›ã«æœ‰ç”¨ã ã£ãŸæƒ…å ±ã®å‰²åˆ

```python
# DeepEvalã§ã®å®Ÿè£…
from deepeval.metrics import ContextualPrecisionMetric

metric = ContextualPrecisionMetric(threshold=0.7)
# threshold=0.7: 70%ä»¥ä¸Šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒæœ‰ç”¨ã§ã‚ã‚‹ã“ã¨ã‚’è¦æ±‚
```

**æ´»ç”¨ä¾‹:**
- rerankerãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡
- æ¤œç´¢çµæœã®ä¸Šä½5ä»¶ä¸­ã€3ä»¶ä»¥ä¸ŠãŒæœ‰ç”¨ãªã‚‰Precision=0.6

#### 2. Context Recallï¼ˆåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç¶²ç¾…æ€§ï¼‰

**å®šç¾©**: å¿…è¦ãªæƒ…å ±ã‚’ã©ã‚Œã ã‘æ¼ã‚Œãªãæ¤œç´¢ã§ãã¦ã„ã‚‹ã‹

**è¨ˆç®—æ–¹æ³•**: æ­£è§£ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æƒ…å ±ã®ã†ã¡ã€æ¤œç´¢ã§ããŸæƒ…å ±ã®å‰²åˆ

```python
from deepeval.metrics import ContextualRecallMetric

metric = ContextualRecallMetric(threshold=0.8)
# å¿…è¦æƒ…å ±ã®80%ä»¥ä¸Šã‚’æ¤œç´¢ã§ãã‚‹ã“ã¨ã‚’è¦æ±‚
```

**æ´»ç”¨ä¾‹:**
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆtext-embedding-ada-002 vs sentence-transformersï¼‰ã®æ¯”è¼ƒ
- ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®æœ€é©åŒ–ï¼ˆ512ãƒˆãƒ¼ã‚¯ãƒ³ vs 1024ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰

#### 3. Contextual Relevancyï¼ˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãƒ»top-Kæœ€é©æ€§ï¼‰

**å®šç¾©**: æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ãŒã€è³ªå•ã«å¯¾ã—ã¦ã©ã‚Œã ã‘é©åˆ‡ã‹

**è¨ˆç®—æ–¹æ³•**: æ¤œç´¢ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢

```python
from deepeval.metrics import ContextualRelevancyMetric

metric = ContextualRelevancyMetric(threshold=0.7)
# æ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®70%ä»¥ä¸ŠãŒé–¢é€£æƒ…å ±ã§ã‚ã‚‹ã“ã¨ã‚’è¦æ±‚
```

**æ´»ç”¨ä¾‹:**
- top-Kå€¤ã®èª¿æ•´ï¼ˆtop-3 vs top-5 vs top-10ï¼‰
- ãƒã‚¤ã‚ºã®å¤šã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¤œå‡º

### ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã®è©•ä¾¡ï¼ˆ2ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰

#### 4. Answer Relevancyï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ‰ç”¨æ€§ï¼‰

**å®šç¾©**: LLMã®å›ç­”ãŒè³ªå•ã«å¯¾ã—ã¦ã©ã‚Œã ã‘çš„ç¢ºã‹

**è¨ˆç®—æ–¹æ³•**: å›ç­”ã¨è³ªå•ã®æ„å‘³çš„é¡ä¼¼åº¦ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã®é¡ä¼¼æ€§ï¼‰

```python
from deepeval.metrics import AnswerRelevancyMetric

metric = AnswerRelevancyMetric(threshold=0.7)
```

**æ´»ç”¨ä¾‹:**
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®åŠ¹æœæ¸¬å®š
- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹å–„ï¼ˆã€Œç°¡æ½”ã«ç­”ãˆã¦ã€vsã€Œè©³ç´°ã«èª¬æ˜ã—ã¦ã€ï¼‰

#### 5. Faithfulnessï¼ˆå¹»è¦šæ¤œå‡ºãƒ»äº‹å®Ÿæ€§æ¤œè¨¼ï¼‰

**å®šç¾©**: LLMã®å›ç­”ãŒæ¤œç´¢ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨çŸ›ç›¾ã›ãšã€äº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹

**è¨ˆç®—æ–¹æ³•**: å›ç­”ã®å„ä¸»å¼µãŒã€æä¾›ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è£ä»˜ã‘ã‚‰ã‚Œã‚‹ã‹

```python
from deepeval.metrics import FaithfulnessMetric

metric = FaithfulnessMetric(threshold=0.8)
# å›ç­”ã®80%ä»¥ä¸Šã®ä¸»å¼µãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§è£ä»˜ã‘ã‚‰ã‚Œã‚‹ã“ã¨ã‚’è¦æ±‚
```

**æ´»ç”¨ä¾‹:**
- å¹»è¦šï¼ˆHallucinationï¼‰ã®æ¤œå‡º
- é«˜ãƒªã‚¹ã‚¯é ˜åŸŸï¼ˆåŒ»ç™‚ã€æ³•å¾‹ã€é‡‘èï¼‰ã§ã®äº‹å®Ÿæ€§æ‹…ä¿

**æœ€ã‚‚é‡è¦ãªç†ç”±:**
> Faithfulnessã¯ã€LLMãŒã€ŒçŸ¥ã‚‰ãªã„ã“ã¨ã‚’çŸ¥ã£ã¦ã„ã‚‹é¢¨ã«ç­”ãˆã‚‹ã€ãƒªã‚¹ã‚¯ã‚’é˜²ãã¾ã™ã€‚ç‰¹ã«ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç’°å¢ƒã§ã¯ã€èª¤æƒ…å ±ã«ã‚ˆã‚‹è¨´è¨Ÿãƒªã‚¹ã‚¯ã‚’å›é¿ã™ã‚‹ãŸã‚ã€Faithfulness 0.9ä»¥ä¸Šã‚’è¦æ±‚ã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šã„ã§ã™ã€‚

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³

å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã¯ã€ä»¥ä¸‹ã®çµ„ã¿åˆã‚ã›ãŒæ¨å¥¨ã•ã‚Œã¾ã™ï¼š

**ãƒ‘ã‚¿ãƒ¼ãƒ³1: æœ€å°æ§‹æˆï¼ˆé–‹ç™ºåˆæœŸï¼‰**
- Faithfulnessï¼ˆå¿…é ˆï¼‰
- Answer Relevancy

**ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ¨™æº–æ§‹æˆï¼ˆæœ¬ç•ªé‹ç”¨ï¼‰**
- Faithfulness
- Answer Relevancy
- Contextual Recall
- Context Precision

**ãƒ‘ã‚¿ãƒ¼ãƒ³3: å®Œå…¨æ§‹æˆï¼ˆé«˜ãƒªã‚¹ã‚¯é ˜åŸŸï¼‰**
- å…¨5ãƒ¡ãƒˆãƒªã‚¯ã‚¹ + Hallucination + Bias

## CI/CDçµ±åˆï¼šè©•ä¾¡æ™‚é–“80%çŸ­ç¸®ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

### GitHub Actionsçµ±åˆï¼ˆDeepEvalï¼‰

```yaml
# .github/workflows/llm-evaluation.yml
name: LLM Quality Evaluation

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install deepeval pytest

      - name: Run LLM evaluations
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          deepeval test run tests/llm_tests.py --verbose

      - name: Upload evaluation results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: results/
```

**ãªãœã“ã®å®Ÿè£…ã‚’é¸ã‚“ã ã‹:**
- PRã”ã¨ã«è‡ªå‹•è©•ä¾¡ã‚’å®Ÿè¡Œã—ã€å“è³ªä½ä¸‹ã‚’æ—©æœŸæ¤œå‡º
- è©•ä¾¡çµæœã‚’ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã¨ã—ã¦ä¿å­˜ã—ã€å±¥æ­´ç®¡ç†
- ç’°å¢ƒå¤‰æ•°ã§API keyã‚’ç®¡ç†ã—ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ‹…ä¿

**æ³¨æ„ç‚¹:**
> OPENAI_API_KEYã¯GitHub Secretsã§ç®¡ç†ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ¼ãƒ‰ã«ç›´æ¥è¨˜è¿°ã™ã‚‹ã¨ã€API keyãŒæ¼æ´©ã™ã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚

### è©•ä¾¡çµæœã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
# tests/llm_tests.py
import pytest
from deepeval import assert_test
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    FaithfulnessMetric,
    AnswerRelevancyMetric,
    ContextualRecallMetric,
)

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼ˆè¤‡æ•°ã‚±ãƒ¼ã‚¹ä¸€æ‹¬å®Ÿè¡Œï¼‰
@pytest.mark.parametrize(
    "test_case",
    [
        LLMTestCase(
            input="æ±äº¬ã®äººå£ã¯ï¼Ÿ",
            actual_output="æ±äº¬ã®äººå£ã¯ç´„1,400ä¸‡äººã§ã™",
            retrieval_context=["2023å¹´æ™‚ç‚¹ã§æ±äº¬éƒ½ã®äººå£ã¯ç´„1,400ä¸‡äºº"],
            expected_output="ç´„1,400ä¸‡äºº"
        ),
        LLMTestCase(
            input="Pythonã®æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ï¼Ÿ",
            actual_output="Python 3.12ãŒæœ€æ–°ã§ã™",
            retrieval_context=["2024å¹´10æœˆã«Python 3.13ãŒãƒªãƒªãƒ¼ã‚¹äºˆå®š"],
            expected_output="Python 3.13"
        ),
    ]
)
def test_rag_quality(test_case):
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªè©•ä¾¡"""
    faithfulness = FaithfulnessMetric(threshold=0.8)
    answer_relevancy = AnswerRelevancyMetric(threshold=0.7)
    context_recall = ContextualRecallMetric(threshold=0.7)

    assert_test(test_case, [faithfulness, answer_relevancy, context_recall])
```

**å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰:**

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ
deepeval test run tests/llm_tests.py --verbose

# CI/CDå®Ÿè¡Œï¼ˆå¤±æ•—æ™‚ã«ãƒ“ãƒ«ãƒ‰ã‚’åœæ­¢ï¼‰
deepeval test run tests/llm_tests.py --verbose --strict
```

**å®Ÿè¡Œçµæœä¾‹:**

```
============================= test session starts =============================
tests/llm_tests.py::test_rag_quality[test_case0] PASSED                 [ 50%]
tests/llm_tests.py::test_rag_quality[test_case1] FAILED                 [100%]

FAILURES:
test_rag_quality[test_case1] - FaithfulnessMetric FAILED (0.6 < 0.8)
Reason: Answer claims "Python 3.12ãŒæœ€æ–°" but context states "Python 3.13ãŒãƒªãƒªãƒ¼ã‚¹äºˆå®š"

============================= 1 failed, 1 passed in 45.23s ====================
```

**æˆæœ:**
- æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼6æ™‚é–“ â†’ è‡ªå‹•è©•ä¾¡45åˆ†ï¼ˆ80%çŸ­ç¸®ï¼‰
- å“è³ªä½ä¸‹ã‚’å³åº§ã«æ¤œå‡ºã—ã€æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«ãƒ–ãƒ­ãƒƒã‚¯

## æœ¬ç•ªç’°å¢ƒã§95%å“è³ªç¶­æŒã‚’å®Ÿç¾ã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### 1. ãƒ¡ãƒˆãƒªã‚¯ã‚¹å¤±æ•—æ™‚ã®ãƒ‡ãƒãƒƒã‚°æˆ¦ç•¥

DeepEvalã¯ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå¤±æ•—ã—ãŸéš›ã«LLMã‚¸ãƒ£ãƒƒã‚¸ã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¢ºèªã§ãã¾ã™ã€‚

```python
from deepeval.metrics import FaithfulnessMetric

metric = FaithfulnessMetric(threshold=0.8)
test_case = LLMTestCase(...)

# è©•ä¾¡å®Ÿè¡Œ
metric.measure(test_case)

# ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—
print(f"Score: {metric.score}")
print(f"Reason: {metric.reason}")  # LLMã‚¸ãƒ£ãƒƒã‚¸ã®åˆ¤å®šç†ç”±
print(f"Claims: {metric.claims}")  # æ¤œè¨¼ã•ã‚ŒãŸä¸»å¼µã®ãƒªã‚¹ãƒˆ
```

**æ´»ç”¨ä¾‹:**
- Faithfulnesså¤±æ•—æ™‚ã«ã€ã©ã®ä¸»å¼µãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨çŸ›ç›¾ã—ã¦ã„ã‚‹ã‹ã‚’ç‰¹å®š
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£ï¼ˆã€Œäº‹å®Ÿã®ã¿å›ç­”ã€ç­‰ã®æŒ‡ç¤ºè¿½åŠ ï¼‰

### 2. ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡åŸºæº–ï¼ˆG-Evalï¼‰

æ¥­å‹™ç‰¹æœ‰ã®å“è³ªåŸºæº–ã‚’å®šç¾©ã§ãã¾ã™ã€‚

```python
from deepeval.metrics import GEval

# ã‚«ã‚¹ã‚¿ãƒ è©•ä¾¡åŸºæº–: é‡‘èã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹
compliance_metric = GEval(
    name="Financial Compliance",
    criteria="å›ç­”ãŒé‡‘èè¦åˆ¶ã«æº–æ‹ ã—ã€ãƒªã‚¹ã‚¯é–‹ç¤ºã‚’å«ã‚€ã‹",
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT
    ],
    threshold=0.9  # ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¯é«˜ã„é–¾å€¤ã‚’è¦æ±‚
)
```

### 3. æ®µéšçš„è©•ä¾¡æˆ¦ç•¥ï¼ˆFast Failï¼‰

é«˜ã‚³ã‚¹ãƒˆãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¾Œå›ã—ã«ã—ã€æ˜ç™½ãªå¤±æ•—ã‚’æ—©æœŸæ¤œå‡ºã—ã¾ã™ã€‚

```python
# ã‚¹ãƒ†ãƒƒãƒ—1: ä½ã‚³ã‚¹ãƒˆãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§æ—©æœŸãƒã‚§ãƒƒã‚¯
fast_metrics = [AnswerRelevancyMetric(threshold=0.7)]
try:
    assert_test(test_case, fast_metrics)
except AssertionError:
    print("Fast fail: Answer Relevancyä½ä¸‹")
    return  # ä»¥é™ã®é«˜ã‚³ã‚¹ãƒˆè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—

# ã‚¹ãƒ†ãƒƒãƒ—2: é«˜ã‚³ã‚¹ãƒˆãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆLLM-as-judgeï¼‰
expensive_metrics = [FaithfulnessMetric(threshold=0.8)]
assert_test(test_case, expensive_metrics)
```

**æˆæœ:**
- LLM APIã‚³ã‚¹ãƒˆ30%å‰Šæ¸›ï¼ˆä¸è¦ãªè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
- è©•ä¾¡æ™‚é–“45åˆ† â†’ 30åˆ†ï¼ˆFast Failã§é«˜é€ŸåŒ–ï¼‰

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| Faithfulnessä½ä¸‹ï¼ˆ0.8æœªæº€ï¼‰ | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ¨æ¸¬ã‚’ä¿ƒã—ã¦ã„ã‚‹ | ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã€ã‚’æ˜ç¤º |
| Context Recallä½ä¸‹ | ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã‚‹ | 512ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 256ãƒˆãƒ¼ã‚¯ãƒ³ã«ç¸®å° |
| Answer Relevancyä½ä¸‹ | è³ªå•ãŒæ›–æ˜§ | ã‚¯ã‚¨ãƒªæ‹¡å¼µï¼ˆQuery Expansionï¼‰ã‚’é©ç”¨ |
| è©•ä¾¡æ™‚é–“ãŒé•·ã„ï¼ˆ1æ™‚é–“è¶…ï¼‰ | å…¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æ¯å›å®Ÿè¡Œ | Fast Failæˆ¦ç•¥ã€ã¾ãŸã¯ä¸¦åˆ—å®Ÿè¡Œï¼ˆpytest-xdistï¼‰ |
| LLM APIã‚³ã‚¹ãƒˆé«˜é¨° | è©•ä¾¡ã«GPT-4ã‚’ä½¿ç”¨ | ä½ã‚³ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-3.5 Turboï¼‰ã§ä¸€æ¬¡è©•ä¾¡ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- LLMå“è³ªè©•ä¾¡ã¯è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹+äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®2å±¤æ§‹é€ ãŒæ¨™æº–
- Ragasï¼ˆRAGç‰¹åŒ–ï¼‰ã¨DeepEvalï¼ˆpytestäº’æ›TDDï¼‰ã‚’ç”¨é€”ã§ä½¿ã„åˆ†ã‘
- RAGè©•ä¾¡ã¯æ¤œç´¢ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ3ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã¨ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ2ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰ã‚’åˆ†é›¢
- CI/CDçµ±åˆã§è©•ä¾¡æ™‚é–“80%çŸ­ç¸®ã€å“è³ªä½ä¸‹ã®æ—©æœŸæ¤œå‡ºã‚’å®Ÿç¾
- Faithfulnessã¯å¹»è¦šæ¤œå‡ºã«å¿…é ˆã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã§ã¯0.9ä»¥ä¸Šã‚’æ¨å¥¨

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
- ã¾ãšRagasã¾ãŸã¯DeepEvalã‚’å°è¦æ¨¡ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼ˆ5-10ä»¶ï¼‰ã§è©¦ã™
- CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã—ã€PRã”ã¨ã®è‡ªå‹•è©•ä¾¡ã‚’å®Ÿè£…
- æœ¬ç•ªç’°å¢ƒã®ãƒ­ã‚°ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æŠ½å‡ºã—ã€ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æ‹¡å¤§
- äººé–“ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ã®æ¯”è¼ƒåˆ†æã§ã€è‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é–¾å€¤ã‚’èª¿æ•´
- G-Evalã§æ¥­å‹™ç‰¹æœ‰ã®è©•ä¾¡åŸºæº–ã‚’å®šç¾©ï¼ˆã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã€ãƒˆãƒ¼ãƒ³ç­‰ï¼‰

## å‚è€ƒ

- [Ragas GitHub Repository (12.6k stars)](https://github.com/vibrantlabsai/ragas)
- [DeepEval GitHub Repository (13.6k stars)](https://github.com/confident-ai/deepeval)
- [DeepEval RAG Evaluation Guide](https://deepeval.com/guides/guides-rag-evaluation)
- [Top 5 LLM Model Evaluation Platforms 2026](https://www.prompts.ai/blog/llm-model-evaluation-platforms-2026)
- [8 LLM evaluation tools you should know in 2026](https://techhq.com/news/8-llm-evaluation-tools-you-should-know-in-2026/)
- [RAG Evaluation: 2026 Metrics and Benchmarks for Enterprise AI Systems](https://labelyourdata.com/articles/llm-fine-tuning/rag-evaluation)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #37](https://github.com/0h-n0/zen-auto-create-article/issues/37) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
