---
title: "LLMå“è³ªè©•ä¾¡ã®å®Œå…¨è‡ªå‹•åŒ–ï¼š85%ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆ500å€å‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ“Š"
type: "tech"
topics: ["llm", "evaluation", "ai", "testing", "automation"]
published: false
---

# LLMå“è³ªè©•ä¾¡ã®å®Œå…¨è‡ªå‹•åŒ–ï¼š85%ç²¾åº¦ã¨ã‚³ã‚¹ãƒˆ500å€å‰Šæ¸›ã‚’å®Ÿç¾ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LLM-as-a-Judgeæ‰‹æ³•ã§äººé–“è©•ä¾¡ã®500-5000å€ã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•
- GPT-4è©•ä¾¡ãŒãƒ’ãƒˆè©•ä¾¡ã¨85%ä¸€è‡´ï¼ˆãƒ’ãƒˆåŒå£«81%ã‚’ä¸Šå›ã‚‹ï¼‰ã—ãŸæœ€æ–°ç ”ç©¶ã®è©³ç´°
- ä¸»è¦è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯7ç¨®ï¼ˆDeepEvalã€RAGASã€Inspect AIç­‰ï¼‰ã®ç‰¹å¾´ã¨é¸å®šåŸºæº–
- **æœ¬ç•ªé‹ç”¨ã§ç›´é¢ã™ã‚‹4ã¤ã®ãƒã‚¤ã‚¢ã‚¹**ã¨è§£æ±ºç­–ï¼ˆPosition/Length/Concreteness/Self-Enhancementï¼‰
- ã€Œ5 Metric Ruleã€ã«åŸºã¥ãåŠ¹ç‡çš„ãªè©•ä¾¡æŒ‡æ¨™è¨­è¨ˆ

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–‹ç™ºè€…ã€å“è³ªä¿è¨¼æ‹…å½“è€…
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.10+ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹
  - OpenAI API/Anthropic Claude APIã®åŸºç¤çŸ¥è­˜
  - LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè£…çµŒé¨“ï¼ˆRAG/Chatbot/Agentç­‰ï¼‰

## çµè«–ãƒ»æˆæœ

LLM-as-a-Judgeæ‰‹æ³•ã«ã‚ˆã‚Šã€**å¾“æ¥ã®äººé–“è©•ä¾¡ã¨åŒç­‰ã®å“è³ªï¼ˆ85%ä¸€è‡´ç‡ï¼‰ã‚’ç¶­æŒã—ãªãŒã‚‰ã€ã‚³ã‚¹ãƒˆã‚’500-5000å€å‰Šæ¸›**ã§ãã¾ã™ã€‚2026å¹´ç¾åœ¨ã€ä¸»è¦ä¼æ¥­ã®89%ãŒæœ¬ç•ªç’°å¢ƒã§LLMè‡ªå‹•è©•ä¾¡ã‚’å°å…¥ã—ã€å¹³å‡30%ã®é–‹ç™ºæœŸé–“çŸ­ç¸®ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

## LLMè©•ä¾¡ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚·ãƒ•ãƒˆï¼šçµ±è¨ˆæŒ‡æ¨™ã‹ã‚‰LLM-as-a-Judgeã¸

### å¾“æ¥ã®çµ±è¨ˆæŒ‡æ¨™ã®é™ç•Œ

BLEUã€ROUGEã€METEORã¨ã„ã£ãŸå¾“æ¥ã®çµ±è¨ˆæŒ‡æ¨™ã¯ã€n-gramã®é‡è¤‡ã‚„ç·¨é›†è·é›¢ã‚’æ¸¬å®šã—ã¾ã™ãŒã€**æ„å‘³çš„ç†è§£ã‚„æ¨è«–èƒ½åŠ›ãŒã»ã¼ã‚¼ãƒ­**ã§ã™ã€‚ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã¯ã€ã‚¹ãƒšãƒ«ãƒã‚§ãƒƒã‚¯ã®ã‚ˆã†ãªç‰¹åŒ–ã‚¿ã‚¹ã‚¯ã§ã®ã¿æœ‰åŠ¹ã§ã€è¤‡é›‘ãªLLMå‡ºåŠ›ã®è©•ä¾¡ã«ã¯ä¸é©åˆ‡ã§ã™ã€‚

```python
from nltk.translate.bleu_score import sentence_bleu

# å¾“æ¥ã®BLEUè©•ä¾¡ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’ç„¡è¦–ï¼‰
reference = [["the", "cat", "is", "on", "the", "mat"]]
candidate = ["the", "cat", "sat", "on", "the", "mat"]
score = sentence_bleu(reference, candidate)  # 0.75ï¼ˆ"sat"ã®é•ã„ã—ã‹æ¤œå‡ºã§ããªã„ï¼‰
```

**ãªãœå¾“æ¥æ‰‹æ³•ã§ã¯ä¸ååˆ†ã‹:**
- æ„å‘³ãŒåŒã˜ã§ã‚‚è¡¨ç¾ãŒç•°ãªã‚‹ã¨ä½ã‚¹ã‚³ã‚¢ï¼ˆ"happy" vs "joyful"ï¼‰
- æ–‡è„ˆã‚’ç„¡è¦–ã—ãŸè©•ä¾¡ï¼ˆå‰å¾Œã®æ–‡ç« ã¨ã®é–¢é€£æ€§ã‚’è€ƒæ…®ã—ãªã„ï¼‰
- å‰µé€ çš„ãªå‡ºåŠ›ã‚’æ­£ã—ãè©•ä¾¡ã§ããªã„ï¼ˆå°èª¬ç”Ÿæˆã€è©©ä½œç­‰ï¼‰

### LLM-as-a-Judgeï¼šè©•ä¾¡ã®æ°‘ä¸»åŒ–

LLM-as-a-Judgeã¯ã€**LLMè‡ªèº«ã‚’è©•ä¾¡è€…ã¨ã—ã¦ä½¿ç”¨**ã™ã‚‹é©æ–°çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚äººé–“ã¨åŒç­‰ã®è‡ªç„¶è¨€èªç†è§£èƒ½åŠ›ã‚’æŒã¤LLMã«è©•ä¾¡åŸºæº–ã‚’æ˜ç¤ºçš„ã«æŒ‡ç¤ºã™ã‚‹ã“ã¨ã§ã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
from openai import OpenAI

client = OpenAI()

def evaluate_response(user_query: str, llm_output: str) -> dict:
    """LLM-as-a-Judgeå®Ÿè£…ä¾‹"""
    evaluation_prompt = f"""
    ä»¥ä¸‹ã®è¦³ç‚¹ã§LLMã®å›ç­”ã‚’1-5ã§è©•ä¾¡ã—ã¦ãã ã•ã„:

    ã€è©•ä¾¡åŸºæº–ã€‘
    1. é–¢é€£æ€§: ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã«é©åˆ‡ã«ç­”ãˆã¦ã„ã‚‹ã‹
    2. æ­£ç¢ºæ€§: äº‹å®Ÿã«åŸºã¥ã„ãŸæƒ…å ±ã‹
    3. æœ‰ç”¨æ€§: å®Ÿç”¨çš„ã§å…·ä½“çš„ãªå†…å®¹ã‹
    4. ç°¡æ½”æ€§: å†—é•·ãªèª¬æ˜ã‚’é¿ã‘ã¦ã„ã‚‹ã‹

    ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_query}
    LLMå›ç­”: {llm_output}

    JSONå½¢å¼ã§å‡ºåŠ›:
    {{"relevance": ç‚¹æ•°, "accuracy": ç‚¹æ•°, "usefulness": ç‚¹æ•°, "conciseness": ç‚¹æ•°, "reason": "ç†ç”±"}}
    """

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": evaluation_prompt}],
        response_format={"type": "json_object"}
    )

    return response.choices[0].message.content
```

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆ2026å¹´ç ”ç©¶ï¼‰:**
- GPT-4 vs äººé–“è©•ä¾¡ä¸€è‡´ç‡: **85%**
- äººé–“åŒå£«ã®ä¸€è‡´ç‡: 81%
- ã‚³ã‚¹ãƒˆ: äººé–“è©•ä¾¡ã®**500-5000åˆ†ã®1**

## æœ¬ç•ªé‹ç”¨ã§ç›´é¢ã™ã‚‹4ã¤ã®ãƒã‚¤ã‚¢ã‚¹ã¨å¯¾ç­–

LLM-as-a-Judgeã¯å¼·åŠ›ã§ã™ãŒã€**ä½“ç³»çš„ãªãƒã‚¤ã‚¢ã‚¹**ã‚’æŒã¡ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§95%ä»¥ä¸Šã®ä¿¡é ¼æ€§ã‚’ç¢ºä¿ã™ã‚‹ã«ã¯ã€ã“ã‚Œã‚‰ã®å¯¾ç­–ãŒå¿…é ˆã§ã™ã€‚

### 1. Position Biasï¼ˆé †åºãƒã‚¤ã‚¢ã‚¹ï¼‰

è©•ä¾¡å¯¾è±¡ã®æç¤ºé †åºã§çµæœãŒå¤‰ã‚ã‚‹å•é¡Œã€‚Aâ†’B ã¨ Bâ†’A ã§è©•ä¾¡ãŒé€†è»¢ã™ã‚‹ã‚±ãƒ¼ã‚¹ãŒ30-40%å­˜åœ¨ã—ã¾ã™ã€‚

**å¯¾ç­–: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ« + è¤‡æ•°å›è©•ä¾¡**

```python
import random
from statistics import mean

def robust_comparison(response_a: str, response_b: str, trials: int = 3) -> str:
    """Position Biasã‚’è»½æ¸›ã™ã‚‹è¤‡æ•°å›è©•ä¾¡"""
    results = []

    for _ in range(trials):
        # ãƒ©ãƒ³ãƒ€ãƒ ã«é †åºã‚’å…¥ã‚Œæ›¿ãˆ
        if random.random() > 0.5:
            score = evaluate_pair(response_a, response_b)
            results.append("A" if score > 0 else "B")
        else:
            score = evaluate_pair(response_b, response_a)
            results.append("B" if score > 0 else "A")

    # å¤šæ•°æ±ºã§æœ€çµ‚åˆ¤å®š
    return max(set(results), key=results.count)
```

### 2. Length Biasï¼ˆé•·ã•ãƒã‚¤ã‚¢ã‚¹ï¼‰

é•·ã„å›ç­”ã‚’éå¤§è©•ä¾¡ã™ã‚‹å‚¾å‘ã€‚è©³ç´°ãªèª¬æ˜ãŒå¿…è¦ãªã„è³ªå•ã§ã‚‚ã€é•·æ–‡ã‚’é«˜ã‚¹ã‚³ã‚¢ã«ã—ã¦ã—ã¾ã„ã¾ã™ã€‚

**å¯¾ç­–: æ˜ç¤ºçš„ãªé•·ã•åˆ¶ç´„ + ç°¡æ½”æ€§ã‚¹ã‚³ã‚¢**

```python
evaluation_rubric = """
ã€ç°¡æ½”æ€§ã®è©•ä¾¡åŸºæº–ã€‘
5ç‚¹: å¿…è¦æœ€å°é™ã®æƒ…å ±ã§å®Œçµï¼ˆ100-200æ–‡å­—ï¼‰
3ç‚¹: ã‚„ã‚„å†—é•·ã ãŒè¨±å®¹ç¯„å›²ï¼ˆ200-400æ–‡å­—ï¼‰
1ç‚¹: éåº¦ã«å†—é•·ï¼ˆ400æ–‡å­—ä»¥ä¸Šã§å‰Šæ¸›å¯èƒ½ï¼‰

â€»ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ãŒã€Œç°¡å˜ãªèª¬æ˜ã€ã‚’æ±‚ã‚ã¦ã„ã‚‹å ´åˆã€é•·æ–‡ã¯æ¸›ç‚¹å¯¾è±¡
"""
```

### 3. Concreteness Biasï¼ˆå…·ä½“æ€§ãƒã‚¤ã‚¢ã‚¹ï¼‰

æ•°å€¤ãƒ»å¼•ç”¨ãƒ»æ¨©å¨ã‚’å«ã‚€å›ç­”ã‚’éå¤§è©•ä¾¡ã€‚å®šæ€§çš„ãªèª¬æ˜ãŒé©åˆ‡ãªå ´åˆã§ã‚‚ã€ç„¡æ„å‘³ãªæ•°å€¤ã‚’è¿½åŠ ã™ã‚‹ã¨é«˜è©•ä¾¡ã«ãªã‚Šã¾ã™ã€‚

**å¯¾ç­–: ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã«å¿œã˜ãŸè©•ä¾¡åŸºæº–è¨­è¨ˆ**

```python
task_specific_rubric = {
    "factual_qa": "çµ±è¨ˆã‚„å‡ºå…¸ã‚’å«ã‚€å›ç­”ã‚’é«˜è©•ä¾¡",
    "creative_writing": "å…·ä½“çš„æ•°å€¤ã¯ä¸è¦ã€‚å‰µé€ æ€§ã¨è¡¨ç¾åŠ›ã‚’é‡è¦–",
    "code_generation": "å®Ÿè¡Œå¯èƒ½æ€§ã¨ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å¯¾å¿œã‚’è©•ä¾¡"
}
```

### 4. Self-Enhancement Biasï¼ˆè‡ªå·±å¼·åŒ–ãƒã‚¤ã‚¢ã‚¹ï¼‰

åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®å‡ºåŠ›ã‚’é«˜è©•ä¾¡ã™ã‚‹å‚¾å‘ã€‚GPT-4ã§è©•ä¾¡ã™ã‚‹ã¨ã€GPT-4ç”Ÿæˆã®å›ç­”ãŒå„ªé‡ã•ã‚Œã¾ã™ã€‚

**å¯¾ç­–: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ + ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**

```python
async def cross_model_evaluation(output: str, criteria: str) -> dict:
    """è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã§ã‚¯ãƒ­ã‚¹è©•ä¾¡"""
    evaluators = [
        {"model": "gpt-4o", "provider": "openai"},
        {"model": "claude-3-5-sonnet-20241022", "provider": "anthropic"},
        {"model": "gemini-2.0-flash-exp", "provider": "google"}
    ]

    scores = []
    for evaluator in evaluators:
        score = await evaluate_with_model(output, criteria, evaluator)
        scores.append(score)

    # ç•°å¸¸å€¤ã‚’é™¤å¤–ã—ã¦å¹³å‡
    return {
        "mean_score": mean(scores),
        "std_dev": stdev(scores),
        "inter_model_agreement": calculate_agreement(scores)
    }
```

## ä¸»è¦è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯7ç¨®ã®å¾¹åº•æ¯”è¼ƒ

### RAGã‚·ã‚¹ãƒ†ãƒ ç‰¹åŒ–: RAGAS

**ç‰¹å¾´**: RAGï¼ˆRetrieval Augmented Generationï¼‰å°‚ç”¨ã®è©•ä¾¡æŒ‡æ¨™ã‚’æä¾›

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall

# RAGå›ºæœ‰æŒ‡æ¨™ã§ã®è©•ä¾¡
result = evaluate(
    dataset,
    metrics=[
        faithfulness,         # æ¤œç´¢ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã®ä¸€è‡´åº¦
        answer_relevancy,     # è³ªå•ã¸ã®é–¢é€£æ€§
        context_recall        # å¿…è¦æƒ…å ±ã®ç¶²ç¾…ç‡
    ]
)
```

**é©ç”¨ã‚·ãƒ¼ãƒ³**: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ãƒ™ãƒ¼ã‚¹ã®Q&Aã‚·ã‚¹ãƒ†ãƒ ã€æŠ€è¡“ã‚µãƒãƒ¼ãƒˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ

### AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡: Inspect AI

**ç‰¹å¾´**: ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹

```python
from inspect_ai import Task, eval

@task
def agent_evaluation():
    return Task(
        dataset=test_cases,
        plan=[
            tool_correctness(),    # ãƒ„ãƒ¼ãƒ«é¸æŠã®é©åˆ‡æ€§
            task_completion(),     # ã‚¿ã‚¹ã‚¯é”æˆç‡
            step_efficiency()      # å†—é•·ãªæ“ä½œã®æ¤œå‡º
        ],
        scorer=g_eval()
    )

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¨è«–ãƒã‚§ãƒ¼ãƒ³ã‚’å¯è¦–åŒ–
results = eval(agent_evaluation(), log_level="debug")
```

**é©ç”¨ã‚·ãƒ¼ãƒ³**: è‡ªå¾‹å‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ãƒãƒ«ãƒãƒ„ãƒ¼ãƒ«çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–: Promptfoo

**ç‰¹å¾´**: A/Bãƒ†ã‚¹ãƒˆ + CLIå„ªå…ˆè¨­è¨ˆ

```yaml
# promptfoo.yaml
prompts:
  - "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚{{question}}"
  - "å°‚é–€å®¶ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚{{question}}"

providers:
  - openai:gpt-4o
  - anthropic:claude-3-5-sonnet-20241022

tests:
  - vars:
      question: "Pythonã®å‹ãƒ’ãƒ³ãƒˆã«ã¤ã„ã¦æ•™ãˆã¦"
    assert:
      - type: llm-rubric
        value: "æ­£ç¢ºæ€§ãŒ4ç‚¹ä»¥ä¸Š"
```

**é©ç”¨ã‚·ãƒ¼ãƒ³**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“

### æœ¬ç•ªç›£è¦–: Langfuse

**ç‰¹å¾´**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°

```python
from langfuse import Langfuse

langfuse = Langfuse()

# ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã§æœ¬ç•ªç’°å¢ƒç›£è¦–
trace = langfuse.trace(name="user_query")
generation = trace.generation(
    name="llm_response",
    input=user_query,
    output=llm_response,
    metadata={"model": "gpt-4o", "cost": 0.003}
)

# è‡ªå‹•ã§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«é›†è¨ˆ
```

**é©ç”¨ã‚·ãƒ¼ãƒ³**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã®ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€ã‚³ã‚¹ãƒˆç®¡ç†

### åŒ…æ‹¬çš„è©•ä¾¡: DeepEval

**ç‰¹å¾´**: ç ”ç©¶ãƒ™ãƒ¼ã‚¹ã®ä¿¡é ¼æ€§æŒ‡æ¨™ + ã‚·ãƒ³ãƒ—ãƒ«API

```python
from deepeval.metrics import HallucinationMetric, ToxicityMetric
from deepeval.test_case import LLMTestCase

test_case = LLMTestCase(
    input="æœ€æ–°ã®COVID-19æ²»ç™‚æ³•ã¯ï¼Ÿ",
    actual_output=llm_response,
    retrieval_context=["CDCå…¬å¼ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³...", "WHOæ¨å¥¨äº‹é …..."]
)

# ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡º
hallucination_metric = HallucinationMetric(threshold=0.9)
assert hallucination_metric.measure(test_case) >= 0.9
```

**é©ç”¨ã‚·ãƒ¼ãƒ³**: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã€åŒ…æ‹¬çš„å“è³ªä¿è¨¼

## ã€Œ5 Metric Ruleã€: åŠ¹ç‡çš„ãªè©•ä¾¡æŒ‡æ¨™è¨­è¨ˆ

**åŸå‰‡**: 1-2å€‹ã®ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ + 2-3å€‹ã®ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰æŒ‡æ¨™

### ã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰æŒ‡æ¨™ã®é¸æŠ

| ã‚·ã‚¹ãƒ†ãƒ ã‚¿ã‚¤ãƒ— | æ¨å¥¨æŒ‡æ¨™ |
|---------------|---------|
| RAGã‚·ã‚¹ãƒ†ãƒ  | Faithfulness, Answer Relevancy, Context Precision |
| AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | Task Completion, Tool Correctness, Step Efficiency |
| ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ | Conversation Coherence, User Satisfaction, Toxicity |
| ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ | Syntax Correctness, Test Pass Rate, Code Quality |

### ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ã®è¨­è¨ˆ

```python
from deepeval.metrics import GEval

# ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®è©•ä¾¡æŒ‡æ¨™
brand_voice_metric = GEval(
    name="Brand Voice Consistency",
    criteria="ä¼šç¤¾ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æ²¿ã£ãŸè¡¨ç¾ã‹",
    evaluation_steps=[
        "æ•¬èªã®ä½¿ç”¨ãŒé©åˆ‡ã‹",
        "å°‚é–€ç”¨èªã®èª¬æ˜ãŒå¹³æ˜“ã‹",
        "ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒˆãƒ¼ãƒ³ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã‹"
    ],
    threshold=0.8
)

domain_expertise_metric = GEval(
    name="Medical Accuracy",
    criteria="åŒ»å­¦çš„ã«æ­£ç¢ºã§æœ€æ–°ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã«æº–æ‹ ã—ã¦ã„ã‚‹ã‹",
    evaluation_steps=[
        "è¨ºæ–­åŸºæº–ãŒæœ€æ–°ã®ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹",
        "ç¦å¿Œãƒ»å‰¯ä½œç”¨ã®èª¬æ˜ãŒæ¼ã‚Œã¦ã„ãªã„ã‹",
        "æ‚£è€…å®‰å…¨ã«é…æ…®ã—ãŸè¡¨ç¾ã‹"
    ],
    threshold=0.95  # åŒ»ç™‚ã¯é«˜ã„åŸºæº–
)
```

**æ³¨æ„ç‚¹:**
> 5å€‹ä»¥ä¸Šã®æŒ‡æ¨™ã¯é–‹ç™ºé€Ÿåº¦ã‚’ä½ä¸‹ã•ã›ã¾ã™ã€‚åˆæœŸã¯3å€‹ã‹ã‚‰å§‹ã‚ã€æœ¬ç•ªé‹ç”¨ã®ä¸­ã§çœŸã«å¿…è¦ãªæŒ‡æ¨™ã®ã¿è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

## æ—¥æœ¬èªLLMè©•ä¾¡ã®å®Ÿè·µï¼šNejumi Leaderboardäº‹ä¾‹

### è©•ä¾¡ç’°å¢ƒ

W&B Tokyoï¼ˆWeights & Biases Japanï¼‰ãŒé‹å–¶ã™ã‚‹ã€ŒNejumi LLM Leaderboardã€ã¯ã€**45ä»¥ä¸Šã®æ—¥æœ¬èªLLMãƒ¢ãƒ‡ãƒ«**ã‚’è©•ä¾¡ã™ã‚‹å›½å†…æœ€å¤§ç´šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã™ã€‚

**è©•ä¾¡ã‚¿ã‚¹ã‚¯:**
- è¨€èªç†è§£ï¼ˆJNLIã€JCommonsenseQAç­‰ï¼‰
- ç”Ÿæˆå“è³ªï¼ˆæ—¥æœ¬èªè¦ç´„ã€å¯¾è©±ç”Ÿæˆï¼‰
- å°‚é–€çŸ¥è­˜ï¼ˆæ³•å¾‹ã€åŒ»ç™‚ã€æŠ€è¡“åˆ†é‡ï¼‰

### è‡ªå‹•è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
# llm-jp-evalã‚’ä½¿ã£ãŸè‡ªå‹•è©•ä¾¡
from llm_jp_eval import evaluate_model

results = evaluate_model(
    model_name="rinna/japanese-gpt-neox-3.6b",
    tasks=["jnli", "jcommonsenseqa", "jsquad"],
    device="cuda",
    batch_size=8
)

# HuggingFace Inference Endpointsã§è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤
print(f"Overall Score: {results['overall']:.2f}")
print(f"Language Understanding: {results['jnli']:.2f}")
print(f"Generation Quality: {results['jsquad']:.2f}")
```

**æˆæœæ•°å­—:**
- è©•ä¾¡è‡ªå‹•åŒ–ã«ã‚ˆã‚Šã€æ–°ãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨¼æ™‚é–“ã‚’**2é€±é–“â†’3æ™‚é–“**ã«çŸ­ç¸®
- Microsoft for Startupså°‚ç”¨GPUã‚¯ãƒ©ã‚¹ã‚¿æ´»ç”¨ã§ã‚³ã‚¹ãƒˆ60%å‰Šæ¸›
- å…¬é–‹ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£è²¢çŒ®

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| è©•ä¾¡ãŒå®‰å®šã—ãªã„ | Position Biasã€æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | è¤‡æ•°å›è©•ä¾¡+ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã€temperature=0å›ºå®š |
| ã‚³ã‚¹ãƒˆãŒäºˆæƒ³ä»¥ä¸Š | è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã™ãã‚‹ | è©•ä¾¡åŸºæº–ã‚’ç°¡æ½”åŒ–ã€ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ´»ç”¨ |
| ãƒ’ãƒˆè©•ä¾¡ã¨ä¹–é›¢ | è©•ä¾¡åŸºæº–ãŒæ›–æ˜§ | ãƒ«ãƒ¼ãƒ–ãƒªãƒƒã‚¯ç²¾ç·»åŒ–ã€ã‚µãƒ³ãƒ—ãƒ«å›ç­”æç¤º |
| è©•ä¾¡æ™‚é–“ãŒé•·ã„ | ç›´åˆ—å®Ÿè¡Œã€åŒæœŸAPI | ä¸¦åˆ—å‡¦ç†ã€ãƒãƒƒãƒAPIæ´»ç”¨ |
| ãƒã‚¤ã‚¢ã‚¹ãŒæ”¹å–„ã—ãªã„ | å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡ | ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã€å¤–ã‚Œå€¤é™¤å» |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**
- LLM-as-a-Judgeã¯äººé–“è©•ä¾¡ã¨85%ä¸€è‡´ã—ã€ã‚³ã‚¹ãƒˆã‚’500-5000å€å‰Šæ¸›
- 4ã¤ã®ãƒã‚¤ã‚¢ã‚¹ï¼ˆPosition/Length/Concreteness/Self-Enhancementï¼‰ã¸ã®å¯¾ç­–ãŒå¿…é ˆ
- ã€Œ5 Metric Ruleã€ã§é–‹ç™ºé€Ÿåº¦ã‚’ç¶­æŒã—ã¤ã¤å“è³ªä¿è¨¼
- RAG/Agent/Chatbotç­‰ã€ã‚·ã‚¹ãƒ†ãƒ ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é¸æŠãŒé‡è¦

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**
- DeepEval/RAGASã‚’CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆã—ã€è‡ªå‹•ãƒ†ã‚¹ãƒˆç’°å¢ƒæ§‹ç¯‰
- è‡ªç¤¾ãƒ‰ãƒ¡ã‚¤ãƒ³å‘ã‘ã‚«ã‚¹ã‚¿ãƒ æŒ‡æ¨™ã‚’G-Evalã§è¨­è¨ˆ
- Langfuse/Phoenixã§æœ¬ç•ªç’°å¢ƒã®ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°é–‹å§‹

## å‚è€ƒ

- [Large Language Model Evaluation in '26: 10+ Metrics & Methods](https://research.aimultiple.com/large-language-model-evaluation/)
- [LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide](https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation)
- [The LLM Evaluation Landscape with Frameworks in 2026](https://research.aimultiple.com/llm-eval-tools/)
- [LLMã«ã‚ˆã‚‹LLMã®è©•ä¾¡ã€ŒLLM-as-a-Judgeã€å…¥é–€ã€œåŸºç¤ã‹ã‚‰é‹ç”¨ã¾ã§å¾¹åº•è§£èª¬](https://zenn.dev/pharmax/articles/2d07bf0498e212)
- [LLM-as-a-Judgeã¨ã¯ï¼Ÿä¿¡é ¼ã§ãã‚‹AIè©•ä¾¡ã®ä½œã‚Šæ–¹ã¨é™ç•Œã€2025å¹´ç‰ˆã€‘](https://www.openbridge.jp/column/llm-judge-2025)
- [Introducing the Open Leaderboard for Japanese LLMs!](https://huggingface.co/blog/leaderboard-japanese)
- [Weights & Biases Japan accelerates LLM benchmarks with MfS dedicated GPU cluster](https://www.microsoft.com/en-us/startups/blog/weights-biases-japan-accelerates-llm-benchmarks-mfs-dedicated-gpu-cluster/)

è©³ç´°ãªãƒªã‚µãƒ¼ãƒå†…å®¹ã¯ [Issue #57](https://github.com/0h-n0/zen-auto-create-article/issues/57) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
