---
title: "LLMフォールバックチェーン設計：3層パターンで高可用性を実現する"
emoji: "🔗"
type: "tech"
topics: ["llm", "python", "litellm", "reliability", "ai"]
published: false
---

# LLMフォールバックチェーン設計：3層パターンで高可用性を実現する

## この記事でわかること

- LLMアプリの**3層フォールバック設計**（リトライ→フォールバック→サーキットブレーカー）の使い分け
- LiteLLMを使った**マルチプロバイダフォールバックチェーン**のPython実装
- 本番運用で**高可用性**を達成するための設計パターンと監視手法

## 対象読者

- **想定読者**: LLM APIを本番利用している中級〜上級のPython開発者
- **必要な前提知識**: Python 3.11+、OpenAI API互換の呼び出し経験、HTTPステータスコードの基本理解

## 結論・成果

**リトライ・フォールバック・サーキットブレーカーの3層を組み合わせることで、LLMアプリのダウンタイムを大幅に削減できます。** 単一プロバイダの稼働率（一般的に99.5〜99.9%）に対し、3層チェーンで冗長化することで99.9%以上の可用性を目指せます。LiteLLMなら**実装工数1〜2日**で本番導入可能です（2026年2月時点、LiteLLM v1.81）。

## 3層アーキテクチャとLiteLLM実装

LLMアプリの信頼性設計は、**障害の種類に応じて3つの防御層を段階的にエスカレーションする**ことが鍵です。

### 各層の役割

| 層 | パターン | 対象障害 | 発動条件 |
|---|---|---|---|
| 第1層 | リトライ（指数バックオフ） | 一時的障害（ネットワーク不安定） | HTTP 429, 503 |
| 第2層 | フォールバック（プロバイダ切替） | プロバイダ障害（API停止） | リトライ上限到達 |
| 第3層 | サーキットブレーカー（遮断） | 持続的障害（大規模障害） | 失敗率閾値超過 |

**なぜこの順番か：** 一時障害にプロバイダ切替すると不要なコスト増が発生します。逆に持続的障害へのリトライ継続は**リトライストーム**（障害プロバイダへのリクエスト集中で復旧遅延）を引き起こします。

### Pythonでの基本実装

```python
# fallback_chain.py
import litellm
from litellm import completion

def call_with_fallback(messages: list[dict]) -> str:
    """3プロバイダのフォールバックチェーンでLLM呼び出し"""
    fallback_models = [
        "anthropic/claude-sonnet-4-6",
        "openai/gpt-4.1",
        "gemini/gemini-3-pro-preview",
    ]
    last_error = None
    for model in fallback_models:
        try:
            response = completion(
                model=model, messages=messages,
                timeout=30, num_retries=2,  # 第1層: リトライ
            )
            return response.choices[0].message.content
        except (litellm.RateLimitError, litellm.Timeout,
                litellm.ServiceUnavailableError) as e:
            last_error = e
            continue  # 第2層: 次のプロバイダへ
    raise RuntimeError(f"全プロバイダが失敗: {last_error}")
```

`completion()`の`num_retries`で第1層、`for`ループで第2層が動作するシンプルなチェーンです。

### LiteLLM Proxyでの本番構成

本番ではYAML設定で宣言的に管理します。

```yaml
# litellm_config.yaml
model_list:
  - model_name: "main-model"
    litellm_params:
      model: "anthropic/claude-sonnet-4-6"
      api_key: "os.environ/ANTHROPIC_API_KEY"
  - model_name: "fallback-gpt"
    litellm_params:
      model: "openai/gpt-4.1"
      api_key: "os.environ/OPENAI_API_KEY"

litellm_settings:
  num_retries: 3
  request_timeout: 30
  allowed_fails: 3      # 3回失敗でクールダウン（第3層）
  cooldown_time: 60      # 60秒間プロバイダ除外
  fallbacks:
    - main-model: ["fallback-gpt"]
```

**この設定だけでリトライ→フォールバック→クールダウンの3層が動作します。** `allowed_fails`と`cooldown_time`がLiteLLMの簡易サーキットブレーカーです。

> **制約条件:** LiteLLMのクールダウンは「N回失敗で一定時間除外」の単純ロジックです。半開状態（Half-Open）での段階的復旧が必要な場合は、`pybreaker`ライブラリとの併用を検討してください。

## フォールバック順序の設計戦略と監視

フォールバックチェーンの**モデル順序**はユースケースで切り替えます。

| 戦略 | 順序例 | 適用シーン |
|---|---|---|
| 品質優先 | Claude Sonnet 4.6 → GPT-4.1 → Gemini Flash | 医療・法務・金融 |
| コスト優先 | Gemini Flash → GPT-4.1 mini → Claude Haiku 4.5 | バッチ処理・要約 |
| レイテンシ優先 | Gemini Flash → Claude Haiku 4.5 → GPT-4.1 | リアルタイムチャット |

> **よくある間違い:** フォールバック順序を全タスク共通で固定しがちですが、**要約はコスト優先、コード生成は品質優先**のように使い分けることで、コストと品質のバランスが大幅に改善します。

### 監視で見落としがちなポイント

フォールバックチェーンを「設定して終わり」にすると、**サイレント品質劣化**に気づけません。フォールバック先の低品質モデルが常時使われている状態です。

監視すべき主要メトリクス:

- **フォールバック発動率**: 5%超過で警告、10%超過で緊急アラート
- **サーキットブレーカー開放回数**: 3回/時間超過で緊急
- **プロバイダ別エラー率**: 1%超過でプラン見直し検討

**ハマりポイント:** フォールバック発動率が常時5%超の場合、プライマリプロバイダのレート制限不足が原因の可能性があります。フォールバックで対処するのではなく、**プランのアップグレードやリクエスト分散**を先に検討してください。

## まとめと次のステップ

**まとめ:**

- LLMフォールバックチェーンは**3層（リトライ→フォールバック→サーキットブレーカー）** で設計する
- LiteLLMのYAML設定で3層すべてを宣言的に構成でき、**実装工数1〜2日**で導入可能
- フォールバック順序は**タスク種別ごとに切り替え**が効果的
- **フォールバック発動率の監視**を怠るとサイレント品質劣化に気づけない

**次にやるべきこと:**

- LiteLLM Proxyを`docker compose`で起動し、フォールバックチェーンを構成する
- フォールバック発動率のダッシュボードを作成し、5%閾値のアラートを設定する

**関連記事（公開後にリンクが有効になります）:**

- GeminiとClaudeを使い分けるマルチLLMルーティング実装ガイド（準備中）
- AIエージェントのエラーハンドリング実践ガイド（準備中）

## 関連する深掘り記事

この記事に関連する1次情報（論文・テックブログ）の深掘り記事です。

- [論文解説: Reliability Frameworks for Production LLM Systems](https://0h-n0.github.io/posts/paper-2505-02097/) — LLM障害分類体系と6層フォールバック階層の設計論
- [AWS解説: Multi-Provider Generative AI Gateway](https://0h-n0.github.io/posts/techblog-aws-multi-provider-gateway/) — LiteLLM on ECS Fargateによるマルチリージョンフェイルオーバー
- [論文解説: Strategic and Selective Mixtures - LLM Routing and Cascades](https://0h-n0.github.io/posts/paper-2503-04625/) — ルーティング vs カスケードのコスト効率比較
- [論文解説: Agentic AI in Financial Services](https://0h-n0.github.io/posts/paper-2501-01144/) — 金融12機関のインシデント分析とエラーバウンダリパターン
- [論文解説: RouteLLM - Learning to Route LLMs with Preference Data](https://0h-n0.github.io/posts/paper-2403-12031/) — 因果推論フレームワークによるGPT-4コスト40-52%削減

## 参考

- [Portkey: Retries, fallbacks, and circuit breakers in LLM apps](https://portkey.ai/blog/retries-fallbacks-and-circuit-breakers-in-llm-apps/)
- [LiteLLM: Model Fallbacks Documentation](https://docs.litellm.ai/docs/tutorials/model_fallbacks)
- [LiteLLM: Proxy Reliability Configuration](https://docs.litellm.ai/docs/proxy/reliability)
- [Portkey: Failover routing strategies for LLMs in production](https://portkey.ai/blog/failover-routing-strategies-for-llms-in-production/)
- [pybreaker: Python Circuit Breaker pattern](https://github.com/danielfm/pybreaker)

詳細なリサーチ内容は [Issue #186](https://github.com/0h-n0/zen-auto-create-article/issues/186) を参照してください。

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
