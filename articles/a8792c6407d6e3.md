---
title: "2026年版 フロンティアLLM学習パイプライン完全解説：事前学習からRLまで"
emoji: "🏋️"
type: "tech"
topics: ["llm", "machinelearning", "deeplearning", "ai"]
published: false
---

# 2026年版 フロンティアLLM学習パイプライン完全解説：事前学習からRLまで

## この記事でわかること

- SmolLM3・Kimi K2・DeepSeek R1など最新オープンウェイトモデルの**学習パイプライン全体像**
- GQA・MoE・MLAなどアーキテクチャ選定の判断基準と実測データ
- Muonオプティマイザが AdamW比**約2倍の計算効率**を達成した仕組み
- GRPO・DPO・RLVRなどPost-training手法の使い分け
- マルチステージ事前学習における**データ品質スケジューリング**の実践知見

## 対象読者

- **想定読者**: 中級〜上級のMLエンジニア・リサーチャー
- **必要な前提知識**:
  - Transformerアーキテクチャの基本理解
  - PyTorchでのモデル学習経験
  - Attention機構（Self-Attention, Multi-Head Attention）の基礎

## 結論・成果

2026年現在、フロンティアLLMの学習は**3段階パイプライン**（事前学習 → 中間学習 → Post-training）に標準化されています。Muonオプティマイザの導入で計算効率が約2倍に、MoEアーキテクチャの高スパース化で推論コストが60%以上削減されました。本記事では、7つの最新オープンウェイトモデルのテクニカルレポートを横断比較し、実践で再現可能な学習プレイブックを整理します。

## アーキテクチャ選定を理解する

フロンティアモデルのアーキテクチャは、2026年時点で大きく**Dense型**と**MoE（Mixture of Experts）型**に分かれます。まず主要モデルの構成を確認してみましょう。

### 主要モデルのアーキテクチャ比較

| モデル | パラメータ | 注意機構 | タイプ | オプティマイザ |
|--------|-----------|---------|--------|--------------|
| Kimi K2 | 1.06T（活性32B） | MLA | MoE（384専門家） | Muon |
| gpt-oss-120b | 116.83B | GQA(8) | MoE | AdamW |
| DeepSeek R1 | 671B（活性37B） | MLA | MoE（256専門家） | AdamW |
| SmolLM3 | 3B | GQA(4) | Dense | AdamW |
| OLMo 3 | 32B | GQA | Dense | AdamW |

**GQA+RoPEがデフォルトの理由:** GQAはMHA比でKVキャッシュを最大8倍削減、RoPEはコンテキスト長拡張が容易です。DeepSeekとKimi K2は**MLA**を採用しKV圧縮をさらに進めていますが、実装が複雑なためDenseモデルではGQA(4/8)が安定した選択です。

### MoEの設計ポイント

MoEモデルでは**ロードバランシング**が安定性の鍵です。従来の補助損失（auxiliary loss）に代わり、DeepSeek V3は**補助損失なし**のバランシング手法を導入しました。

```python
# MoEルーティングの基本構造（概念コード）
import torch, torch.nn as nn

class MoERouter(nn.Module):
    def __init__(self, hidden_dim: int, num_experts: int, top_k: int = 2):
        super().__init__()
        self.gate = nn.Linear(hidden_dim, num_experts, bias=False)
        self.top_k = top_k

    def forward(self, x: torch.Tensor):
        logits = self.gate(x)  # (batch, seq, num_experts)
        scores, indices = torch.topk(logits, self.top_k, dim=-1)
        return torch.softmax(scores, dim=-1), indices
```

Kimi K2は384専門家でTop-8を採用し、スケーリング則分析で**スパース度増加がモデル品質に大きく寄与**することを確認しています。

## 事前学習の実践：データとオプティマイザ

事前学習フェーズでは、**データ品質スケジューリング**と**オプティマイザ選定**が最も結果に影響します。

### マルチステージ事前学習

SmolLM3（3Bパラメータ）の事前学習は3段階で構成されています。

| ステージ | トークン数 | コンテキスト長 | データ品質 |
|---------|-----------|-------------|----------|
| Stage 1 | 8T | 4K | 標準（ウェブクロール中心） |
| Stage 2 | 2T | 4K | 高品質注入（教科書・Q&A） |
| Stage 3 | 1.1T | 4K | 推論・QA特化 |

**データミックス（SmolLM3 Stage 1）:**
- 英語ウェブ: 75%
- 多言語ウェブ: 12%
- コード: 10%
- 数学: 3%

**最初はステージ1のミックスでよいと思っていましたが、実際にはステージ後半で高品質データを集中投入するスケジューリングが品質に決定的な差を生みます。** SmolLM3のアブレーション実験では、高品質データを均等配分した場合と後半集中で配分した場合で、下流タスク精度に5〜8%の差が出ています。

### Muonオプティマイザの台頭

2026年の最大の変化は**Muonオプティマイザ**の実用化です。Kimi K2で大規模に採用され、AdamW比で**約2倍の計算効率**を実証しました。

```python
# Muonオプティマイザの更新ステップ（擬似コード）
# 1. 勾配計算
g_t = compute_gradient(loss, params)

# 2. モメンタム更新
momentum = mu * momentum_prev + g_t

# 3. Newton-Schulz直交化（5回反復）
# 行列レベルで更新方向を直交化し、軸方向バイアスを除去
orthogonal_update = newton_schulz_5(momentum)

# 4. パラメータ更新
params = params - lr * orthogonal_update
```

**Muonの核心は「行列直交化」です。** AdamWがパラメータごとに適応的学習率を計算するのに対し、Muonは行列全体を直交化して更新方向の偏りを排除します。

**トレードオフ:** 全対全通信が必要でネットワーク帯域がボトルネックになり得る点、AdamW事前学習チェックポイントからの継続学習で性能低下が報告されている点に注意が必要です。Embedding層とHead層にはAdamWを併用するのが一般的です。

学習率はKimi K2の例で**WSD（Warmup-Stable-Decay）** を採用：500ステップのウォームアップ → 10Tトークンで2e-4定数 → 5.5Tでコサイン減衰（2e-5まで）。安定期を長く取ることで中間チェックポイントの品質が安定します。

## Post-trainingを設計する

事前学習済みモデルを実用レベルに仕上げるPost-trainingは、**SFT → 選好最適化 → RL**の3段階が標準です。

### SFTとマスク付き損失

SFTでは**マスク付き損失**（アシスタント応答部分のみで損失計算）が標準です。推論トークン（CoT）と直接回答の**バッチ内比率管理が重要**で、推論トークン過多は短い回答の品質を低下させます。

### GRPO：RLの新標準

DeepSeek R1で実証された**GRPO（Group Relative Policy Optimization）** は、PPOのような価値関数モデルを不要にし、メモリ効率を大幅に改善しました。

```python
# GRPOの基本フロー（概念実装）
def grpo_step(policy_model, prompt: str, group_size: int = 8, reward_fn=None):
    # 1. 同一プロンプトからK個の応答を生成
    responses = [policy_model.generate(prompt) for _ in range(group_size)]
    # 2. 各応答をスコアリング → グループ平均を基準値に
    rewards = [reward_fn(prompt, r) for r in responses]
    baseline = sum(rewards) / len(rewards)
    # 3. 基準値との差分でポリシー更新（価値関数モデル不要）
    advantages = [r - baseline for r in rewards]
    update_policy(policy_model, responses, advantages)
```

**GRPOの利点:** 価値関数不要でGPUメモリ約50%削減、グループ内相対比較で安定学習、RLVR（検証可能報酬）との組み合わせで数学・コーディングに特に有効です。

**制約条件:** GRPOは**検証可能な報酬**がある領域（数学の正誤判定、コードのテスト通過）で最も効果的です。自由記述やクリエイティブタスクでは、DPOやKTOのような選好ベースの手法が依然として有効です。

## 学習安定化のテクニック集

大規模学習では**損失スパイク**が頻発します。主要な安定化手法を整理します。

| 手法 | 採用モデル | 注意点 |
|------|----------|--------|
| ロジットソフトキャッピング | Gemma 3, SmolLM3 | z損失より安定、推奨 |
| QK-norm | GPT-oss, OLMo 3 | 128K以上で性能低下の報告あり |
| RMSNorm | ほぼ全モデル | LayerNormの上位互換 |
| 重み減衰（Embedding除外） | Kimi K2, DeepSeek | 標準プラクティス |

**ハマりポイント:** QK-normは4K以下のコンテキストでは有効ですが、**長コンテキストでは逆効果**の報告があります。ロジットソフトキャッピング（`soft_cap * tanh(logits / soft_cap)`、Gemma方式）が最も安定です。

## まとめと次のステップ

**まとめ:**
- **アーキテクチャ**: Dense+GQA+RoPEがデフォルト。大規模モデルにはMoE+MLAが有効
- **オプティマイザ**: MuonがAdamW比2倍の効率を実現。ただしEmbedding/HeadにはAdamWを併用
- **データ**: マルチステージ学習で高品質データを後半に配置するスケジューリングが品質を決定
- **Post-training**: SFT → DPO/GRPO → RLVRの3段階が標準化
- **安定化**: ロジットソフトキャッピング（Gemma方式）が最も信頼性が高い

**次にやるべきこと:**
- [SmolLM3テクニカルレポート](https://huggingface.co/blog/smollm3)を読み、3Bモデルの学習プレイブックを確認する
- [Muonオプティマイザの論文](https://arxiv.org/abs/2502.16982)でNewton-Schulz直交化の詳細を理解する
- 小規模アブレーション（100Bトークン程度）で自身のデータミックスを検証する

## 参考

- [Frontier Model Training Methodologies](https://djdumpling.github.io/2026/01/31/frontier_training.html)
- [Muon is Scalable for LLM Training](https://arxiv.org/abs/2502.16982)
- [New LLM Pre-training and Post-training Paradigms](https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training)
- [GRPO解説](https://cameronrwolfe.substack.com/p/grpo)
- [Kimi K2 Technical Report](https://arxiv.org/pdf/2507.20534)

詳細なリサーチ内容は [Issue #174](https://github.com/0h-n0/zen-auto-create-article/issues/174) を参照してください。

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
