---
title: "G-EvalでLLM評価パイプラインを構築しCI/CDに組み込む実践ガイド"
emoji: "📊"
type: "tech"
topics: ["llm", "deepeval", "python", "cicd", "machinelearning"]
published: false
---

# G-EvalでLLM評価パイプラインを構築しCI/CDに組み込む実践ガイド

## この記事でわかること

- G-Evalの仕組み（CoT評価ステップ生成・フォーム充填型判定・確率重み付けスコアリング）の理解
- DeepEval v3.7を使ったG-Evalメトリクスの実装方法（5つのユースケース別）
- LLMアプリケーション向け評価パイプラインの設計と構築手順
- pytest統合によるCI/CDパイプラインへの組み込み方法
- 評価基準のカスタマイズと運用時のチューニング手法

## 対象読者

- **想定読者**: 中級者のPython・LLMアプリケーション開発者
- **必要な前提知識**:
  - Python 3.9以上の基本的な使い方
  - LLM API（OpenAI、Anthropic等）の呼び出し経験
  - pytestの基礎知識
  - CI/CD（GitHub Actions等）の基本概念

## 結論・成果

G-Evalを活用した評価パイプラインを構築することで、LLM出力の品質評価を自動化できます。G-Evalは原論文（Liu et al., EMNLP 2023）のベンチマークで、要約タスクにおいてSpearman相関0.514を達成し、従来のBLEUやROUGEといった参照ベースメトリクスを上回る人間判断との一致度が報告されています。DeepEval v3.7のGEvalMetricを使えば、カスタム評価基準を自然言語で定義し、5行のコードで評価を開始できます。さらにpytest統合により、PRごとにLLM出力の品質回帰テストを自動実行するCI/CDパイプラインを構築できます。

## G-Evalの仕組みを理解する

G-Evalは、LLMを評価者（Judge）として活用するフレームワークです。従来のBLEUやROUGEが参照テキストとの表層的な一致度を測定するのに対し、G-Evalは**Chain-of-Thought（CoT）推論**を用いてLLM出力の品質を多角的に評価します。

### G-Evalの3ステップアーキテクチャ

G-Evalは以下の3つのフェーズで動作します。

**フェーズ1: CoT評価ステップの自動生成**

ユーザーが定義した「タスク紹介」と「評価基準」をLLMに入力し、詳細な「評価ステップ」をCoTプロンプティングで自動生成します。たとえば「回答の正確性を評価する」という基準から、「事実の矛盾がないか確認する」「重要な詳細の省略を検出する」といった具体的な評価手順が生成されます。

**フェーズ2: フォーム充填型判定**

生成された評価ステップと元のプロンプトを組み合わせ、LLMがNLG出力を「フォーム充填」パターンで評価します。各評価観点について1〜5のスコアを付与します。

**フェーズ3: 確率重み付けスコアリング**

LLMが出力するスコアトークンの確率を利用して、重み付き合算を行い最終スコアを算出します。LLMは通常、整数スコアしか出力しないため、微妙な品質差を捉えにくいという課題があります。トークンの出力確率を活用することで、連続的できめ細かいスコアを生成できます。

```
[ユーザー定義]          [LLM (CoT)]           [LLM (Judge)]          [スコア計算]
タスク紹介    ─┐
               ├──→ 評価ステップ生成 ──→ フォーム充填型評価 ──→ 確率重み付け
評価基準      ─┘                              ↑                      ↓
                                          LLM出力            最終G-Evalスコア
```

**なぜこのアプローチを選んだか:**

- CoTによる評価ステップの自動生成により、評価者間の一貫性を担保できる
- フォーム充填パターンにより、構造化されたスコアリングが可能になる
- 確率重み付けにより、整数スコアの粗さを解消し、連続的なスコアを実現できる

> **注意点:** G-Evalは非決定論的なメトリクスです。同じ入力に対して実行ごとにスコアが微妙に異なる場合があります。決定論的な評価が必要な場合は、DeepEvalの`DAGMetric`など別のアプローチを検討してください。

### 従来手法との違い

G-Evalが従来のNLG評価手法と異なる点を整理します。

| 評価手法 | 参照テキスト | 評価方法 | 人間との相関（要約タスク） |
|---------|-------------|---------|------------------------|
| BLEU | 必要 | n-gram一致度 | 低い |
| ROUGE | 必要 | n-gram再現率 | 低い |
| BERTScore | 必要 | 埋め込み類似度 | 中程度 |
| G-Eval | 不要（参照なしも可） | LLM-as-Judge + CoT | Spearman 0.514（論文報告値） |

BLEU・ROUGEは表層的な単語一致に依存するため、言い換えや創造的な表現を正しく評価できません。G-Evalはこの課題に対して、LLMの言語理解能力を活用した参照なし評価を可能にします。

**制約条件:** G-Evalのスコアは評価に使用するLLMの能力に依存します。小規模なモデルでは評価精度が低下する傾向があり、論文ではGPT-4を使用した場合に最も高い相関が報告されています。また、評価対象のLLMと評価者LLMが同じ場合、自己バイアスが生じるリスクがあります。

## DeepEvalでG-Evalを実装する

DeepEvalはG-Evalを`GEval`クラスとして実装しているPythonのLLM評価フレームワークです。ここからは、DeepEval v3.7を使った具体的な実装手順を見ていきます。

### 環境構築

まずDeepEvalをインストールし、環境を準備します。

```bash
# DeepEvalのインストール
pip install deepeval

# OpenAI APIキーの設定（デフォルトのJudge LLMとして使用）
export OPENAI_API_KEY="sk-..."
```

DeepEvalはPython 3.9以上が必要です。Judge LLMにはデフォルトで`gpt-4.1`が使用されますが、Anthropic ClaudeやOllamaなど他のLLMに変更することも可能です。

### 基本的なG-Evalメトリクスの定義

G-Evalメトリクスを定義するために必要なパラメータは3つです。

```python
# eval_metrics.py
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

# 1. 回答正確性メトリクス（参照ベース）
correctness_metric = GEval(
    name="Correctness",
    criteria="Determine whether the actual output is factually correct based on the expected output.",
    evaluation_params=[
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.EXPECTED_OUTPUT,
    ],
    threshold=0.7,  # 0.7以上で合格
)

# テストケースの作成と評価
test_case = LLMTestCase(
    input="Pythonの型ヒントについて説明してください",
    actual_output="Pythonの型ヒントはPEP 484で導入され、変数や関数の引数・戻り値に型情報を付与する機能です。",
    expected_output="型ヒントはPEP 484（Python 3.5）で導入された機能で、静的型チェッカーやIDEの支援を受けるために使用されます。",
)

correctness_metric.measure(test_case)
print(f"Score: {correctness_metric.score}")   # 0.0〜1.0
print(f"Reason: {correctness_metric.reason}")  # 評価理由
```

**`criteria`** は自然言語で評価基準を記述します。G-Evalはこの基準からCoTで評価ステップを自動生成します。

**`evaluation_params`** は評価に使用するテストケースのフィールドを指定します。主要なパラメータは以下の通りです。

| パラメータ | 用途 | 使用場面 |
|-----------|------|---------|
| `ACTUAL_OUTPUT` | LLMの実際の出力 | 全メトリクス |
| `EXPECTED_OUTPUT` | 期待される出力 | 正確性評価 |
| `INPUT` | ユーザー入力 | 関連性評価 |
| `RETRIEVAL_CONTEXT` | 検索コンテキスト | RAG評価 |

### evaluation_stepsで評価の再現性を高める

`criteria`のみを指定した場合、G-EvalはCoTで評価ステップを自動生成しますが、実行ごとに異なるステップが生成される可能性があります。**`evaluation_steps`を明示的に定義することで、評価の一貫性を高められます。**

```python
# 明示的な評価ステップを定義
correctness_with_steps = GEval(
    name="Correctness",
    evaluation_steps=[
        "Check whether the actual output contradicts any facts in the expected output.",
        "Heavily penalize omission of important detail from the expected output.",
        "Vague language or differing opinions are acceptable.",
        "Check that technical terms are used accurately.",
    ],
    evaluation_params=[
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.EXPECTED_OUTPUT,
    ],
    threshold=0.7,
)
```

**なぜevaluation_stepsを明示するのか:**

- `criteria`のみの場合、CoTが毎回異なる評価ステップを生成するため、スコアのばらつきが大きくなる
- `evaluation_steps`を明示すると、CoTステップ生成（フェーズ1）をスキップし、常に同じ基準で評価する
- CI/CDでの回帰テストでは、一貫した評価基準が必要なため、`evaluation_steps`の明示を推奨

### 5つのユースケース別実装

G-Evalは`evaluation_params`を変えることで多様な評価に対応できます。代表的な5パターンを紹介します。

| ユースケース | evaluation_params | threshold目安 | 用途 |
|-------------|-------------------|--------------|------|
| 回答正確性 | `ACTUAL_OUTPUT`, `EXPECTED_OUTPUT` | 0.7 | 参照ベースのファクトチェック |
| 一貫性・明瞭さ | `ACTUAL_OUTPUT` | 0.6 | 参照なしの品質評価 |
| トーン・専門性 | `ACTUAL_OUTPUT` | 0.6 | 文体・トーンの統一性チェック |
| 安全性（PII） | `ACTUAL_OUTPUT` | 0.9 | 個人情報漏洩の検出 |
| RAG忠実性 | `ACTUAL_OUTPUT`, `RETRIEVAL_CONTEXT` | 0.7 | コンテキストとの整合性 |

たとえば安全性評価とRAG忠実性評価は以下のように定義します。

```python
# 安全性メトリクス（高い閾値を設定）
safety = GEval(
    name="PII Safety",
    evaluation_steps=[
        "Check the output for personally identifiable information.",
        "Detect any hallucinated PII or training data remnants.",
        "Verify that placeholder values are used instead of real data.",
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    threshold=0.9,
)

# RAG忠実性メトリクス
rag_faithfulness = GEval(
    name="RAG Faithfulness",
    evaluation_steps=[
        "Extract all factual claims from the actual output.",
        "Cross-reference each claim against the retrieval context.",
        "Penalize hallucinated information not present in the context.",
    ],
    evaluation_params=[
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.RETRIEVAL_CONTEXT,
    ],
    threshold=0.7,
)
```

**ハマりポイント:** `threshold`の設定には注意が必要です。最初は低め（0.5〜0.6）に設定し、実際の評価結果を見ながら段階的に引き上げることを推奨します。閾値を高く設定しすぎると、妥当な出力が不合格になりCI/CDが頻繁に失敗します。

## 評価パイプラインを設計する

個別のメトリクスを定義したら、次はLLMアプリケーション全体の評価パイプラインを構築します。ここでは、RAGアプリケーションを対象に、複数のG-Evalメトリクスを組み合わせた評価パイプラインの設計を解説します。

### パイプラインの全体設計

評価パイプラインは3層構造で設計します。

```
┌─────────────────────────────────────────────────┐
│ Layer 1: ゴールデンデータセット管理               │
│  - 入力/期待出力ペアの管理                        │
│  - テストケースのバージョン管理                    │
└──────────────────────┬──────────────────────────┘
                       ↓
┌─────────────────────────────────────────────────┐
│ Layer 2: LLMアプリケーション実行                  │
│  - テスト入力でLLMアプリを実行                    │
│  - actual_outputとretrieved_contextを取得         │
└──────────────────────┬──────────────────────────┘
                       ↓
┌─────────────────────────────────────────────────┐
│ Layer 3: G-Evalメトリクス評価                     │
│  - 複数メトリクスで多角的に評価                   │
│  - 合格/不合格の判定                              │
│  - 結果のレポーティング                           │
└─────────────────────────────────────────────────┘
```

### ゴールデンデータセットと評価パイプラインの実装

評価パイプラインの品質は、ゴールデンデータセット（正解データ）の品質に直結します。テストケースをJSONで管理し、`deepeval.evaluate`で一括評価する構成を取ります。

```python
# evaluation_pipeline.py
from deepeval import evaluate
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.dataset import EvaluationDataset

# メトリクス定義（前セクションで定義したものを再利用）
METRICS = [
    GEval(
        name="Correctness",
        evaluation_steps=[
            "Compare actual output against expected output for factual accuracy.",
            "Penalize factual contradictions and significant omissions.",
        ],
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.7,
    ),
    GEval(
        name="Coherence",
        evaluation_steps=[
            "Evaluate whether the response is logically structured.",
            "Identify any contradictions within the response.",
        ],
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
        threshold=0.6,
    ),
]


def run_evaluation(llm_app_fn, dataset: EvaluationDataset):
    """ゴールデンデータセットに対してLLMアプリを評価する"""
    test_cases = []
    for golden in dataset.goldens:
        actual_output = llm_app_fn(golden.input)
        test_case = LLMTestCase(
            input=golden.input,
            actual_output=actual_output,
            expected_output=golden.expected_output,
            retrieval_context=golden.context,
        )
        test_cases.append(test_case)
    return evaluate(test_cases=test_cases, metrics=METRICS)
```

ゴールデンデータセットはJSON形式で管理します。`input`、`expected_output`、`context`の3フィールドを含めます。

```json
[
    {
        "input": "Pythonのリスト内包表記とは何ですか？",
        "expected_output": "リスト内包表記は、既存のリストやイテラブルから新しいリストを簡潔に生成するPythonの構文です。",
        "context": ["Python公式ドキュメント: リスト内包表記はリストを生成するための簡潔な方法を提供します。"]
    }
]
```

**トレードオフ:** メトリクスを増やすほど評価の網羅性は高まりますが、LLM API呼び出し回数が増加しコストと実行時間が増えます。テストケース10件 × メトリクス3種 = 30回のLLM API呼び出しが発生します。

## pytest連携でCI/CDに組み込む

DeepEvalはpytestとネイティブに統合されており、LLM評価をユニットテストとして実行できます。これにより、既存のCI/CDパイプラインにLLM評価を追加できます。

### pytestテストファイルの作成

```python
# tests/test_llm_evaluation.py
import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from your_app import generate_response  # テスト対象のLLMアプリ

# メトリクス定義
correctness_metric = GEval(
    name="Correctness",
    evaluation_steps=[
        "Compare actual output against expected output for factual accuracy.",
        "Penalize factual contradictions and significant omissions.",
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
    threshold=0.7,
)

# ゴールデンデータセットの読み込み
dataset = EvaluationDataset()
dataset.add_goldens_from_json_file(
    file_path="tests/golden_dataset.json",
    input_key_name="input",
    expected_output_key_name="expected_output",
)

@pytest.mark.parametrize("golden", dataset.goldens, ids=[g.input[:50] for g in dataset.goldens])
def test_correctness(golden: Golden):
    """回答の正確性テスト"""
    actual_output = generate_response(golden.input)
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=actual_output,
        expected_output=golden.expected_output,
    )
    assert_test(test_case=test_case, metrics=[correctness_metric])
```

### テストの実行

```bash
# DeepEvalのテストランナーで実行（推奨）
deepeval test run tests/test_llm_evaluation.py

# 通常のpytestでも実行可能
pytest tests/test_llm_evaluation.py -v
```

`deepeval test run`コマンドを使用すると、テスト結果がConfident AIプラットフォームに自動送信され、時系列でのスコア推移を確認できます（オプション）。

### GitHub Actionsへの組み込み

PRごとにLLM評価を自動実行するGitHub Actionsワークフローを設定します。

```yaml
# .github/workflows/llm-eval.yml
name: LLM Evaluation Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  llm-eval:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install deepeval

      - name: Run LLM evaluation tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: deepeval test run tests/test_llm_evaluation.py
```

**よくある間違い:** GitHub ActionsのSecretsに`OPENAI_API_KEY`を設定し忘れるケースが多いです。設定手順は「リポジトリ Settings → Secrets and variables → Actions → New repository secret」です。

### 評価コストを管理する

CI/CDでのG-Eval実行はLLM API呼び出しを伴うため、コスト管理が重要です。

```python
# conftest.py - テスト実行時のコスト削減設定
import os
import pytest


def pytest_configure(config):
    """CI環境ではコスト削減モードを有効にする"""
    if os.getenv("CI"):
        # CI環境ではテストケース数を制限
        os.environ.setdefault("EVAL_MAX_SAMPLES", "10")


@pytest.fixture(scope="session")
def eval_config():
    """評価設定を返す"""
    max_samples = int(os.getenv("EVAL_MAX_SAMPLES", "50"))
    return {"max_samples": max_samples}
```

| 戦略 | 方法 | コスト削減効果 |
|------|------|--------------|
| サンプリング | テストケース数を制限 | テストケース数に比例 |
| キャッシュ | DeepEvalの結果キャッシュを活用 | 同一入力の再評価を回避 |
| モデル選択 | Judge LLMに低コストモデルを使用 | GPT-4.1 → GPT-4.1-mini でコスト削減 |
| 実行頻度 | mainブランチへのマージ時のみ実行 | PR数に比例 |

**トレードオフ:** Judge LLMを低コストモデルに変更するとコストは下がりますが、評価精度も低下する可能性があります。論文のベンチマークはGPT-4を使用しており、小規模モデルでの性能保証はありません。本番環境では、コストと精度のバランスを実データで検証することを推奨します。

## カスタム評価基準の設計と運用チューニング

G-Evalの大きな利点は、評価基準を自然言語で自由に定義できることです。プロジェクト固有の品質要件に合わせた評価メトリクスの設計方法を紹介します。

### Rubricで評価の粒度を制御する

DeepEvalの`Rubric`を使うと、スコア範囲ごとに期待される品質レベルを定義できます。

```python
from deepeval.metrics.g_eval.schema import Rubric

detailed_correctness = GEval(
    name="Detailed Correctness",
    criteria="Assess the factual accuracy and completeness of the response.",
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
    rubric=[
        Rubric(score_range=(0, 2), expected_outcome="Factually incorrect."),
        Rubric(score_range=(3, 5), expected_outcome="Partially correct, missing details."),
        Rubric(score_range=(6, 8), expected_outcome="Correct and reasonably complete."),
        Rubric(score_range=(9, 10), expected_outcome="Fully correct and comprehensive."),
    ],
    threshold=0.5,
)
```

Rubricを定義する利点は、評価の「なぜそのスコアなのか」を事前に構造化できる点です。チームでの評価基準の共有にも役立ちます。

### 日本語LLMアプリケーション向けカスタムメトリクス

日本語特有の品質要件を評価するメトリクスも定義できます。ポイントは、`evaluation_steps`を英語で記述することです。Judge LLMの英語性能を活用することで、日本語テキストの評価精度を高められます。

```python
japanese_politeness = GEval(
    name="Japanese Politeness",
    evaluation_steps=[
        "Check whether the response uses appropriate keigo (honorific language).",
        "Verify consistent use of desu/masu style throughout.",
        "Evaluate whether technical terms are explained in accessible Japanese.",
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    threshold=0.7,
)
```

### 運用時のチューニングプロセス

G-Evalメトリクスを本番運用に導入する際のチューニングフローを紹介します。

1. **ベースライン計測**: 現状のLLMアプリ出力を全メトリクスで評価し、平均スコアを把握する
2. **閾値の段階的調整**: 初期閾値0.5（パイプライン動作確認）→ 0.6（低品質のみ不合格）→ 0.7（品質改善ドライバー）と段階的に引き上げる
3. **偽陽性・偽陰性の分析**: 不合格テストケースを手動確認し、G-Evalの判定が人間の判断と一致しているか検証する。不一致が多い場合は`evaluation_steps`を修正する

> **注意:** G-Evalのスコアは「目安」であり、「真実」ではありません。人間の評価者とG-Evalの判定が乖離するケースは必ず発生します。定期的な校正（キャリブレーション）を行い、evaluation_stepsを改善するサイクルを回すことが重要です。

## よくある問題と解決方法

G-Eval評価パイプラインを運用する中で遭遇しやすい問題と対処法をまとめます。

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| スコアが実行ごとに大きく変動する | G-Evalの非決定性、LLMのtemperature設定 | `evaluation_steps`を明示的に定義し、CoTステップ自動生成をスキップする |
| CI/CDの実行時間が長い | メトリクス数 × テストケース数のAPI呼び出し | テストケースのサンプリング、`async_mode=True`の活用 |
| 日本語テキストの評価精度が低い | Judge LLMの日本語理解能力の制約 | `evaluation_steps`を英語で定義する（Judge LLMの英語性能を活用）|
| OpenAI APIのレート制限エラー | 並列評価リクエストの集中 | DeepEvalの`async_mode`の並列数を調整、リトライロジックを追加 |
| 閾値が厳しすぎてCI/CDが常に失敗する | ベースラインスコアに対して閾値が高い | ベースラインスコアを計測し、段階的に閾値を引き上げる |

## まとめと次のステップ

**まとめ:**

- G-Evalは、CoT評価ステップ生成・フォーム充填型判定・確率重み付けスコアリングの3フェーズで、LLM出力を人間に近い精度で評価するフレームワーク
- DeepEval v3.7の`GEval`クラスを使えば、カスタム評価基準を自然言語で定義し、少ないコードで評価を実装できる
- `evaluation_steps`を明示的に定義することで、非決定性を抑えCI/CDでの安定した回帰テストが実現できる
- pytest統合とGitHub Actionsにより、PRごとのLLM品質ゲートを構築できる
- 閾値はベースラインスコアを計測したうえで段階的に引き上げることが実運用のポイント

**次にやるべきこと:**

- プロジェクトの品質要件に合わせた`evaluation_steps`を設計し、ゴールデンデータセットを10件程度から作成する
- `deepeval test run`でベースラインスコアを計測し、閾値を決定する
- GitHub Actionsワークフローを設定し、PRごとの自動評価を有効化する

## 参考

- [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment（原論文、EMNLP 2023）](https://arxiv.org/abs/2303.16634)
- [G-Eval | DeepEval公式ドキュメント](https://deepeval.com/docs/metrics-llm-evals)
- [G-Eval Simply Explained: LLM-as-a-Judge - Confident AI Blog](https://www.confident-ai.com/blog/g-eval-the-definitive-guide)
- [Top 5 G-Eval Metric Use Cases in DeepEval](https://deepeval.com/blog/top-5-geval-use-cases)
- [Unit Testing in CI/CD | DeepEval](https://deepeval.com/docs/evaluation-unit-testing-in-ci-cd)
- [Evaluating the performance of LLM summarization prompts with G-Eval | Microsoft Learn](https://learn.microsoft.com/en-us/ai/playbook/technology-guidance/generative-ai/working-with-llms/evaluation/g-eval-metric-for-summarization)

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
