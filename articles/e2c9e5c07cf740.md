---
title: "vLLMãƒ»SGLangãƒ»TensorRT-LLMã®ãƒãƒ«ãƒGPUä¸¦åˆ—æ¨è«–æ¯”è¼ƒã¨æœ€é©åŒ–å®Ÿè·µ"
emoji: "ğŸ”€"
type: "tech"
topics: ["llm", "vllm", "gpu", "deeplearning", "python"]
published: false
---

# vLLMãƒ»SGLangãƒ»TensorRT-LLMã®ãƒãƒ«ãƒGPUä¸¦åˆ—æ¨è«–æ¯”è¼ƒã¨æœ€é©åŒ–å®Ÿè·µ

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- LLMæ¨è«–ã«ãŠã‘ã‚‹**5ç¨®é¡ã®ä¸¦åˆ—åŒ–æˆ¦ç•¥**ï¼ˆTPãƒ»PPãƒ»DPãƒ»EPãƒ»CPï¼‰ã®ä½¿ã„åˆ†ã‘
- vLLM V1ã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ã¨**V0æ¯”1.7å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„**ã‚’å¼•ãå‡ºã™è¨­å®šæ–¹æ³•
- SGLangã®DP Attentionï¼‹Expert Parallelismã§**DeepSeek-R1ã‚’52.3kå…¥åŠ›tok/s/node**ã§é…ä¿¡ã™ã‚‹æ§‹æˆ
- Prefill/Decodeåˆ†é›¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã°ã‚‰ã¤ãæœ€å¤§20å€æ”¹å–„**ã®ä»•çµ„ã¿
- 3ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒã‹ã‚‰å°ã**ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ¥ã®é¸å®šåŸºæº–**

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šã€œä¸Šç´šã®LLMã‚¤ãƒ³ãƒ•ãƒ©ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»MLOpsã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Python 3.11+ã®éåŒæœŸå‡¦ç†ï¼ˆasync/awaitï¼‰
  - CUDAãƒ»GPU ãƒ¡ãƒ¢ãƒªç®¡ç†ã®åŸºç¤
  - Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆAttentionã€MLPå±¤ï¼‰ã®åŸºæœ¬ç†è§£
  - LLMæ¨è«–ã®åŸºæœ¬æ¦‚å¿µï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼‰
- **å‹•ä½œç¢ºèªç’°å¢ƒ**: vLLM 0.8.xï¼ˆV1ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰ã€SGLang v0.4+ã€TensorRT-LLM 0.18.xã€NVIDIA H100/H200ï¼ˆ2026å¹´2æœˆæ™‚ç‚¹ï¼‰

## çµè«–ãƒ»æˆæœ

ãƒãƒ«ãƒGPUä¸¦åˆ—æ¨è«–ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®æˆæœãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

- **vLLM V1**: V0æ¯”æœ€å¤§**1.7å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„**ã€H200ã§**2.2k tok/s/GPU**ã‚’é”æˆï¼ˆvLLMå…¬å¼ãƒ–ãƒ­ã‚°ã«ã‚ˆã‚‹ï¼‰
- **SGLang**: 96å°ã®H100ã§**52.3kå…¥åŠ›tok/s/node**ã€DeepSeek APIå…¬å¼ä¾¡æ ¼ã®**1/5ã®ã‚³ã‚¹ãƒˆ**ã‚’å®Ÿç¾ï¼ˆLMSYSå…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹ï¼‰
- **TensorRT-LLM**: NVIDIA Blackwellï¼ˆB200ï¼‰ç’°å¢ƒã§vLLMãƒ»SGLangã‚’ä¸Šå›ã‚‹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆClarifaiç¤¾ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã‚ˆã‚‹ï¼‰

ãŸã ã—ã€ã“ã‚Œã‚‰ã®æ•°å€¤ã¯ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æ¸¬å®šå€¤ã§ã‚ã‚Šã€ç’°å¢ƒã«ã‚ˆã£ã¦çµæœã¯ç•°ãªã‚Šã¾ã™ã€‚è‡ªç¤¾ã®è¦ä»¶ã«åˆã£ãŸæ§‹æˆã‚’é¸ã¶ã“ã¨ãŒé‡è¦ã§ã™ã€‚

> é–¢é€£è¨˜äº‹: [LLMãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ï¼šAPIã‚³ã‚¹ãƒˆ50%å‰Šæ¸›ã¨æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ23å€ã‚’å®Ÿç¾ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰](https://zenn.dev/0h_n0/articles/fdb73841a9ac71) â€” API Batchå‡¦ç†ï¼ˆOpenAI/Anthropicï¼‰ã¨vLLMã®åŸºç¤ã‚’è§£èª¬ã—ã¦ã„ã¾ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ãƒãƒ«ãƒGPUç’°å¢ƒã§ã®ä¸¦åˆ—æ¨è«–æœ€é©åŒ–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚

## ãƒãƒ«ãƒGPUä¸¦åˆ—åŒ–ã®5ã¤ã®æˆ¦ç•¥ã‚’ç†è§£ã™ã‚‹

LLMæ¨è«–ã®ãƒãƒ«ãƒGPUä¸¦åˆ—åŒ–ã«ã¯5ã¤ã®ä¸»è¦ãªæˆ¦ç•¥ãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œãã‚Œã®ç‰¹æ€§ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«æœ€é©ãªæ§‹æˆã‚’é¸æŠã§ãã¾ã™ã€‚

### Tensor Parallelismï¼ˆTPï¼‰: å±¤å†…åˆ†å‰²ã§ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å®Ÿç¾ã™ã‚‹

Tensor Parallelismã¯ã€ãƒ¢ãƒ‡ãƒ«ã®å€‹ã€…ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆAttentionå±¤ã‚„MLPå±¤ï¼‰ã‚’è¤‡æ•°GPUã«åˆ†å‰²ã—ã¦ä¸¦åˆ—è¨ˆç®—ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›ã«æœ‰åŠ¹ã§ã™ãŒã€GPUé–“é€šä¿¡ï¼ˆallreduceï¼‰ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

```python
# vLLM: Tensor Parallelismã®è¨­å®šä¾‹
# tensor_parallel_size ã§GPUåˆ†å‰²æ•°ã‚’æŒ‡å®š
from vllm import LLM, SamplingParams

# 4GPU Tensor Parallelism
llm = LLM(
    model="meta-llama/Llama-3.3-70B-Instruct",
    tensor_parallel_size=4,  # 4GPUé–“ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆ†å‰²
    dtype="auto",
    gpu_memory_utilization=0.90,
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=512)
outputs = llm.generate(["Explain tensor parallelism in LLM inference"], sampling_params)
```

**TPã®é©ç”¨æŒ‡é‡**:
- ãƒ¢ãƒ‡ãƒ«ãŒ1GPUã®VRAMã«åã¾ã‚‰ãªã„å ´åˆã«å¿…é ˆ
- NVLink/NVSwitchæ¥ç¶šã®GPUé–“ã§åŠ¹æœãŒé«˜ã„ï¼ˆInfiniBandè·¨ãã§ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å¢—å¤§ï¼‰
- Metaã®å ±å‘Šã§ã¯ã€allreduceé€šä¿¡ãŒã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®**æœ€å¤§30%**ã‚’å ã‚ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚Šã€DDAï¼ˆDirect Data Accessï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æ”¹å–„ã‚’å›³ã£ã¦ã„ã‚‹

> **æ³¨æ„**: TPã®åˆ†å‰²æ•°ã‚’å¢—ã‚„ã™ã»ã©GPUé–“é€šä¿¡ãŒå¢—åŠ ã—ã¾ã™ã€‚é€šå¸¸ã€1ãƒãƒ¼ãƒ‰å†…ã®8GPUä»¥ä¸‹ã«ç•™ã‚ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ãƒãƒ¼ãƒ‰é–“TPã¯é«˜é€Ÿã‚¤ãƒ³ã‚¿ãƒ¼ã‚³ãƒã‚¯ãƒˆãŒãªã„é™ã‚Šå®Ÿç”¨çš„ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

### Pipeline Parallelismï¼ˆPPï¼‰: ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¾¤ã‚’ç›´åˆ—é…ç½®ã™ã‚‹

Pipeline Parallelismã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ç¾¤ã‚’è¤‡æ•°GPUã«é †ç•ªã«é…ç½®ã—ã€å„GPUãŒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®1ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æ‹…å½“ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

```python
# vLLM: Pipeline Parallelism + Tensor Parallelismã®çµ„ã¿åˆã‚ã›
# 8GPU = TP4 Ã— PP2 ã§æ§‹æˆã™ã‚‹ä¾‹
llm = LLM(
    model="meta-llama/Llama-3.3-70B-Instruct",
    tensor_parallel_size=4,
    pipeline_parallel_size=2,  # 2æ®µãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
)
```

**PP vs TPã®ä½¿ã„åˆ†ã‘**:

| è¦³ç‚¹ | Tensor Parallelism | Pipeline Parallelism |
|------|-------------------|---------------------|
| ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ä½ã„ï¼ˆä¸¦åˆ—å®Ÿè¡Œï¼‰ | é«˜ã„ï¼ˆç›´åˆ—å®Ÿè¡Œï¼‰ |
| é€šä¿¡é‡ | å¤§ãã„ï¼ˆæ¯ãƒ¬ã‚¤ãƒ¤ãƒ¼allreduceï¼‰ | å°ã•ã„ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¸é–“ã®ã¿ï¼‰ |
| é©ç”¨å ´é¢ | ãƒãƒ¼ãƒ‰å†…8GPUä»¥ä¸‹ | ãƒãƒ¼ãƒ‰é–“åˆ†æ•£ |
| GPUç¨¼åƒç‡ | é«˜ã„ | ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ã§ä½ä¸‹ã®å¯èƒ½æ€§ |

**æ³¨æ„ç‚¹**: vLLM V1ã‚¨ãƒ³ã‚¸ãƒ³ã§ã¯2026å¹´2æœˆæ™‚ç‚¹ã§Pipeline ParallelismãŒæœªã‚µãƒãƒ¼ãƒˆã§ã™ã€‚PPãŒå¿…è¦ãªå ´åˆã¯V0ã‚¨ãƒ³ã‚¸ãƒ³ã¾ãŸã¯TensorRT-LLMã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

### Data Parallelismï¼ˆDPï¼‰: ãƒ¬ãƒ—ãƒªã‚«ã§å¸¯åŸŸã‚’ç¨¼ã

Data Parallelismã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°GPUã«è¤‡è£½ã—ã€å…¥åŠ›ãƒãƒƒãƒã‚’åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å„GPUãŒç‹¬ç«‹ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒGPUæ•°ã«ã»ã¼æ¯”ä¾‹ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚

```bash
# vLLM: Data Parallelismã¯è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿç¾
# ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µ(nginxç­‰)ã§è¤‡æ•°vLLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ†æ•£

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹1ï¼ˆGPU 0-3: TP=4ï¼‰
CUDA_VISIBLE_DEVICES=0,1,2,3 vllm serve meta-llama/Llama-3.3-70B-Instruct \
    --tensor-parallel-size 4 --port 8000

# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹2ï¼ˆGPU 4-7: TP=4ï¼‰
CUDA_VISIBLE_DEVICES=4,5,6,7 vllm serve meta-llama/Llama-3.3-70B-Instruct \
    --tensor-parallel-size 4 --port 8001
```

### Expert Parallelismï¼ˆEPï¼‰: MoEãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã®æœ€é©åŒ–

Expert Parallelism ã¯ã€Mixture-of-Expertsï¼ˆMoEï¼‰ãƒ¢ãƒ‡ãƒ«ï¼ˆDeepSeek-V3/R1ãªã©ï¼‰ã®å„Expertã‚’ç•°ãªã‚‹GPUã«é…ç½®ã—ã€å¿…è¦ãªExpertã®ã¿ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã™ã‚‹æ‰‹æ³•ã§ã™ã€‚

SGLangã®å ±å‘Šã«ã‚ˆã‚‹ã¨ã€DeepSeek-R1ï¼ˆ671Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–37Bï¼‰ã‚’TPå˜ç‹¬ã§é…ä¿¡ã™ã‚‹å ´åˆã¨æ¯”è¼ƒã—ã¦ã€EPæ§‹æˆã§ã¯**æœ€å¤§5å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š**ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

```bash
# SGLang: Expert Parallelismã®è¨­å®š
# 8GPUç’°å¢ƒã§EP8ï¼ˆ8-way Expert Parallelismï¼‰
python -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1 \
    --tp 1 --dp 8 \
    --enable-ep \
    --trust-remote-code
```

### Context Parallelismï¼ˆCPï¼‰: è¶…é•·æ–‡å…¥åŠ›ã‚’åˆ†å‰²å‡¦ç†ã™ã‚‹

Context Parallelismã¯ã€128Kãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã®è¶…é•·æ–‡å…¥åŠ›ã‚’è¤‡æ•°GPUã«åˆ†å‰²ã—ã¦ä¸¦åˆ—å‡¦ç†ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚Metaã®å ±å‘Šã§ã¯ã€**100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã®å…¥åŠ›ã‚’1åˆ†æœªæº€**ã§å‡¦ç†ã§ããŸã¨ã•ã‚Œã¦ã„ã¾ã™ï¼ˆH100 8GPUãƒãƒ¼ãƒ‰ã€Llama 3 405Bï¼‰ã€‚

**CPã®åˆ©ç”¨å ´é¢**:
- 128K+ ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†
- ã‚³ãƒ¼ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªå…¨ä½“ã®ä¸€æ‹¬è§£æ
- é•·æ–‡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç´„

> **åˆ¶ç´„**: CPã¯Prefillãƒ•ã‚§ãƒ¼ã‚ºå°‚ç”¨ã®æœ€é©åŒ–ã§ã™ã€‚Decodeãƒ•ã‚§ãƒ¼ã‚ºã§ã¯åŠ¹æœãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã¾ãŸã€å®Ÿè£…ãŒè¤‡é›‘ã§ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚µãƒãƒ¼ãƒˆãŒé™å®šçš„ã§ã™ï¼ˆ2026å¹´2æœˆæ™‚ç‚¹ã§Metaå†…éƒ¨ãƒ»NVIDIA Megatronä¸­å¿ƒï¼‰ã€‚

### ä¸¦åˆ—åŒ–æˆ¦ç•¥ã®é¸å®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
flowchart TD
    A[ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºç¢ºèª] --> B{1GPUã«åã¾ã‚‹?}
    B -->|Yes| C[DP: ãƒ¬ãƒ—ãƒªã‚«ä¸¦åˆ—]
    B -->|No| D{MoEãƒ¢ãƒ‡ãƒ«?}
    D -->|Yes| E[EP + DP Attention]
    D -->|No| F{ãƒãƒ¼ãƒ‰å†…å®Œçµ?}
    F -->|Yes| G[TP: ãƒãƒ¼ãƒ‰å†…ä¸¦åˆ—]
    F -->|No| H[TP + PP: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰]
    E --> I{è¶…é•·æ–‡å…¥åŠ›?}
    G --> I
    H --> I
    I -->|Yes 128K+| J[CPè¿½åŠ ]
    I -->|No| K[æ§‹æˆç¢ºå®š]
    J --> K
```

## vLLM V1ã‚¨ãƒ³ã‚¸ãƒ³ã®æœ€é©åŒ–ã‚’å®Ÿè·µã™ã‚‹

vLLM V1ã¯2025å¹´1æœˆã«ã‚¢ãƒ«ãƒ•ã‚¡ãƒªãƒªãƒ¼ã‚¹ã•ã‚Œã€V0ã‹ã‚‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ ¹æœ¬çš„ã«åˆ·æ–°ã—ã¾ã—ãŸã€‚vLLMå…¬å¼ãƒ–ãƒ­ã‚°ã®å ±å‘Šã§ã¯ã€V0æ¯”ã§**æœ€å¤§1.7å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„**ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

### V1ã‚¨ãƒ³ã‚¸ãƒ³ã®ä¸»è¦ãªæ”¹å–„ç‚¹

V1ã§ã¯ä»¥ä¸‹ã®è¨­è¨ˆå¤‰æ›´ã«ã‚ˆã‚Šã€CPUãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆã—ã¦ã„ã¾ã™ã€‚

1. **çµ±ä¸€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©**: Prefillã¨Decodeã‚’åŒºåˆ¥ã›ãšã€`{request_id: num_tokens}`ã®è¾æ›¸ã§ç®¡ç†ã€‚Chunked Prefillã¨Prefix CachingãŒè‡ªç„¶ã«çµ±åˆ
2. **ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: EngineCoreï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ï¼‹ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œï¼‰ã‚’ç‹¬ç«‹ãƒ—ãƒ­ã‚»ã‚¹ã«åˆ†é›¢ã€‚ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®CPUè² è·ãŒGPUå®Ÿè¡Œã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
3. **Persistent Batch**: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€å·®åˆ†ã®ã¿é©ç”¨ã€‚ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ†ãƒ³ã‚½ãƒ«å†ç”Ÿæˆã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›
4. **ã‚¼ãƒ­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰Prefix Caching**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡0%ã§ã‚‚ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã»ã¼ã‚¼ãƒ­ã€‚V1ã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹

```python
# vLLM V1ã‚¨ãƒ³ã‚¸ãƒ³ã®æœ‰åŠ¹åŒ–ã¨æœ€é©åŒ–è¨­å®š
import os

# V1ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æœ‰åŠ¹åŒ–ï¼ˆvLLM 0.8.x ã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
os.environ["VLLM_USE_V1"] = "1"

from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3.3-70B-Instruct",
    tensor_parallel_size=4,
    gpu_memory_utilization=0.92,          # VRAMã®92%ã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å‰²ã‚Šå½“ã¦
    max_num_seqs=256,                     # åŒæ™‚å‡¦ç†ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸Šé™
    enable_chunked_prefill=True,          # é•·æ–‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åˆ†å‰²å‡¦ç†
    max_num_batched_tokens=8192,          # 1ã‚¹ãƒ†ãƒƒãƒ—ã§å‡¦ç†ã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
)

# ãƒãƒƒãƒæ¨è«–ã®å®Ÿè¡Œ
prompts = [f"Summarize the concept of {topic}" for topic in topics_list]
sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)
results = llm.generate(prompts, sampling_params)
```

**ãªãœV1ã‚’é¸ã¶ã®ã‹:**
- V0ã¨åŒã˜APIã§ã€ç’°å¢ƒå¤‰æ•°1ã¤ã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- Prefix CachingãŒè‡ªå‹•ã§åŠ¹ãï¼ˆRAGã®ã‚ˆã†ã«å…±é€šãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒå¤šã„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ç‰¹ã«æœ‰åŠ¹ï¼‰
- torch.compileçµ±åˆã«ã‚ˆã‚Šã€ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ãªã—ã§ãƒ¢ãƒ‡ãƒ«æœ€é©åŒ–ãŒåŠ¹ã

### Chunked Prefillã§é•·æ–‡å…¥åŠ›ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’é˜²ã

Chunked Prefillã¯ã€32Kãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã®é•·æ–‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€Decodeãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã—ã¦å‡¦ç†ã™ã‚‹æ©Ÿèƒ½ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é•·æ–‡Prefillå‡¦ç†ä¸­ã«æ—¢å­˜ã®Decodeãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¾…ãŸã•ã‚Œã‚‹ï¼ˆHead-of-Line Blockingï¼‰å•é¡Œã‚’è§£æ¶ˆã—ã¾ã™ã€‚

```python
# Chunked Prefillã®å‹•ä½œã‚¤ãƒ¡ãƒ¼ã‚¸
# é•·æ–‡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ32Kãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’4096ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤åˆ†å‰²

# ã‚¹ãƒ†ãƒƒãƒ—1: [Prefill chunk 1: 4096 tok] + [Decode: req_1, req_2, req_3]
# ã‚¹ãƒ†ãƒƒãƒ—2: [Prefill chunk 2: 4096 tok] + [Decode: req_1, req_2, req_3, req_4]
# ã‚¹ãƒ†ãƒƒãƒ—3: [Prefill chunk 3: 4096 tok] + [Decode: req_2, req_3, req_4]
# ...

# vLLM V1ã§ã¯ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’max_num_batched_tokensã§åˆ¶å¾¡
llm = LLM(
    model="meta-llama/Llama-3.3-70B-Instruct",
    tensor_parallel_size=4,
    enable_chunked_prefill=True,
    max_num_batched_tokens=4096,  # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºä¸Šé™
)
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹ã¨Decodeã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå®‰å®šã—ã¾ã™ãŒã€Prefillã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆTTFT: Time to First Tokenï¼‰ã¯æ‚ªåŒ–ã—ã¾ã™ã€‚ãƒãƒƒãƒå‡¦ç†ä¸­å¿ƒã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯`max_num_batched_tokens`ã‚’å¤§ãã‚ï¼ˆ8192ã€œ16384ï¼‰ã«è¨­å®šã™ã‚‹ã®ãŒæœ‰åŠ¹ã§ã™ã€‚

### Continuous Batchingã®å†…éƒ¨å‹•ä½œã‚’æŠŠæ¡ã™ã‚‹

vLLMã®Continuous Batchingï¼ˆé€£ç¶šãƒãƒƒãƒãƒ³ã‚°ï¼‰ã¯ã€å¾“æ¥ã®Static Batchingã¨ã¯ç•°ãªã‚Šã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†ã™ã‚‹ãŸã³ã«æ–°ã—ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒƒãƒã«è¿½åŠ ã—ã¾ã™ã€‚

```mermaid
sequenceDiagram
    participant Client
    participant Scheduler
    participant GPU

    Client->>Scheduler: ãƒªã‚¯ã‚¨ã‚¹ãƒˆA (Prefill 2048tok)
    Client->>Scheduler: ãƒªã‚¯ã‚¨ã‚¹ãƒˆB (Prefill 512tok)
    Scheduler->>GPU: Step 1: [A_prefill_chunk, B_prefill]

    Client->>Scheduler: ãƒªã‚¯ã‚¨ã‚¹ãƒˆC (Prefill 256tok)
    Scheduler->>GPU: Step 2: [A_prefill_chunk, B_decode, C_prefill]

    Note over GPU: Bå®Œäº† â†’ ã‚¹ãƒ­ãƒƒãƒˆè§£æ”¾
    Client->>Scheduler: ãƒªã‚¯ã‚¨ã‚¹ãƒˆD (Prefill 1024tok)
    Scheduler->>GPU: Step 3: [A_decode, C_decode, D_prefill]
```

**Static Batchingã¨ã®æ€§èƒ½å·®**: Continuous Batchingã«ã‚ˆã‚Šã€GPUç¨¼åƒç‡ã¯å ±å‘Šã«ã‚ˆã£ã¦ç•°ãªã‚Šã¾ã™ãŒã€vLLMå…¬å¼ã§ã¯**3ã€œ10å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š**ãŒå¾—ã‚‰ã‚Œã‚‹ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ç‰¹ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡ºåŠ›é•·ã«ã°ã‚‰ã¤ããŒã‚ã‚‹å ´åˆã«åŠ¹æœãŒå¤§ããã€çŸ­ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†æ¬¡ç¬¬ã™ãã«ã‚¹ãƒ­ãƒƒãƒˆãŒå†åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚

## SGLangã®DP Attentionï¼‹EPã§ MoEãƒ¢ãƒ‡ãƒ«ã‚’é«˜é€Ÿé…ä¿¡ã™ã‚‹

SGLangã¯ã€LMSYSï¼ˆChatbot Arenaã®é‹å–¶ãƒãƒ¼ãƒ ï¼‰ãŒé–‹ç™ºã™ã‚‹é«˜æ€§èƒ½æ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ç‰¹ã«MoEãƒ¢ãƒ‡ãƒ«ï¼ˆDeepSeek-V3/R1ï¼‰å‘ã‘ã®æœ€é©åŒ–ãŒå……å®Ÿã—ã¦ã„ã¾ã™ã€‚

### DP Attention: KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é‡è¤‡ã‚’æ’é™¤ã™ã‚‹

SGLang v0.4ã§å°å…¥ã•ã‚ŒãŸDP Attentionã¯ã€é€šå¸¸ã®Data Parallelismã¨ç•°ãªã‚Šã€**Attentionå±¤ã«ã®ã¿Data Parallelismã‚’é©ç”¨**ã—ã€MLP/Expertå±¤ã«ã¯Expert Parallelismã‚’é©ç”¨ã™ã‚‹ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥ã§ã™ã€‚

```python
# SGLangã®DP Attentionæ§‹æˆã‚¤ãƒ¡ãƒ¼ã‚¸
# 8GPUç’°å¢ƒã®å ´åˆ

# å¾“æ¥ã®TP8æ§‹æˆ:
#   - å„GPUãŒå…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®1/8ã‚’ä¿æŒ
#   - KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯8GPUé–“ã§åˆ†æ•£ â†’ å„GPUã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å®¹é‡ãŒåˆ¶é™

# DP8 + EPæ§‹æˆï¼ˆSGLangï¼‰:
#   - Attentionå±¤: å„GPUãŒç‹¬ç«‹ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿æŒï¼ˆDPï¼‰
#   - Expertå±¤: Expertã‚’8GPUã«åˆ†æ•£é…ç½®ï¼ˆEPï¼‰
#   - é€šä¿¡: Expertå±¤ã§ã®all-to-allã®ã¿ï¼ˆAttentionå±¤ã¯ãƒ­ãƒ¼ã‚«ãƒ«è¨ˆç®—ï¼‰
```

ã“ã®æ§‹æˆã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®åˆ©ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚

- **KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å®¹é‡ã®å¢—å¤§**: TPæ§‹æˆã§ã¯å„GPUã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆ¶é™ã•ã‚Œã‚‹ãŒã€DPæ§‹æˆã§ã¯å„GPUãŒç‹¬ç«‹ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†
- **é€šä¿¡é‡ã®å‰Šæ¸›**: LMSYSå…¬å¼ã®å ±å‘Šã§ã¯ã€TPå˜ç‹¬æ¯”ã§é€šä¿¡ã‚³ã‚¹ãƒˆãŒ**50%å‰Šæ¸›**
- **Decodeã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®å‘ä¸Š**: DeepSeekãƒ¢ãƒ‡ãƒ«ã®Multi-head Latent Attentionï¼ˆMLAï¼‰æ§‹é€ ã¨ã®ç›¸æ€§ãŒè‰¯ãã€LMSYSå ±å‘Šã§ã¯**æœ€å¤§1.9å€ã®Decodeã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„**

### å¤§è¦æ¨¡EPæ§‹æˆã§DeepSeek-R1ã‚’é…ä¿¡ã™ã‚‹

LMSYSå…¬å¼ãƒ–ãƒ­ã‚°ã®å ±å‘Šã«ã‚ˆã‚‹ã¨ã€96å°ã®H100 GPUï¼ˆ12ãƒãƒ¼ãƒ‰ï¼‰ã§DeepSeek-R1ã‚’é…ä¿¡ã—ã€ä»¥ä¸‹ã®æˆæœã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

```bash
# SGLang: 96 H100 GPUã§ã®å¤§è¦æ¨¡é…ä¿¡æ§‹æˆä¾‹
# 12ãƒãƒ¼ãƒ‰ Ã— 8GPU/ãƒãƒ¼ãƒ‰ = 96 GPU

# Prefillãƒãƒ¼ãƒ‰ï¼ˆ4ãƒãƒ¼ãƒ‰ = 32 GPUï¼‰
python -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1 \
    --tp 1 --dp 8 --enable-ep \
    --node-rank 0 --nnodes 4 \
    --disagg-mode prefill \
    --trust-remote-code

# Decodeãƒãƒ¼ãƒ‰ï¼ˆ8ãƒãƒ¼ãƒ‰ = 64 GPUï¼‰
python -m sglang.launch_server \
    --model deepseek-ai/DeepSeek-R1 \
    --tp 1 --dp 8 --enable-ep \
    --node-rank 0 --nnodes 8 \
    --disagg-mode decode \
    --trust-remote-code
```

**LMSYSå ±å‘Šã®é”æˆæ•°å€¤ï¼ˆ96 H100, 2000ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰:**

| æŒ‡æ¨™ | æ•°å€¤ |
|------|------|
| å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | 52.3k tok/s/node |
| å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | 22.3k tok/s/node |
| ã‚³ã‚¹ãƒˆ | $0.20/M output tokens |
| TPå˜ç‹¬æ¯”ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | 5å€æ”¹å–„ |

**æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã®å†…è¨³:**

1. **Two-Batch Overlapï¼ˆTBOï¼‰**: è¨ˆç®—ã¨é€šä¿¡ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã—ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’27-35%å‰Šæ¸›ã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’40.5%æ‹¡å¤§å¯èƒ½ã«
2. **Expert Parallel Load Balancerï¼ˆEPLBï¼‰**: Experté–“ã®ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ä¸å‡è¡¡ã‚’æœ€é©åŒ–ã—ã€Prefill 1.49å€ãƒ»Decode 2.54å€ã®é«˜é€ŸåŒ–
3. **DeepEPãƒ»DeepGEMMçµ±åˆ**: DeepSeekç¤¾æä¾›ã®æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«ã‚’æ´»ç”¨

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ**: EPæ§‹æˆã§ã¯Experté–“ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«all-to-allé€šä¿¡ãŒå¿…è¦ã§ã™ã€‚ãƒãƒ¼ãƒ‰é–“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã€InfiniBand 400Gbpsä»¥ä¸Šã®ç’°å¢ƒãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ä½å¸¯åŸŸç’°å¢ƒã§ã¯TPæ§‹æˆã®æ–¹ãŒå®ŸåŠ¹ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒé«˜ããªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚

## TensorRT-LLMã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ã¨In-Flight Batchingã‚’æ´»ç”¨ã™ã‚‹

TensorRT-LLMã¯NVIDIAå…¬å¼ã®æ¨è«–æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€NVIDIAãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¨ã®æœ€æ·±ãƒ¬ãƒ™ãƒ«ã®çµ±åˆãŒå¼·ã¿ã§ã™ã€‚

### In-Flight Batchingã®ä»•çµ„ã¿

TensorRT-LLMã®In-Flight Batchingã¯ã€vLLMã®Continuous Batchingã¨åŒæ§˜ã®æ¦‚å¿µã§ã™ãŒã€NVIDIAã®ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã¨ç·Šå¯†ã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ç‚¹ãŒç•°ãªã‚Šã¾ã™ã€‚

```python
# TensorRT-LLM: In-Flight Batchingã®è¨­å®š
# trtllm-buildã§ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ“ãƒ«ãƒ‰ã—ã€tritonserver ã§é…ä¿¡

# Step 1: ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ‰ï¼ˆTensor Parallelism 4GPUï¼‰
# trtllm-build \
#     --checkpoint_dir ./llama-70b-checkpoint \
#     --output_dir ./llama-70b-engine \
#     --tp_size 4 \
#     --max_batch_size 256 \
#     --max_input_len 4096 \
#     --max_seq_len 8192 \
#     --use_paged_kv_cache \
#     --enable_chunked_context

# Step 2: Triton Inference Serverã§é…ä¿¡
# mpirun -n 4 --allow-run-as-root \
#     tritonserver \
#     --model-repository ./triton_model_repo \
#     --grpc-port 8001 \
#     --http-port 8000
```

**TensorRT-LLMã®ç‹¬è‡ªæœ€é©åŒ–:**
- **ã‚«ã‚¹ã‚¿ãƒ Attentionã‚«ãƒ¼ãƒãƒ«**: FP8/INT8é‡å­åŒ–å¯¾å¿œã®fused Attentionã‚«ãƒ¼ãƒãƒ«
- **Paged KV Cache**: vLLMã®PagedAttentionã¨åŒæ§˜ã®ãƒšãƒ¼ã‚¸ãƒ™ãƒ¼ã‚¹ãƒ¡ãƒ¢ãƒªç®¡ç†
- **Speculative Decoding**: Draft modelã«ã‚ˆã‚‹æŠ•æ©Ÿçš„ãƒ‡ã‚³ãƒ¼ãƒ‰ã®ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚µãƒãƒ¼ãƒˆ

### Blackwell GPUï¼ˆB200ï¼‰ã§ã®æ€§èƒ½å„ªä½

Clarifaiç¤¾ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆGPT-OSS-120Bã€2Ã—GPUæ§‹æˆï¼‰ã«ã‚ˆã‚‹ã¨ã€B200ç’°å¢ƒã§ã¯TensorRT-LLMãŒå…¨æŒ‡æ¨™ã§vLLMãƒ»SGLangã‚’ä¸Šå›ã£ãŸã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯NVIDIAãŒæœ€æ–°ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å‘ã‘ã«å…ˆè¡Œã—ã¦ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚’è¡Œã†ãŸã‚ã§ã™ã€‚

ä¸€æ–¹ã€H100ç’°å¢ƒã§ã¯ç•°ãªã‚‹çµæœãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ã€‚

| ã‚¨ãƒ³ã‚¸ãƒ³ | H100 ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (tok/s, 100ä¸¦åˆ—) | H100 TTFTç‰¹æ€§ | å‚™è€ƒ |
|----------|-----------------------------------|---------------|------|
| vLLM | 4,741 | æœ€é€Ÿï¼ˆå…¨ä¸¦åˆ—åº¦ã§ä¸€è²«ï¼‰ | GPT-OSS-120B, 2Ã—H100 |
| SGLang | ã‚„ã‚„ä½ã„ | å®‰å®šï¼ˆ4-21ms/tokï¼‰ | ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã°ã‚‰ã¤ã |
| TensorRT-LLM | ä¸­ç¨‹åº¦ | ä¸­ç¨‹åº¦ | ãƒ“ãƒ«ãƒ‰å·¥ç¨‹ãŒå¿…è¦ |

> å‡ºå…¸: [Comparing SGLANG, vLLM, and TensorRT-LLM with GPT-OSS-120B](https://www.clarifai.com/blog/comparing-sglang-vllm-and-tensorrt-llm-with-gpt-oss-120b)ï¼ˆClarifaiç¤¾ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€2025å¹´ï¼‰

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**: TensorRT-LLMã¯ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ‰ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ã§ã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã®ãŸã³ã«å†ãƒ“ãƒ«ãƒ‰ãŒç™ºç”Ÿã—ã¾ã™ã€‚vLLMãƒ»SGLangã¯Hugging Faceãƒ¢ãƒ‡ãƒ«ã‚’ãã®ã¾ã¾èª­ã¿è¾¼ã‚ã‚‹ãŸã‚ã€é–‹ç™ºã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé€Ÿã„ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ãƒ¢ãƒ‡ãƒ«ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹å ´åˆã«ã¯TensorRT-LLMã®æ€§èƒ½ãƒ¡ãƒªãƒƒãƒˆãŒæ´»ãã¾ã™ã€‚

## Prefill/Decodeåˆ†é›¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰ã™ã‚‹

2025å¹´ä»¥é™ã€Prefillï¼ˆå…¥åŠ›å‡¦ç†ï¼‰ã¨Decodeï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼‰ã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ãŸGPUã‚¯ãƒ©ã‚¹ã‚¿ã§å®Ÿè¡Œã™ã‚‹**Prefill/Decodeåˆ†é›¢ï¼ˆPD Disaggregationï¼‰**ãŒã€å¤šãã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§æ¨™æº–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãªã£ã¦ã„ã¾ã™ã€‚Metaã€LinkedInã€Mistralã€HuggingFaceãŒvLLMãƒ™ãƒ¼ã‚¹ã®PDåˆ†é›¢ã‚’æœ¬ç•ªé‹ç”¨ã—ã¦ã„ã¾ã™ã€‚

### ãªãœåˆ†é›¢ã™ã‚‹ã®ã‹

Prefillã¨Decodeã¯è¨ˆç®—ç‰¹æ€§ãŒæ ¹æœ¬çš„ã«ç•°ãªã‚Šã¾ã™ã€‚

| ãƒ•ã‚§ãƒ¼ã‚º | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | GPUåˆ©ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ | æœ€é©ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ |
|---------|-------------|----------------|-----------------|
| Prefill | **è¨ˆç®—å¾‹é€Ÿ** | å¤§è¡Œåˆ—ã®ä¸¦åˆ—ä¹—ç®— | é«˜FLOPSã®GPUï¼ˆH100 SXMï¼‰ |
| Decode | **ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¾‹é€Ÿ** | KVã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å‡ºã— | é«˜HBMå¸¯åŸŸã®GPUï¼ˆH200ï¼‰ |

åŒä¸€GPUä¸Šã§ä¸¡ãƒ•ã‚§ãƒ¼ã‚ºã‚’æ··åœ¨å®Ÿè¡Œã™ã‚‹ã¨ã€é•·æ–‡PrefillãŒDecodeã®å¿œç­”ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ï¼ˆHead-of-Line Blockingï¼‰ã€Inter-Token Latencyï¼ˆITLï¼‰ã®ãƒ†ãƒ¼ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ‚ªåŒ–ã—ã¾ã™ã€‚

### vLLMã§ã®PDåˆ†é›¢æ§‹æˆ

```bash
# vLLM: Prefill/Decodeåˆ†é›¢æ§‹æˆï¼ˆå®Ÿé¨“çš„æ©Ÿèƒ½ã€vLLM 0.8.xï¼‰

# Prefillãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆé«˜FLOPS GPUå‰²ã‚Šå½“ã¦ï¼‰
VLLM_USE_V1=1 vllm serve meta-llama/Llama-3.3-70B-Instruct \
    --tensor-parallel-size 4 \
    --port 8100 \
    --kv-connector PyNcclConnector \
    --kv-role kv_producer \
    --kv-parallel-size 4 \
    --kv-buffer-size 1e10

# Decodeãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆé«˜ãƒ¡ãƒ¢ãƒªå¸¯åŸŸGPUå‰²ã‚Šå½“ã¦ï¼‰
VLLM_USE_V1=1 vllm serve meta-llama/Llama-3.3-70B-Instruct \
    --tensor-parallel-size 4 \
    --port 8200 \
    --kv-connector PyNcclConnector \
    --kv-role kv_consumer \
    --kv-parallel-size 4 \
    --kv-buffer-size 1e10
```

**PDåˆ†é›¢ã®åŠ¹æœ:**
- **ITLãƒ†ãƒ¼ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®å‰Šæ¸›**: é•·æ–‡Prefillã®å½±éŸ¿ãŒDecodeã«æ³¢åŠã—ãªã„
- **ç‹¬ç«‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°**: Prefillãƒãƒ¼ãƒ‰ã®æ•°ã‚’Decodeãƒãƒ¼ãƒ‰ã¨ç‹¬ç«‹ã«å¢—æ¸›å¯èƒ½
- **ã‚¤ãƒ³ãƒ•ãƒ©ã‚³ã‚¹ãƒˆ15-40%å‰Šæ¸›ã®å¯èƒ½æ€§**: DistServeè«–æ–‡ï¼ˆOSDI '24ï¼‰ã®è‘—è€…ã‚‰ã®å®Ÿé¨“ã«ã‚ˆã‚‹å ±å‘Š

**åˆ¶ç´„ã¨æ³¨æ„ç‚¹**:
- PDåˆ†é›¢ã¯**ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ”¹å–„ã™ã‚‹ã‚ã‘ã§ã¯ãªã„**ï¼ˆvLLMå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ˜è¨˜ï¼‰
- KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’Prefillãƒãƒ¼ãƒ‰ã‹ã‚‰Decodeãƒãƒ¼ãƒ‰ã¸è»¢é€ã™ã‚‹ãŸã‚ã€**é«˜é€Ÿãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆNVLink/InfiniBandï¼‰ãŒå¿…é ˆ**
- æ§‹æˆã®è¤‡é›‘ã•ãŒå¢—ã—ã€é‹ç”¨è² è·ãŒä¸ŠãŒã‚‹
- vLLMã§ã¯2026å¹´2æœˆæ™‚ç‚¹ã§å®Ÿé¨“çš„æ©Ÿèƒ½ï¼ˆExperimentalï¼‰

## ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ¥ã®ã‚¨ãƒ³ã‚¸ãƒ³é¸å®šã¨ã‚ˆãã‚ã‚‹å•é¡Œã‚’æ•´ç†ã™ã‚‹

### é¸å®šåˆ¤æ–­ãƒ•ãƒ­ãƒ¼

```mermaid
flowchart TD
    A[ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ç‰¹æ€§ã¯?] --> B{ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”?}
    B -->|Yes| C{MoEãƒ¢ãƒ‡ãƒ«?}
    B -->|No ãƒãƒƒãƒå‡¦ç†| D{GPUå›ºå®šæ§‹æˆ?}
    C -->|Yes DeepSeekç­‰| E[SGLang + EP + PDåˆ†é›¢]
    C -->|No Dense Model| F{GPUç¨®åˆ¥ã¯?}
    F -->|Blackwell B200| G[TensorRT-LLM]
    F -->|H100/H200/A100| H[vLLM V1]
    D -->|Yes| I[TensorRT-LLM]
    D -->|No é »ç¹ãªãƒ¢ãƒ‡ãƒ«å¤‰æ›´| J[vLLM V1]
```

### ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨æ§‹æˆ

| ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ | æ¨å¥¨ã‚¨ãƒ³ã‚¸ãƒ³ | ä¸¦åˆ—åŒ–æˆ¦ç•¥ | æ ¹æ‹  |
|-------------|-------------|-----------|------|
| ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é…ä¿¡ï¼ˆDenseï¼‰ | vLLM V1 | TP | TTFTæœ€é€Ÿï¼ˆClarifaiå ±å‘Šï¼‰ |
| ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é…ä¿¡ï¼ˆMoEï¼‰ | SGLang | EP + DP Attention | DeepSeekã§æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆLMSYSå ±å‘Šï¼‰ |
| ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒ | vLLM V1 | TP + DP | Continuous BatchingãŒè‡ªå‹•ã§åŠ¹ã |
| è¶…é•·æ–‡å…¥åŠ›ï¼ˆ128K+ï¼‰ | SGLang/vLLM | TP + CP | CPå¯¾å¿œã¯é™å®šçš„ |
| Blackwell GPUå›ºå®š | TensorRT-LLM | TP | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æœ€é©åŒ–ãŒåŠ¹ã |
| æœ¬ç•ªå›ºå®šãƒ¢ãƒ‡ãƒ« | TensorRT-LLM | TP + PP | ãƒ“ãƒ«ãƒ‰æ¸ˆã¿ã‚¨ãƒ³ã‚¸ãƒ³ã§æœ€é«˜æ€§èƒ½ |

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| GPUç¨¼åƒç‡ãŒä½ã„ï¼ˆ<50%ï¼‰ | ãƒãƒƒãƒã‚µã‚¤ã‚ºä¸è¶³ | `max_num_seqs`ã‚’å¢—ã‚„ã™ / DPæ§‹æˆã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆé›†ç´„ |
| OOMï¼ˆOut of Memoryï¼‰ | KVã‚­ãƒ£ãƒƒã‚·ãƒ¥éå¤§ | `gpu_memory_utilization`ã‚’ä¸‹ã’ã‚‹ / TPæ•°ã‚’å¢—ã‚„ã™ |
| TTFTé…å»¶ï¼ˆ>5ç§’ï¼‰ | é•·æ–‡PrefillãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ | Chunked Prefillæœ‰åŠ¹åŒ– / PDåˆ†é›¢ |
| ITLãƒ†ãƒ¼ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å¤§ | Prefill/Decodeæ··åœ¨å®Ÿè¡Œ | PDåˆ†é›¢æ§‹æˆã‚’æ¤œè¨ |
| EPæ§‹æˆã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‡ºãªã„ | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸä¸è¶³ | InfiniBand 400Gbps+ã‚’ç¢ºä¿ / TPæ§‹æˆã«åˆ‡æ›¿ |
| TensorRT-LLMãƒ“ãƒ«ãƒ‰ã‚¨ãƒ©ãƒ¼ | ãƒ¢ãƒ‡ãƒ«æœªå¯¾å¿œ | ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆç¢ºèª / vLLM/SGLangã¸åˆ‡æ›¿ |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- LLMæ¨è«–ã®ä¸¦åˆ—åŒ–ã¯**TPãƒ»PPãƒ»DPãƒ»EPãƒ»CP**ã®5æˆ¦ç•¥ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ãƒ»ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œã˜ãŸä½¿ã„åˆ†ã‘ãŒé‡è¦
- **vLLM V1**ã¯V0æ¯”1.7å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ”¹å–„ã‚’é”æˆã—ã€ã‚¼ãƒ­ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰Prefix Cachingã¨Chunked PrefillãŒæ¨™æº–æ­è¼‰
- **SGLang**ã¯DP Attention + EPã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ§‹æˆã§MoEãƒ¢ãƒ‡ãƒ«ã«å¼·ãã€DeepSeek-R1ã§52.3kå…¥åŠ›tok/s/nodeï¼ˆLMSYSå ±å‘Šï¼‰
- **TensorRT-LLM**ã¯NVIDIAæœ€æ–°GPUã¨ã®æ·±ã„çµ±åˆãŒå¼·ã¿ã§ã€Blackwellç’°å¢ƒã§æœ€é«˜æ€§èƒ½
- **Prefill/Decodeåˆ†é›¢**ã¯2025å¹´ä»¥é™ã®æ¨™æº–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãªã‚Šã¤ã¤ã‚ã‚‹ãŒã€æ§‹æˆã®è¤‡é›‘ã•ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¦ä»¶ãŒãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•
- å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°å€¤ã¯ç‰¹å®šæ¡ä»¶ä¸‹ã§ã®æ¸¬å®šå€¤ã§ã‚ã‚Šã€å®Ÿç’°å¢ƒã§ã¯è‡ªç¤¾ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®è¨ˆæ¸¬ãŒä¸å¯æ¬ 

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

1. è‡ªç¤¾ã®ãƒ¢ãƒ‡ãƒ«ãƒ»GPUæ§‹æˆã§3ã‚¨ãƒ³ã‚¸ãƒ³ã‚’å®Ÿéš›ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆ[vLLM benchmark suite](https://docs.vllm.ai/en/stable/performance/benchmarks.html)ã‚’æ´»ç”¨ï¼‰
2. MoEãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†å ´åˆã¯SGLangã®EPæ§‹æˆã‚’æ¤œè¨¼ã—ã€TPæ§‹æˆã¨æ¯”è¼ƒ
3. æœ¬ç•ªç’°å¢ƒã§ã¯PDåˆ†é›¢ã®å°å…¥ã‚’æ®µéšçš„ã«æ¤œè¨ï¼ˆã¾ãšã¯Chunked Prefillã‹ã‚‰é–‹å§‹ï¼‰

## å‚è€ƒ

- [vLLM V1: A Major Upgrade to vLLM's Core Architecture](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html) â€” vLLM V1ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³è§£
- [Inside vLLM: Anatomy of a High-Throughput LLM Inference System](https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html) â€” vLLM V1å†…éƒ¨å‹•ä½œã®è§£èª¬
- [vLLM Large Scale Serving: DeepSeek @ 2.2k tok/s/H200 with Wide-EP](https://blog.vllm.ai/2025/12/17/large-scale-serving.html) â€” vLLMã®å¤§è¦æ¨¡EPé…ä¿¡
- [Deploying DeepSeek with PD Disaggregation and Large-Scale EP on 96 H100 GPUs](https://lmsys.org/blog/2025-05-05-large-scale-ep/) â€” SGLangå…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
- [Scaling LLM Inference: Innovations in Tensor Parallelism, Context Parallelism, and Expert Parallelism](https://engineering.fb.com/2025/10/17/ai-research/scaling-llm-inference-innovations-tensor-parallelism-context-parallelism-expert-parallelism/) â€” Meta Engineering Blog
- [Data, tensor, pipeline, expert and hybrid parallelisms](https://bentoml.com/llm/inference-optimization/data-tensor-pipeline-expert-hybrid-parallelism) â€” BentoML LLM Inference Handbook
- [Comparing SGLANG, vLLM, and TensorRT-LLM with GPT-OSS-120B](https://www.clarifai.com/blog/comparing-sglang-vllm-and-tensorrt-llm-with-gpt-oss-120b) â€” 3ã‚¨ãƒ³ã‚¸ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
- [Disaggregated Prefilling â€” vLLM Documentation](https://docs.vllm.ai/en/latest/features/disagg_prefill/) â€” PDåˆ†é›¢ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving](https://www.usenix.org/system/files/osdi24-zhong-yinmin.pdf) â€” OSDI '24è«–æ–‡

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
