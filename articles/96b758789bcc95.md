---
title: "Ollama 0.17ã§ã‚ªãƒ³ãƒ—ãƒ¬LLMæ¨è«–ç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰"
emoji: "ğŸ¦™"
type: "tech"
topics: ["ollama", "llm", "docker", "gpu", "selfhosted"]
published: false
---

# Ollama 0.17ã§ã‚ªãƒ³ãƒ—ãƒ¬LLMæ¨è«–ç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹å®Ÿè·µã‚¬ã‚¤ãƒ‰

## ã“ã®è¨˜äº‹ã§ã‚ã‹ã‚‹ã“ã¨

- Ollama 0.17ã®æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ç‚¹ã®ç†è§£
- Docker Compose + GPUã§Ollamaã‚’ã‚ªãƒ³ãƒ—ãƒ¬ç’°å¢ƒã«æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹å…·ä½“çš„æ‰‹é †
- OpenAIäº’æ›APIã‚’ä½¿ã£ãŸæ—¢å­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®çµ±åˆæ–¹æ³•
- VRAMè¦ä»¶ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«é¸å®šã¨KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã®å®Ÿè·µ
- ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã¨ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼é‹ç”¨ã®æ§‹æˆ

## å¯¾è±¡èª­è€…

- **æƒ³å®šèª­è€…**: ä¸­ç´šè€…ã®ã‚¤ãƒ³ãƒ•ãƒ©ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒ»MLã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢
- **å¿…è¦ãªå‰æçŸ¥è­˜**:
  - Docker / Docker Composeã®åŸºæœ¬æ“ä½œ
  - NVIDIA GPUãƒ‰ãƒ©ã‚¤ãƒã¨CUDAã®åŸºç¤çŸ¥è­˜
  - REST APIã®åŸºæœ¬çš„ãªåˆ©ç”¨çµŒé¨“
  - Linuxã®ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ï¼ˆsystemdã€ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ï¼‰

## çµè«–ãƒ»æˆæœ

Ollama 0.17ã‚’ä½¿ã£ãŸã‚ªãƒ³ãƒ—ãƒ¬LLMæ¨è«–ç’°å¢ƒã¯ã€ã‚¯ãƒ©ã‚¦ãƒ‰APIæ¯”ã§**å¹´é–“ã‚³ã‚¹ãƒˆã‚’98%å‰Šæ¸›**ã§ãã‚‹æ§‹æˆã§ã™ã€‚Kongç¤¾ã®2025 Enterprise AIãƒ¬ãƒãƒ¼ãƒˆã«ã‚ˆã‚‹ã¨ã€44%ã®çµ„ç¹”ãŒãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’LLMå°å…¥ã®éšœå£ã¨ã—ã¦æŒ™ã’ã¦ãŠã‚Šã€ã‚ªãƒ³ãƒ—ãƒ¬æ§‹æˆãŒãã®è§£æ±ºç­–ã¨ãªã‚Šã¾ã™ã€‚Ollama 0.17ã§ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ãŒ**æœ€å¤§40%é«˜é€ŸåŒ–**ã•ã‚Œã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥8bité‡å­åŒ–ã«ã‚ˆã‚ŠåŒä¸€GPUã§**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ç´„2å€**ã«æ‹¡å¼µå¯èƒ½ã§ã™ã€‚ãŸã ã—ã€ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼é«˜è² è·ç’°å¢ƒã§ã¯vLLMã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒå¤§å¹…ã«ä¸Šå›ã‚‹ãŸã‚ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«å¿œã˜ãŸä½¿ã„åˆ†ã‘ãŒå¿…è¦ã§ã™ã€‚

## Ollamaã®ä½ç½®ã¥ã‘ã¨ãƒ„ãƒ¼ãƒ«é¸å®šã‚’ç†è§£ã™ã‚‹

ãƒ­ãƒ¼ã‚«ãƒ«LLMæ¨è«–ãƒ„ãƒ¼ãƒ«ã¯2026å¹´æ™‚ç‚¹ã§è¤‡æ•°ã®é¸æŠè‚¢ãŒã‚ã‚Šã¾ã™ã€‚ã¾ãšã¯ä¸»è¦ãƒ„ãƒ¼ãƒ«ã®ç‰¹æ€§ã‚’æŠŠæ¡ã—ã€OllamaãŒé©ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’æ˜ç¢ºã«ã—ã¾ã—ã‚‡ã†ã€‚

### ä¸»è¦æ¨è«–ãƒ„ãƒ¼ãƒ«ã®æ¯”è¼ƒ

| ãƒ„ãƒ¼ãƒ« | ã‚·ãƒ³ã‚°ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼é€Ÿåº¦ | ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ | ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é›£æ˜“åº¦ | ä¸»ãªç”¨é€” |
|--------|---------------------|--------------------------|-------------------|----------|
| **Ollama** | llama.cppã¨åŒç­‰ï¼ˆ6%ä»¥å†…ã®å·®ï¼‰ | 41 TPS | ä½ï¼ˆ3ã‚³ãƒãƒ³ãƒ‰ï¼‰ | é–‹ç™ºãƒ»ä¸­å°è¦æ¨¡æœ¬ç•ª |
| **vLLM** | ã‚„ã‚„é…ã„ | 793 TPS | ä¸­ã€œé«˜ | å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆ |
| **llama.cpp** | æœ€é€Ÿ | ä½ã„ | é«˜ï¼ˆãƒ“ãƒ«ãƒ‰å¿…è¦ï¼‰ | ã‚¨ãƒƒã‚¸ãƒ»çµ„ã¿è¾¼ã¿ |
| **Docker Model Runner** | OllamaåŒç­‰ | ä½ã„ | ä½ | Dockerç’°å¢ƒã®é–‹ç™ºç”¨é€” |

ä¸Šè¨˜ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€Red Hatã®æ¤œè¨¼è¨˜äº‹ã§å ±å‘Šã•ã‚ŒãŸæ•°å€¤ã§ã™ã€‚ã‚·ãƒ³ã‚°ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã¯llama.cppï¼ˆOllamaã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ï¼‰ãŒæœ€é€Ÿã§ã™ãŒã€å·®ã¯6%ä»¥å†…ã§ã™ã€‚ãƒãƒ«ãƒãƒ¦ãƒ¼ã‚¶ãƒ¼ç’°å¢ƒã§ã¯vLLMãŒOllamaã®ç´„19å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

**ãªãœOllamaã‚’é¸ã¶ã®ã‹:**

- **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã®å®¹æ˜“ã•**: `curl -fsSL https://ollama.com/install.sh | sh` ã§å³åº§ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†
- **OpenAIäº’æ›API**: æ—¢å­˜ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚³ãƒ¼ãƒ‰ã‚’ã»ã¼ç„¡ä¿®æ­£ã§ç§»è¡Œå¯èƒ½
- **ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã®çµ±ä¸€**: `ollama pull`ã§ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ãŒå®Œçµ
- **Dockerçµ±åˆ**: GPUå¯¾å¿œã®Docker Composeãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒå…¬å¼ã§æä¾›

> Ollamaã¯ã‚·ãƒ³ã‚°ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚„å°‘äººæ•°ãƒãƒ¼ãƒ ï¼ˆ10åç¨‹åº¦ã¾ã§ï¼‰ã§ã®ã‚ªãƒ³ãƒ—ãƒ¬é‹ç”¨ã«é©ã—ã¦ã„ã¾ã™ã€‚åŒæ™‚æ¥ç¶š100åè¶…ã®å¤§è¦æ¨¡ç’°å¢ƒã§ã¯ã€vLLMã¸ã®ç§»è¡Œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚

é–¢é€£è¨˜äº‹ã¨ã—ã¦ã€Docker Model Runnerã‚’ä½¿ã£ãŸLLMã‚³ãƒ³ãƒ†ãƒŠåŒ–ã«ã¤ã„ã¦ã¯[Docker Model Runnerå®Œå…¨ã‚¬ã‚¤ãƒ‰](https://zenn.dev/0h_n0/articles/9f176e6cc6c104)ã‚‚å‚è€ƒã«ãªã‚Šã¾ã™ã€‚

### Ollama 0.17ã®ä¸»è¦ãªæ”¹å–„ç‚¹

2026å¹´2æœˆã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸOllama 0.17ã¯ã€å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¤§å¹…ãªåˆ·æ–°ã‚’å«ã‚€ãƒ¡ã‚¸ãƒ£ãƒ¼ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã§ã™ã€‚

```mermaid
graph TD
    A[Ollama 0.17 æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£] --> B[æ–°æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³]
    A --> C[ãƒ¡ãƒ¢ãƒªç®¡ç†æ”¹å–„]
    A --> D[ãƒãƒ«ãƒGPUå¼·åŒ–]
    B --> B1[llama.cppç›´æ¥çµ±åˆ]
    B --> B2[ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç† 40%é«˜é€ŸåŒ–]
    B --> B3[ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ 18%é«˜é€ŸåŒ–]
    C --> C1[KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ 8bité‡å­åŒ–]
    C --> C2[GPU VRAMæœ€é©é…åˆ†]
    D --> D1[ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—æ”¹å–„]
    D --> D2[ãƒãƒ«ãƒGPUè‡ªå‹•åˆ†æ•£]
```

ä¸»ãªå¤‰æ›´ç‚¹ã¯ä»¥ä¸‹ã®3ã¤ã§ã™ã€‚

**1. æ–°æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³**: llama.cppã¨ã®çµ±åˆã‚’æ·±åŒ–ã•ã›ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚’Ollamaç‹¬è‡ªã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§åˆ¶å¾¡ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ¡ãƒ¢ãƒªé…åˆ†ãƒ»ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ãŒãã‚ç´°ã‹ãåˆ¶å¾¡ã§ãã¾ã™ã€‚

**2. KVã‚­ãƒ£ãƒƒã‚·ãƒ¥8bité‡å­åŒ–**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®16bitè¡¨ç¾ã«æ¯”ã¹ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’**ç´„åŠåˆ†**ã«å‰Šæ¸›ã§ãã¾ã™ã€‚å‡ºåŠ›å“è³ªã¸ã®å½±éŸ¿ã¯è»½å¾®ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

**3. ãƒãƒ«ãƒGPUãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—**: è¤‡æ•°ã®NVIDIA GPUã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†æ•£é…ç½®ã™ã‚‹åŠ¹ç‡ãŒæ”¹å–„ã•ã‚Œã€70Bä»¥ä¸Šã®å¤§å‹ãƒ¢ãƒ‡ãƒ«ã®é‹ç”¨ãŒç¾å®Ÿçš„ã«ãªã‚Šã¾ã—ãŸã€‚

## Docker Composeã§GPUå¯¾å¿œã®Ollamaç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹

ã“ã“ã‹ã‚‰ã¯å®Ÿéš›ã«Ollamaã‚’Dockerã§ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹æ‰‹é †ã‚’è¦‹ã¦ã„ãã¾ã—ã‚‡ã†ã€‚æœ¬ç•ªé‹ç”¨ã‚’è¦‹æ®ãˆãŸæ§‹æˆã¨ã—ã¦ã€Docker Compose + NVIDIA GPU + ãƒœãƒªãƒ¥ãƒ¼ãƒ æ°¸ç¶šåŒ–ã®æ§‹æˆã‚’æ¡ç”¨ã—ã¾ã™ã€‚

### å‰ææ¡ä»¶ã®ç¢ºèª

ãƒ‡ãƒ—ãƒ­ã‚¤å‰ã«ã€ä»¥ä¸‹ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

```bash
# Docker ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªï¼ˆ24.0ä»¥ä¸Šæ¨å¥¨ï¼‰
docker --version

# Docker Compose ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªï¼ˆv2.20ä»¥ä¸Šæ¨å¥¨ï¼‰
docker compose version

# NVIDIA ãƒ‰ãƒ©ã‚¤ãƒç¢ºèªï¼ˆ535ä»¥ä¸Šæ¨å¥¨ï¼‰
nvidia-smi

# NVIDIA Container Toolkitç¢ºèª
nvidia-ctk --version
```

NVIDIA Container ToolkitãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å ´åˆã¯ã€ä»¥ä¸‹ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚

```bash
# NVIDIA Container Toolkit ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆUbuntu/Debianï¼‰
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
  | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

### Docker Composeæ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«

ä»¥ä¸‹ã®docker-compose.ymlã§Ollamaã‚’GPUå¯¾å¿œã§ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

```yaml
# docker-compose.yml
# Ollama GPUå¯¾å¿œ æœ¬ç•ªæ§‹æˆ
services:
  ollama:
    # æœ¬ç•ªã§ã¯latestã§ã¯ãªãç‰¹å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ãƒ”ãƒ³ç•™ã‚
    image: ollama/ollama:0.17.0
    container_name: ollama-server
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã®æ°¸ç¶šåŒ–ï¼ˆã‚³ãƒ³ãƒ†ãƒŠå†èµ·å‹•ã§ãƒ¢ãƒ‡ãƒ«å†DLã‚’é˜²ãï¼‰
      - ollama_data:/root/.ollama
      # ã‚«ã‚¹ã‚¿ãƒ Modelfileã®ãƒã‚¦ãƒ³ãƒˆ
      - ./modelfiles:/modelfiles:ro
    environment:
      # ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
      - OLLAMA_HOST=0.0.0.0
      # ä¸¦åˆ—ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆGPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
      - OLLAMA_NUM_PARALLEL=4
      # åŒæ™‚ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«æ•°ä¸Šé™
      - OLLAMA_MAX_LOADED_MODELS=2
      # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–ï¼ˆVRAMç¯€ç´„ï¼‰
      - OLLAMA_KV_CACHE_TYPE=q8_0
      # CORSã‚ªãƒªã‚¸ãƒ³åˆ¶é™ï¼ˆç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã¿ï¼‰
      - OLLAMA_ORIGINS=http://192.168.1.0/24,http://10.0.0.0/8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # GPUæ•°ã‚’æŒ‡å®šï¼ˆallã§å…¨GPUï¼‰
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

volumes:
  ollama_data:
    driver: local
```

**ãªãœã“ã®æ§‹æˆã‚’é¸ã‚“ã ã‹:**

- `image: ollama/ollama:0.17.0`: `:latest`ã§ã¯ãªããƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å›ºå®šã—ã€æ„å›³ã—ãªã„ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«ã‚ˆã‚‹äº’æ›æ€§å•é¡Œã‚’é˜²æ­¢
- `OLLAMA_NUM_PARALLEL=4`: RTX 4090ï¼ˆ24GB VRAMï¼‰æƒ³å®šã€‚VRAMä¸è¶³æ™‚ã¯2ã«ä¸‹ã’ã‚‹
- `OLLAMA_KV_CACHE_TYPE=q8_0`: 16bitã«æ¯”ã¹KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªãŒç´„åŠåˆ†ã«å‰Šæ¸›ã•ã‚Œã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ‰±ãˆã‚‹
- `ollama_data`ãƒœãƒªãƒ¥ãƒ¼ãƒ : ã‚³ãƒ³ãƒ†ãƒŠå†ä½œæˆæ™‚ã‚‚ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒä¿æŒã•ã‚Œã‚‹

> **æ³¨æ„**: `OLLAMA_NUM_PARALLEL`ã‚’å¤§ããã—ã™ãã‚‹ã¨ã€GPUãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆOOMï¼‰ã§ãƒ—ãƒ­ã‚»ã‚¹ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ã¾ã™ã€‚RTX 3090ï¼ˆ24GBï¼‰ã§7Bãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†å ´åˆã€4ãŒç›®å®‰ã§ã™ã€‚RTX 4060 Tiï¼ˆ16GBï¼‰ã§ã¯2ç¨‹åº¦ã«æŠ‘ãˆã¦ãã ã•ã„ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨Modelfile

ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•å¾Œã€ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã€èµ·å‹•æ™‚ã«è‡ªå‹•ã§ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã™ã‚‹entrypointã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”¨æ„ã™ã‚‹ã¨é‹ç”¨ãŒæ¥½ã«ãªã‚Šã¾ã™ã€‚

```bash
#!/bin/bash
# scripts/entrypoint.sh
# ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

set -euo pipefail

# Ollamaã‚µãƒ¼ãƒãƒ¼èµ·å‹•ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ï¼‰
ollama serve &
OLLAMA_PID=$!

# ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã‚’å¾…æ©Ÿ
echo "Waiting for Ollama server to start..."
for i in $(seq 1 30); do
    if curl -sf http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "Ollama server is ready."
        break
    fi
    sleep 1
done

# å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
echo "Pulling required models..."
ollama pull llama3.1:8b-instruct-q4_K_M
ollama pull nomic-embed-text

# ã‚«ã‚¹ã‚¿ãƒ Modelfileã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
if [ -f /modelfiles/company-assistant.Modelfile ]; then
    ollama create company-assistant -f /modelfiles/company-assistant.Modelfile
    echo "Custom model 'company-assistant' created."
fi

echo "All models ready. Ollama is serving."
# ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å¾…æ©Ÿ
wait $OLLAMA_PID
```

Modelfileã‚’ä½¿ã†ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚

```dockerfile
# modelfiles/company-assistant.Modelfile
# ç¤¾å†…ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç”¨ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«

FROM llama3.1:8b-instruct-q4_K_M

# æ¸©åº¦ã‚’ä½ã‚ã«è¨­å®šï¼ˆç¤¾å†…æ–‡æ›¸ã®æ­£ç¢ºãªè¦ç´„å‘ã‘ï¼‰
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER num_ctx 8192

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM """
ã‚ãªãŸã¯ç¤¾å†…ã®æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã‹ã¤ç°¡æ½”ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
ä¸æ˜ãªç‚¹ãŒã‚ã‚‹å ´åˆã¯ã€Œæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚å›ç­”ã§ãã¾ã›ã‚“ã€ã¨æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚
ç¤¾å¤–ç§˜æƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¯ååˆ†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
"""
```

### èµ·å‹•ã¨å‹•ä½œç¢ºèª

```bash
# ã‚³ãƒ³ãƒ†ãƒŠèµ·å‹•
docker compose up -d

# ãƒ­ã‚°ç¢ºèª
docker compose logs -f ollama

# ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã®ç¢ºèª
curl -s http://localhost:11434/api/tags | python3 -m json.tool

# æ¨è«–ãƒ†ã‚¹ãƒˆ
curl -s http://localhost:11434/api/generate \
  -d '{
    "model": "llama3.1:8b-instruct-q4_K_M",
    "prompt": "Dockerã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ50æ–‡å­—ä»¥å†…ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    "stream": false
  }' | python3 -m json.tool
```

## OpenAIäº’æ›APIã§æ—¢å­˜ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨çµ±åˆã™ã‚‹

Ollamaã®å¼·ã¿ã®ä¸€ã¤ã¯ã€OpenAIäº’æ›ã®REST APIã‚’æ¨™æº–ã§æä¾›ã—ã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚æ—¢å­˜ã®OpenAI SDKã‚’ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ã‚’ã€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å¤‰æ›´ã ã‘ã§Ollamaã«ç§»è¡Œã§ãã¾ã™ã€‚

### Pythonã§ã®çµ±åˆå®Ÿè£…

```python
# ollama_client.py
"""Ollama OpenAIäº’æ›ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…ä¾‹"""

from openai import OpenAI


def create_ollama_client(
    base_url: str = "http://localhost:11434/v1",
) -> OpenAI:
    """Ollamaç”¨ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

    Args:
        base_url: Ollamaã‚µãƒ¼ãƒãƒ¼ã®OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Returns:
        è¨­å®šæ¸ˆã¿ã®OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    """
    return OpenAI(
        base_url=base_url,
        api_key="ollama",  # Ollamaã§ã¯èªè¨¼ä¸è¦ã ãŒã€SDKã®å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    )


def chat_completion(
    client: OpenAI,
    prompt: str,
    model: str = "llama3.1:8b-instruct-q4_K_M",
    temperature: float = 0.7,
    max_tokens: int = 1024,
) -> str:
    """ãƒãƒ£ãƒƒãƒˆè£œå®Œã‚’å®Ÿè¡Œã™ã‚‹ã€‚

    Args:
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆ0.0-1.0ï¼‰
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    Returns:
        ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "ã‚ãªãŸã¯æŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
            },
            {"role": "user", "content": prompt},
        ],
        temperature=temperature,
        max_tokens=max_tokens,
    )
    return response.choices[0].message.content


def embedding(
    client: OpenAI,
    text: str,
    model: str = "nomic-embed-text",
) -> list[float]:
    """ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—ã™ã‚‹ã€‚

    Args:
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        text: åŸ‹ã‚è¾¼ã¿å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
        model: åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«å

    Returns:
        åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆfloaté…åˆ—ï¼‰
    """
    response = client.embeddings.create(
        model=model,
        input=text,
    )
    return response.data[0].embedding


if __name__ == "__main__":
    client = create_ollama_client()

    # ãƒãƒ£ãƒƒãƒˆè£œå®Œã®å®Ÿè¡Œ
    answer = chat_completion(
        client,
        prompt="Kubernetesã®ãƒãƒƒãƒ‰ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ3è¡Œã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
    )
    print(f"å›ç­”: {answer}")

    # åŸ‹ã‚è¾¼ã¿ã®å–å¾—
    vector = embedding(client, "Docker Composeã«ã‚ˆã‚‹ãƒãƒ«ãƒã‚³ãƒ³ãƒ†ãƒŠç®¡ç†")
    print(f"åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°: {len(vector)}")
```

**ãªãœOpenAIäº’æ›APIã‚’ä½¿ã†ã®ã‹:**

- **ç§»è¡Œã‚³ã‚¹ãƒˆã®æœ€å°åŒ–**: æ—¢å­˜ã®OpenAI SDKãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’`base_url`ã®å¤‰æ›´ã ã‘ã§ç§»è¡Œå¯èƒ½
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äº’æ›**: LangChainã€LlamaIndexã€DSPyãªã©ãŒOpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆ
- **æ®µéšçš„ç§»è¡Œ**: ã‚¯ãƒ©ã‚¦ãƒ‰APIã¨ã‚ªãƒ³ãƒ—ãƒ¬ã‚’ä¸¦è¡Œé‹ç”¨ã—ã€ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹

**æ³¨æ„ç‚¹:**

> OpenAI APIã¨ã®å®Œå…¨ãªäº’æ›æ€§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚Function Callingã¯å¯¾å¿œãƒ¢ãƒ‡ãƒ«ãŒé™ã‚‰ã‚Œã€Structured Outputï¼ˆresponse_formatï¼‰ã¯Ollama 0.17æ™‚ç‚¹ã§ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚µãƒãƒ¼ãƒˆã§ã™ã€‚ç§»è¡Œå‰ã«APIã®äº’æ›æ€§ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®Ÿè£…

ãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŒå¿…é ˆã§ã™ã€‚ä»¥ä¸‹ã¯ã€Server-Sent Eventså½¢å¼ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹ä¾‹ã§ã™ã€‚

```python
# ollama_streaming.py
"""Ollamaã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®Ÿè£…ä¾‹"""

from openai import OpenAI


def stream_chat(
    client: OpenAI,
    prompt: str,
    model: str = "llama3.1:8b-instruct-q4_K_M",
) -> None:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒãƒ£ãƒƒãƒˆå¿œç­”ã‚’è¡¨ç¤ºã™ã‚‹ã€‚

    Args:
        client: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
    """
    stream = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
    )

    for chunk in stream:
        content = chunk.choices[0].delta.content
        if content is not None:
            print(content, end="", flush=True)
    print()  # æ”¹è¡Œ


if __name__ == "__main__":
    client = OpenAI(
        base_url="http://localhost:11434/v1",
        api_key="ollama",
    )
    stream_chat(client, "Pythonã®GILã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ")
```

## VRAMè¦ä»¶ã«åŸºã¥ããƒ¢ãƒ‡ãƒ«é¸å®šã¨æœ€é©åŒ–ã‚’è¡Œã†

GPUãƒ¡ãƒ¢ãƒªï¼ˆVRAMï¼‰ã¯ã‚ªãƒ³ãƒ—ãƒ¬LLMé‹ç”¨ã§æœ€ã‚‚é‡è¦ãªåˆ¶ç´„ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€é‡å­åŒ–ãƒ¬ãƒ™ãƒ«ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®3è¦ç´ ãŒVRAMæ¶ˆè²»ã‚’æ±ºå®šã—ã¾ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºåˆ¥ã®VRAMè¦ä»¶

ä»¥ä¸‹ã¯ã€Q4_K_Mé‡å­åŒ–ã§ã®å„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãŠã‘ã‚‹VRAMè¦ä»¶ã®ç›®å®‰ã§ã™ã€‚

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | Q4_K_M VRAM | æ¨å¥¨GPU | æ—¥æœ¬èªå¯¾å¿œä¾‹ | ãƒˆãƒ¼ã‚¯ãƒ³é€Ÿåº¦ï¼ˆç›®å®‰ï¼‰ |
|-------------|-------------|---------|-------------|-------------------|
| **1.5-3B** | 2-3 GB | RTX 3060 (12GB) | Qwen2.5:3b | 60+ tok/s |
| **7-8B** | 4-5 GB | RTX 4060 Ti (16GB) | Llama3.1:8b, Gemma3:9b | 40+ tok/s |
| **13-14B** | 8-10 GB | RTX 4090 (24GB) | Qwen2.5:14b | 25+ tok/s |
| **32-34B** | 20-22 GB | RTX 4090 (24GB) | Qwen2.5:32b | 12+ tok/s |
| **70B** | 40-48 GB | A100 (80GB) / 2Ã—RTX 4090 | Llama3.3:70b | 5+ tok/s |

ä¸Šè¨˜ã®ãƒˆãƒ¼ã‚¯ãƒ³é€Ÿåº¦ã¯GPUå˜ä½“ã§ã®æ¨è«–æ™‚ã®ç›®å®‰ã§ã™ã€‚å®Ÿéš›ã®é€Ÿåº¦ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã€ç”Ÿæˆé•·ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ã‚ˆã‚Šå¤‰å‹•ã—ã¾ã™ã€‚

**VRAMè¨ˆç®—ã®åŸºæœ¬å¼:**

$$
\text{Total VRAM} = \text{Model Weights} + \text{KV Cache} + \text{Runtime Overhead}
$$

ã“ã“ã§:

- $\text{Model Weights}$ : ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° Ã— é‡å­åŒ–ãƒ“ãƒƒãƒˆæ•° / 8ï¼ˆä¾‹: 7B Ã— 4bit / 8 = 3.5 GBï¼‰
- $\text{KV Cache}$ : ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«æ¯”ä¾‹ï¼ˆ8192ãƒˆãƒ¼ã‚¯ãƒ³ã§ç´„0.5-1.5 GBï¼‰
- $\text{Runtime Overhead}$ : é€šå¸¸0.5-1 GB

### KVã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã®å®Ÿè·µ

Ollama 0.17ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥8bité‡å­åŒ–ã‚’æ´»ç”¨ã™ã‚‹ã¨ã€åŒã˜VRAMã§ã‚ˆã‚Šé•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ãã¾ã™ã€‚

```bash
# KVã‚­ãƒ£ãƒƒã‚·ãƒ¥é‡å­åŒ–ã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ï¼‰
# q8_0: 8bité‡å­åŒ–ï¼ˆå“è³ªã¸ã®å½±éŸ¿ã¯è»½å¾®ï¼‰
export OLLAMA_KV_CACHE_TYPE=q8_0

# ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¼µã—ã¦æ¨è«–ãƒ†ã‚¹ãƒˆ
curl -s http://localhost:11434/api/generate \
  -d '{
    "model": "llama3.1:8b-instruct-q4_K_M",
    "prompt": "é•·ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç´„...",
    "options": {
      "num_ctx": 16384
    },
    "stream": false
  }'
```

| KVã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆ8Bãƒ¢ãƒ‡ãƒ«ã€8Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰ | å“è³ªã¸ã®å½±éŸ¿ |
|-----------------|----------------------------------------|------------|
| f16ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—§ç‰ˆï¼‰ | ç´„1.5 GB | ãªã— |
| q8_0ï¼ˆ0.17æ¨å¥¨ï¼‰ | ç´„0.8 GB | è»½å¾® |
| q4_0 | ç´„0.4 GB | ã‚„ã‚„ä½ä¸‹ |

**ãƒãƒã‚Šãƒã‚¤ãƒ³ãƒˆ:**

æœ€åˆã¯`num_ctx`ã‚’32768ï¼ˆ32Kï¼‰ã«è¨­å®šã—ã¦7Bãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ãã†ã¨ã—ã¾ã—ãŸãŒã€RTX 4060 Tiï¼ˆ16GBï¼‰ã§ã¯OOMãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®VRAMæ¶ˆè²»ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«**ç·šå½¢ã«æ¯”ä¾‹**ã™ã‚‹ãŸã‚ã€VRAMã«ä½™è£•ãŒãªã„å ´åˆã¯`num_ctx`ã‚’8192ã€œ16384ã«æŠ‘ãˆã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«é¸å®šã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```mermaid
flowchart TD
    Start[ç”¨é€”ã‚’æ±ºå®š] --> Q1{æ—¥æœ¬èªãŒå¿…è¦?}
    Q1 -->|Yes| Q2{VRAMå®¹é‡ã¯?}
    Q1 -->|No| Q3{VRAMå®¹é‡ã¯?}
    Q2 -->|8GBä»¥ä¸‹| M1[Qwen2.5:3b / Gemma3:4b]
    Q2 -->|16GB| M2[Llama3.1:8b / Gemma3:9b]
    Q2 -->|24GB| M3[Qwen2.5:14b / Qwen2.5:32b-q4]
    Q2 -->|48GB+| M4[Llama3.3:70b-q4]
    Q3 -->|16GBä»¥ä¸‹| M5[Llama3.1:8b / Mistral:7b]
    Q3 -->|24GB+| M6[Llama3.1:70b / Mixtral:8x7b]
```

## ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’è¨­å®šã™ã‚‹

Ollamaã‚’ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§å…¬é–‹ã™ã‚‹éš›ã€APIã«**èªè¨¼æ©Ÿèƒ½ãŒãªã„**ç‚¹ã¯ç‰¹ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã¨ãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·ã«ã‚ˆã‚‹å¤šå±¤é˜²å¾¡ãŒå¿…é ˆã§ã™ã€‚

### ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ

```mermaid
graph LR
    Client[ç¤¾å†…ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ] --> FW[ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«<br/>192.168.1.0/24ã®ã¿è¨±å¯]
    FW --> Nginx[Nginx ãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·<br/>Basicèªè¨¼ + Rate Limit]
    Nginx --> Ollama[Ollama Server<br/>:11434]
```

### Nginxãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·ã®è¨­å®š

```nginx
# /etc/nginx/conf.d/ollama.conf
# Ollama ãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·è¨­å®šï¼ˆBasicèªè¨¼ + Rate Limitï¼‰

# ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ: 1åˆ†ã‚ãŸã‚Š30ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/IP
limit_req_zone $binary_remote_addr zone=ollama_limit:10m rate=30r/m;

upstream ollama_backend {
    server 127.0.0.1:11434;
    keepalive 32;
}

server {
    listen 8080;
    server_name ollama.internal.example.com;

    # ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã¿è¨±å¯
    allow 192.168.1.0/24;
    allow 10.0.0.0/8;
    deny all;

    # Basicèªè¨¼
    auth_basic "Ollama API";
    auth_basic_user_file /etc/nginx/.htpasswd;

    # ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆé©ç”¨
    limit_req zone=ollama_limit burst=10 nodelay;

    location / {
        proxy_pass http://ollama_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹å¯¾å¿œ
        proxy_buffering off;
        proxy_cache off;
        chunked_transfer_encoding on;

        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆå¤§å‹ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚é•·ã‚ã«è¨­å®šï¼‰
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
    }
}
```

```bash
# Basicèªè¨¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä½œæˆ
sudo apt-get install -y apache2-utils
sudo htpasswd -c /etc/nginx/.htpasswd ollama-user
```

### ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šï¼ˆUFWï¼‰

```bash
# Ollamaãƒãƒ¼ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç¤¾å†…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«åˆ¶é™
sudo ufw allow from 192.168.1.0/24 to any port 8080 proto tcp comment "Ollama proxy"
sudo ufw allow from 10.0.0.0/8 to any port 8080 proto tcp comment "Ollama proxy"

# å¤–éƒ¨ã‹ã‚‰ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã‚’é®æ–­ï¼ˆOllamaã®ãƒãƒ¼ãƒˆã¯å¤–éƒ¨ã«å…¬é–‹ã—ãªã„ï¼‰
sudo ufw deny 11434/tcp comment "Block direct Ollama access"

sudo ufw reload
```

**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã«é–¢ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**

Nginxã‚’ä»‹ã™ã“ã¨ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ•°ãƒŸãƒªç§’å¢—åŠ ã—ã¾ã™ãŒã€èªè¨¼ãƒ»ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒ»ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã‚’ä¸€å…ƒç®¡ç†ã§ãã¾ã™ã€‚ç¤¾å†…ã®é–‹ç™ºç’°å¢ƒã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯ã€`OLLAMA_ORIGINS`ã¨ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã®ã¿ã§é‹ç”¨ã—ã€Nginxã‚’çœç•¥ã™ã‚‹æ§‹æˆã‚‚å¯èƒ½ã§ã™ã€‚ãŸã ã—ã€ãã®å ´åˆã¯APIã¸ã®ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ã‚„éè² è·ã«å¯¾ã™ã‚‹é˜²å¾¡ãŒå¼±ããªã‚Šã¾ã™ã€‚

### ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹è¿½åŠ ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š

```bash
# systemdç’°å¢ƒã®å ´åˆï¼ˆ/etc/systemd/system/ollama.service.d/override.confï¼‰
[Service]
Environment="OLLAMA_HOST=0.0.0.0"
Environment="OLLAMA_ORIGINS=http://192.168.1.0/24,http://10.0.0.0/8"
Environment="OLLAMA_MODELS=/data/ollama/models"
```

`OLLAMA_ORIGINS`ã«ã¯CORSã§è¨±å¯ã™ã‚‹ã‚ªãƒªã‚¸ãƒ³ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŒ‡å®šã—ã¾ã™ã€‚ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å ´åˆã«å¿…è¦ã§ã™ã€‚

## ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

| å•é¡Œ | åŸå›  | è§£æ±ºæ–¹æ³• |
|------|------|----------|
| `Error: model requires more system memory` | GPU VRAMä¸è¶³ | ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆ8Bâ†’3Bï¼‰ã«å¤‰æ›´ã€ã¾ãŸã¯Q4_K_Mé‡å­åŒ–ã‚’ä½¿ç”¨ |
| `connection refused on :11434` | OllamaãŒ`localhost`ã®ã¿ãƒªãƒƒã‚¹ãƒ³ | `OLLAMA_HOST=0.0.0.0`ã‚’è¨­å®š |
| æ¨è«–ãŒæ¥µç«¯ã«é…ã„ï¼ˆ1 tok/sä»¥ä¸‹ï¼‰ | GPUãŒèªè­˜ã•ã‚ŒãšCPUæ¨è«–ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ | `nvidia-smi`ã§ãƒ‰ãƒ©ã‚¤ãƒç¢ºèªã€`docker run --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi`ã§å‹•ä½œãƒ†ã‚¹ãƒˆ |
| `CUDA error: out of memory` | KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ + ãƒ¢ãƒ‡ãƒ«ã§VRAMè¶…é | `num_ctx`ã‚’8192ã«ä¸‹ã’ã‚‹ã€`OLLAMA_KV_CACHE_TYPE=q8_0`ã§åœ§ç¸® |
| Dockerå†…ã§GPUãŒè¦‹ãˆãªã„ | NVIDIA Container Toolkitæœªè¨­å®š | `nvidia-ctk runtime configure --runtime=docker`ã‚’å®Ÿè¡Œå¾Œã€Dockerå†èµ·å‹• |
| ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé€”ä¸­ã§æ­¢ã¾ã‚‹ | ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | `OLLAMA_KEEP_ALIVE=60m`ã‚’è¨­å®šã€ãƒ—ãƒ­ã‚­ã‚·ç’°å¢ƒã§ã¯HTTPS_PROXYã‚’æŒ‡å®š |

## ã¾ã¨ã‚ã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**ã¾ã¨ã‚:**

- Ollama 0.17ã¯å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ·æ–°ã«ã‚ˆã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå‡¦ç†ãŒæœ€å¤§40%ã€ãƒˆãƒ¼ã‚¯ãƒ³ç”ŸæˆãŒ18%é«˜é€ŸåŒ–ã•ã‚ŒãŸ
- Docker Compose + NVIDIA GPUã§ã€æœ¬ç•ªé‹ç”¨å¯èƒ½ãªã‚ªãƒ³ãƒ—ãƒ¬LLMæ¨è«–ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã‚‹
- OpenAIäº’æ›APIã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’æœ€å°é™ã®å¤‰æ›´ã§ç§»è¡Œå¯èƒ½
- KVã‚­ãƒ£ãƒƒã‚·ãƒ¥8bité‡å­åŒ–ã§ã€åŒä¸€GPUã§ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ç´„2å€ã«æ‹¡å¼µã§ãã‚‹
- APIã«èªè¨¼æ©Ÿèƒ½ãŒãªã„ãŸã‚ã€ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ« + Nginxãƒªãƒãƒ¼ã‚¹ãƒ—ãƒ­ã‚­ã‚·ã®å¤šå±¤é˜²å¾¡ãŒå¿…é ˆ

**æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨:**

- `docker compose up -d`ã§Ollamaç’°å¢ƒã‚’èµ·å‹•ã—ã€7Bã€œ8Bãƒ¢ãƒ‡ãƒ«ã§å‹•ä½œç¢ºèªã™ã‚‹
- è‡ªç¤¾ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åˆã£ãŸModelfileã‚’ä½œæˆã—ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹
- åŒæ™‚æ¥ç¶šæ•°ãŒå¢—ãˆãŸéš›ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã€vLLMã¸ã®ç§»è¡Œåˆ¤æ–­åŸºæº–ã‚’å®šã‚ã‚‹

## å‚è€ƒ

- [Ollamaå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ - Docker](https://docs.ollama.com/docker)
- [Ollama 0.17ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆï¼ˆWebProNewsï¼‰](https://www.webpronews.com/ollama-0-17-arrives-with-massive-performance-gains-and-a-new-architecture-that-could-reshape-local-ai-deployment/)
- [Ollama vs. vLLM: A deep dive into performance benchmarkingï¼ˆRed Hat Developerï¼‰](https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking)
- [Ollama VRAM Requirements: Complete 2026 Guideï¼ˆLocalLLM.inï¼‰](https://localllm.in/blog/ollama-vram-requirements-for-local-llms)
- [Self-Hosted LLM Guide: Setup, Tools & Cost Comparison 2026ï¼ˆPrem AIï¼‰](https://blog.premai.io/self-hosted-llm-guide-setup-tools-cost-comparison-2026/)
- [Dify Ã— Ollamaã§æ§‹ç¯‰ã™ã‚‹ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ç”ŸæˆAIåŸºç›¤ï¼ˆLifematicsï¼‰](https://blog.lifematics.co.jp/entry/2026/01/30/090000)
- [Ollama Setup Guide: Run Local LLMs Like a Pro 2026 Editionï¼ˆNerd Level Techï¼‰](https://nerdleveltech.com/ollama-setup-guide-run-local-llms-like-a-pro-2026-edition)

---

:::message
ã“ã®è¨˜äº‹ã¯AIï¼ˆClaude Codeï¼‰ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã®æ­£ç¢ºæ€§ã«ã¤ã„ã¦ã¯è¤‡æ•°ã®æƒ…å ±æºã§æ¤œè¨¼ã—ã¦ã„ã¾ã™ãŒã€å®Ÿéš›ã®åˆ©ç”¨æ™‚ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã”ç¢ºèªãã ã•ã„ã€‚
:::
