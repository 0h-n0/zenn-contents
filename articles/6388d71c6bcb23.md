---
title: "MTEB×JMTEBで選ぶEmbeddingモデル：精度評価の実践ガイド"
emoji: "📐"
type: "tech"
topics: ["embedding", "rag", "nlp", "python", "machinelearning"]
published: false
---

# MTEB×JMTEBで選ぶEmbeddingモデル：精度評価の実践ガイド

## この記事でわかること

- MTEB/JMTEBベンチマークの8タスク構成と、各タスクが測定する能力の違い
- Precision@K・Recall@K・NDCG@Kなど主要評価指標の使い分け方
- 2026年時点の主要Embeddingモデル（Voyage 4・Qwen3-Embedding・Sarashina-Embedding）の性能比較
- Pythonで自社データに対するEmbedding評価パイプラインを構築する方法
- 日本語タスクでのモデル選定における注意点とJMTEBの活用法

## 対象読者

- **想定読者**: 中級者〜上級者のMLエンジニア・データサイエンティスト
- **必要な前提知識**:
  - Python 3.11+の基本的な使い方
  - ベクトル検索・コサイン類似度の基本概念
  - RAG（Retrieval-Augmented Generation）の基礎理解

## 結論・成果

**Embeddingモデルの選定は、単一のベンチマークスコアではなくタスク別・ドメイン別の評価が不可欠です。** MTEB v2.8（2026年2月時点）のリーダーボードによると、Voyage 4-largeがRTEBベンチマーク29データセットでOpenAI text-embedding-3-largeを14.05%上回る一方、日本語タスクではSarashina-Embedding-v1-1BがJMTEBで75.50を記録しOpenAI text-embedding-3-large（74.05）を上回っています。万能モデルは存在しないため、自社データでの実測が必須です。

本記事では、ベンチマークの読み方から自社データでの評価パイプライン構築まで、Embeddingモデル選定に必要な知識を体系的に解説します。

## Embeddingモデル評価の全体像を把握する

Embeddingモデルとは、テキストを固定長の数値ベクトル（埋め込みベクトル）に変換するモデルです。RAGの検索精度、テキスト分類、クラスタリングなど、多くのNLPタスクの土台となります。

しかし、「どのモデルが良いか」は一概に言えません。**検索タスクで強いモデルが、分類タスクでも強いとは限りません。** MTEB論文（Muennighoff et al., 2022）の分析でも、STS（Semantic Textual Similarity）で高スコアのモデルが検索やクラスタリングでは下位に沈むケースが複数報告されています。

### MTEBの8タスクが測定する能力

MTEB（Massive Text Embedding Benchmark）は、Embeddingモデルを8つのタスクカテゴリで総合評価するベンチマークです。2026年2月時点のv2.8では58データセットをカバーしています。

| タスク | 測定する能力 | 評価指標 | ユースケース例 |
|--------|-------------|---------|--------------|
| **Retrieval** | クエリに関連する文書の検索能力 | NDCG@10 | RAG、社内文書検索 |
| **Classification** | テキストの分類能力 | Accuracy | スパム検出、感情分析 |
| **Clustering** | 意味的に類似したテキストのグループ化 | V-measure | トピック分析、文書整理 |
| **STS** | 2文間の意味的類似度の予測 | Spearman相関 | 重複検出、類似質問マッチング |
| **Reranking** | 候補文書の関連度順ソート | MAP | 検索結果のリランキング |
| **Pair Classification** | 2文の関係性の分類（含意・矛盾等） | AP | NLI、ファクトチェック |
| **Bitext Mining** | 対訳ペアの発見 | F1 | 翻訳メモリ構築 |
| **Summarization** | 要約の品質評価 | Spearman相関 | 要約生成の評価 |

**注意点**: MTEBの総合スコアは各タスクの平均値ですが、この「平均」に騙されてはいけません。RAGの検索精度を重視するなら**Retrievalタスクのスコアのみ**を見るべきですし、テキスト分類を重視するならClassificationスコアが判断基準になります。

> **よくある間違い**: 「MTEBスコアが高いモデルを選べば間違いない」と考えがちですが、総合スコアはあくまで8タスクの平均です。自分のユースケースに関連するタスクのスコアを個別に確認しましょう。

### MMTEB: 多言語対応の拡張ベンチマーク

2025年2月に発表されたMMTEB（Massive Multilingual Text Embedding Benchmark、arxiv:2502.13595）は、MTEBを250以上の言語・500以上のタスクに拡張したベンチマークです。ICLR 2025で発表され、多言語Embeddingモデルの評価標準になりつつあります。

MMTEBの主な特徴は以下のとおりです。

- **10タスクカテゴリ**: MTEBの8タスクに加え、命令追従（instruction following）・長文書検索（long-document retrieval）を追加
- **ドメイン多様性**: フィクション、ソーシャルメディア、医療テキスト、プログラミングドキュメントなど幅広い分野をカバー
- **多言語対応**: 英語以外の言語でのモデル性能を公平に比較可能

MMTEBの分析結果として、数十億パラメータのLLMベースモデルが特定言語・タスクでSOTA（State of the Art）を達成する一方、**全言語・全タスクで一様に強いモデルは存在しない**ことが確認されています。

## 評価指標の仕組みと使い分けを理解する

Embeddingモデルの性能を正しく評価するには、各指標が何を測定しているかを理解する必要があります。ここでは、Retrievalタスクで使われる主要4指標を解説します。

### Precision@K と Recall@K：基本の2指標

**Precision@K**は「上位K件のうち、何件が正解か」を測定します。

```
Precision@K = (上位K件中の正解数) / K
```

例えば、上位10件のうち7件が関連文書なら `Precision@10 = 0.7` です。

**Recall@K**は「全正解のうち、上位K件に何件含まれているか」を測定します。

```
Recall@K = (上位K件中の正解数) / (全正解数)
```

全体で20件の関連文書があり、上位10件に8件含まれていれば `Recall@10 = 0.4` です。

この2指標の使い分けは次のとおりです。

- **Precision@Kを重視する場面**: ユーザーに表示する件数が限られている場合（検索結果Top 5表示など）
- **Recall@Kを重視する場面**: 関連情報の取りこぼしを避けたい場合（法律文書検索、医療情報検索など）

**制約条件**: Precision@KとRecall@Kはどちらも**順序を考慮しない**指標です。上位K件に正解が含まれていれば、1位でも10位でも同じスコアになります。順序を考慮したい場合はNDCG@KやMRRを使います。

### NDCG@K：順序を考慮した総合評価

**NDCG@K（Normalized Discounted Cumulative Gain at K）** は、MTEBのRetrievalタスクで**標準の評価指標**として採用されています。

NDCGの特徴は、**関連度に段階がある**ことを評価できる点です。Precision/Recallが「関連/非関連」の二値判定なのに対し、NDCGは「非常に関連=3、やや関連=2、わずかに関連=1、無関連=0」のような段階的な関連度を扱えます。

```python
# ndcg_at_k.py
# NDCG@Kの計算例（Python実装）
import numpy as np


def dcg_at_k(relevances: list[float], k: int) -> float:
    """Discounted Cumulative Gain を計算する。

    Args:
        relevances: 検索結果の関連度スコアリスト（上位から順に）
        k: 評価する上位件数
    """
    relevances = np.array(relevances[:k])
    # 位置 i の割引係数は 1 / log2(i + 2)（0始まりインデックス）
    discounts = np.log2(np.arange(len(relevances)) + 2)
    return float(np.sum(relevances / discounts))


def ndcg_at_k(relevances: list[float], k: int) -> float:
    """Normalized DCG を計算する。

    理想的な並び順（降順ソート）でのDCGで正規化する。
    """
    actual_dcg = dcg_at_k(relevances, k)
    # 理想的な並び順 = 関連度の降順
    ideal_dcg = dcg_at_k(sorted(relevances, reverse=True), k)
    if ideal_dcg == 0:
        return 0.0
    return actual_dcg / ideal_dcg


# 使用例: 検索結果の関連度 [3, 2, 0, 1, 0] で上位5件を評価
relevances = [3, 2, 0, 1, 0]
score = ndcg_at_k(relevances, k=5)
print(f"NDCG@5: {score:.4f}")  # NDCG@5: 0.9203
```

**なぜNDCG@10がMTEBの標準なのか:**
- 検索タスクでは「1位の結果が正解」と「10位の結果が正解」では、ユーザー体験が大きく異なる
- NDCGは上位に正解がある場合により高いスコアを付与するため、実際のユーザー体験に近い評価になる
- 段階的な関連度を扱えるため、「完全一致」以外の部分的な正解も評価可能

### MRR：最初の正解の位置を重視する指標

**MRR（Mean Reciprocal Rank）** は「最初の正解が何位に出てくるか」を評価します。

```
MRR = (1/クエリ数) × Σ (1 / 最初の正解の順位)
```

1位に正解があれば1.0、2位なら0.5、3位なら0.333...です。「1つの正解を素早く見つけたい」FAQ検索やQ&Aシステムでの評価に適しています。

| 指標 | 順序考慮 | 関連度段階 | 推奨ユースケース |
|------|---------|-----------|----------------|
| Precision@K | なし | 二値 | 表示件数が固定の検索UI |
| Recall@K | なし | 二値 | 取りこぼし防止が重要な検索 |
| NDCG@K | あり | 段階的 | 汎用的な検索品質評価（MTEB標準） |
| MRR | あり | 二値 | FAQ・Q&Aの単一回答検索 |
| MAP@K | あり | 二値 | リランキング品質評価 |

## 2026年の主要Embeddingモデルを比較する

2026年2月時点の主要モデルを、MTEBリーダーボードの公開スコアとAPIモデルの公表値に基づいて比較します。

### グローバルモデル比較

| モデル | パラメータ数 | MTEB（英語） | 最大トークン | 次元数 | 特徴 |
|--------|------------|------------|-----------|--------|------|
| **Voyage 4-large** | 非公開 | RTEB 29データセットでトップ | 32K | 2048/1024/512/256 | MoEアーキテクチャ、共有埋め込み空間 |
| **Qwen3-Embedding-8B** | 8B | 70.58（多言語1位） | 8K | 可変 | Apache 2.0、100+言語対応 |
| **Cohere Embed v4** | 非公開 | 65.2 | 128K | 1536 | テキスト+画像入力、100+言語 |
| **text-embedding-3-large** | 非公開 | 64.6 | 8K | 3072/1536/256 | Matryoshka対応、広いエコシステム |
| **NV-Embed** | 非公開 | 69.32 | - | - | NVIDIA GPU最適化 |
| **bge-m3** | - | 上位 | 8K | 1024 | MIT、密/疎/多ベクトル対応 |

**Voyage 4の技術的革新**: 2026年1月にリリースされたVoyage 4ファミリーは、Embeddingモデルとして業界初のMoE（Mixture of Experts）アーキテクチャを採用しています。Voyage AI社のブログによると、dense modelと比較してサービング（推論）コストを40%削減しながらSOTA精度を達成したと報告されています。

さらに、**共有埋め込み空間（Shared Embedding Space）** もVoyage 4の注目点です。voyage-4-large、voyage-4、voyage-4-lite、voyage-4-nanoの4モデルが互換性のあるベクトルを出力するため、ドキュメント側はvoyage-4-largeで高精度に埋め込み、クエリ側はvoyage-4-liteで低コストに処理する「非対称検索（Asymmetric Retrieval）」が可能です。

> **トレードオフ**: Voyage 4-largeの精度は高いものの、APIサービスへの依存が生まれます。オンプレミスや自社GPU環境での運用が必須の場合は、Apache 2.0ライセンスのQwen3-Embedding-8BやMITライセンスのbge-m3が選択肢になります。

### 日本語モデル比較（JMTEB）

日本語Embeddingの評価には**JMTEB（Japanese Massive Text Embedding Benchmark）** を使います。SB Intuitions社が2024年に構築した日本語特化ベンチマークで、6タスク・16データセットで構成されています。

| モデル | JMTEB平均 | Retrieval | Classification | STS | Reranking | 特徴 |
|--------|----------|-----------|---------------|-----|-----------|------|
| **Sarashina-Embedding-v1-1B** | 75.50 | 77.61 | 78.37 | - | 93.74 | 日本語LLMベース、1Bパラメータ |
| **text-embedding-3-large** | 74.05 | - | - | - | - | API利用、多言語対応 |
| **ruri-v3-310m** | - | 81.89 | 78.66 | - | - | 日本語特化、軽量 |
| **PLaMo-Embedding-1B** | - | - | - | 83.14 | - | STS特化で高スコア |
| **Qwen3-Embedding-0.6B** | 低め | 72.81 | 66.09 | - | 93.10 | 多言語モデル、日本語は不得意 |

**日本語での注目点**: JMTEB評価において、Qwen3-Embedding-0.6Bは多言語MTEBリーダーボードでトップクラスにもかかわらず、日本語タスクではruri-v3やSarashina-Embeddingに大きく劣っています。secon.dev のJMTEB実測レポートでも、「日本語のタスクがあまり学習されていないからなのか、日本語の結果は振るわない」と報告されています。

**教訓**: 多言語ベンチマークのスコアが高くても、特定言語では専用モデルに劣ることがあります。日本語を主対象とするシステムでは、JMTEBスコアまたは自社日本語データでの実測を必ず実施してください。

## MTEBを使ったEmbedding評価パイプラインを構築する

ベンチマークスコアは参考値に過ぎません。本番環境に近い条件での実測が、モデル選定の最終判断材料になります。ここではPythonで評価パイプラインを構築する方法を解説します。

### MTEBライブラリでの標準ベンチマーク実行

MTEB v2.8（2026年2月時点）を使って、任意のモデルを標準ベンチマークで評価できます。

```python
# run_mteb_benchmark.py
# MTEB標準ベンチマークの実行例
# 動作確認: Python 3.11, mteb 2.8.4, sentence-transformers 3.4

import mteb

# モデルの読み込み（Sentence Transformers互換モデル）
model = mteb.get_model("sentence-transformers/all-MiniLM-L6-v2")

# タスクの選択（Retrieval系のみ評価する例）
tasks = mteb.get_tasks(
    tasks=["NFCorpus", "SciFact", "FiQA2018"]  # 検索タスクを3つ選択
)

# 評価実行（結果はディスクにキャッシュされる）
results = mteb.evaluate(
    model,
    tasks=tasks,
    output_folder="results/all-MiniLM-L6-v2",
)

# 結果の確認
for task_result in results:
    print(f"Task: {task_result.task_name}")
    print(f"  NDCG@10: {task_result.get_score():.4f}")
```

CLIからも実行可能です。

```bash
# CLI実行例（バッチサイズ指定）
mteb run \
    -m sentence-transformers/all-MiniLM-L6-v2 \
    -t "NFCorpus" "SciFact" \
    --output-folder results/benchmark \
    --batch-size 64
```

**なぜこの方法を選んだか:**
- `mteb.get_model()` はSentence Transformers、OpenAI、Cohereなど主要なモデルラッパーを統一APIで扱える
- 結果は自動的にJSON形式でキャッシュされ、再実行時にスキップされる
- HuggingFace Leaderboardへの結果提出も同じフォーマットで可能

### 自社データでのカスタム評価

標準ベンチマークだけでは不十分です。自社のドメインデータでの評価が必要な場合は、`pytrec_eval` と `sentence-transformers` を組み合わせて独自の評価パイプラインを構築します。

```python
# custom_eval_pipeline.py
# 自社データでのEmbedding評価パイプライン
# 動作確認: Python 3.11, sentence-transformers 3.4, pytrec-eval-terrier 0.5.6

from __future__ import annotations

import json
from dataclasses import dataclass

import numpy as np
import pytrec_eval
from sentence_transformers import SentenceTransformer


@dataclass(frozen=True)
class EvalDataset:
    """評価データセットの構造体。

    Attributes:
        queries: クエリID→テキストのマッピング
        corpus: ドキュメントID→テキストのマッピング
        relevance: クエリID→{ドキュメントID: 関連度} のマッピング
    """

    queries: dict[str, str]
    corpus: dict[str, str]
    relevance: dict[str, dict[str, int]]


def load_eval_dataset(path: str) -> EvalDataset:
    """JSON形式の評価データを読み込む。

    期待するJSON構造:
    {
        "queries": {"q1": "検索クエリ1", ...},
        "corpus": {"d1": "ドキュメント1", ...},
        "relevance": {"q1": {"d1": 2, "d3": 1}, ...}
    }
    """
    with open(path) as f:
        data = json.load(f)
    return EvalDataset(**data)


def evaluate_model(
    model_name: str,
    dataset: EvalDataset,
    top_k: int = 10,
) -> dict[str, float]:
    """指定モデルで検索評価を実行する。

    Returns:
        {"ndcg_cut_10": 0.85, "recall_10": 0.72, ...} 形式の評価結果
    """
    model = SentenceTransformer(model_name)

    # クエリとドキュメントをエンコード
    query_ids = list(dataset.queries.keys())
    doc_ids = list(dataset.corpus.keys())

    query_embeddings = model.encode(
        [dataset.queries[qid] for qid in query_ids],
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True,  # コサイン類似度用に正規化
    )
    doc_embeddings = model.encode(
        [dataset.corpus[did] for did in doc_ids],
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True,
    )

    # コサイン類似度で検索（正規化済みなので内積=コサイン類似度）
    similarity_matrix = query_embeddings @ doc_embeddings.T

    # pytrec_eval形式に変換
    run: dict[str, dict[str, float]] = {}
    for i, qid in enumerate(query_ids):
        scores = similarity_matrix[i]
        # 上位K件を取得
        top_indices = np.argsort(scores)[::-1][:top_k]
        run[qid] = {
            doc_ids[idx]: float(scores[idx]) for idx in top_indices
        }

    # pytrec_evalで評価
    evaluator = pytrec_eval.RelevanceEvaluator(
        dataset.relevance,
        {"ndcg_cut_10", "recall_10", "map_cut_10", "recip_rank"},
    )
    results = evaluator.evaluate(run)

    # クエリ平均を計算
    metrics = {}
    for metric in ["ndcg_cut_10", "recall_10", "map_cut_10", "recip_rank"]:
        values = [r[metric] for r in results.values()]
        metrics[metric] = float(np.mean(values))

    return metrics


# 使用例
if __name__ == "__main__":
    dataset = load_eval_dataset("eval_data.json")

    models_to_compare = [
        "sentence-transformers/all-MiniLM-L6-v2",
        "intfloat/multilingual-e5-large",
        # OpenAIモデルはAPIラッパー経由で利用
    ]

    for model_name in models_to_compare:
        print(f"\n=== {model_name} ===")
        metrics = evaluate_model(model_name, dataset)
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
```

**評価データセットの作成ポイント:**

| 項目 | 推奨 | 理由 |
|------|------|------|
| クエリ数 | 100〜500件 | 統計的に有意な差を検出するため |
| 正解ラベル | 段階的関連度（0-3） | NDCGを正しく計算するため |
| ドメイン | 本番データに近いもの | ベンチマーク≠本番性能のギャップを埋める |
| ラベル付け | 複数人で実施 | ラベルの一貫性を担保するため |

> **ハマりポイント**: 正解ラベルを二値（0/1）で作成すると、NDCGとPrecision/Recallの結果が一致してしまい、NDCGの優位性が活きません。可能であれば、3段階以上の関連度でラベル付けしましょう。

### APIモデルの評価方法

OpenAIやCohereなどのAPIモデルを評価する場合は、カスタムラッパーを作成します。

```python
# api_model_wrapper.py
# APIモデルをMTEB互換で評価するラッパー
# 動作確認: Python 3.11, openai 1.60

from __future__ import annotations

import numpy as np
from openai import OpenAI


class OpenAIEmbeddingModel:
    """OpenAI Embedding APIのラッパー。

    MTEBのget_model()と互換性のあるencode()メソッドを提供する。
    """

    def __init__(
        self,
        model_name: str = "text-embedding-3-large",
        dimensions: int = 1536,
    ) -> None:
        self.client = OpenAI()  # OPENAI_API_KEY環境変数を使用
        self.model_name = model_name
        self.dimensions = dimensions

    def encode(
        self,
        sentences: list[str],
        batch_size: int = 100,
        **kwargs,
    ) -> np.ndarray:
        """テキストリストをエンコードしてnumpy配列で返す。"""
        all_embeddings: list[list[float]] = []

        for i in range(0, len(sentences), batch_size):
            batch = sentences[i : i + batch_size]
            response = self.client.embeddings.create(
                model=self.model_name,
                input=batch,
                dimensions=self.dimensions,
            )
            batch_embeddings = [item.embedding for item in response.data]
            all_embeddings.extend(batch_embeddings)

        return np.array(all_embeddings)


# 使用例: 自社データでOpenAIモデルを評価
# model = OpenAIEmbeddingModel(dimensions=1536)
# metrics = evaluate_model_with_wrapper(model, dataset)
```

**注意点:**
> APIモデルの評価は、レート制限とコストに注意が必要です。1,000クエリ × 10,000ドキュメントのエンコードで、text-embedding-3-largeの場合は約$0.50-$2.00程度のコストが発生します（2026年2月時点の公式価格に基づく推定）。小規模な評価データセットで先にフィルタリングし、最終候補2-3モデルに絞ってから大規模評価を行う戦略が現実的です。

## 日本語Embeddingの評価をJMTEBで実施する

日本語を主対象とするシステムでは、英語ベンチマークの結果だけでは不十分です。JMTEB（Japanese Massive Text Embedding Benchmark）を使って日本語での性能を確認しましょう。

### JMTEBの構成と特徴

JMTEBはSB Intuitions社が2024年に構築した日本語特化ベンチマークです。以下の6タスク・16データセットで構成されています。

| タスク | データセット例 | 評価指標 |
|--------|-------------|---------|
| Retrieval | Mr. TyDi (Japanese)、MIRACL | NDCG@10 |
| Classification | Amazon Review、livedoor news | Accuracy |
| STS | JSTS、JSICK | Spearman相関 |
| Reranking | esci (Japanese) | MAP |
| Clustering | livedoor news clustering | V-measure |
| Pair Classification | PAWS-X (Japanese) | AP |

### JMTEBでの評価実行

MTEBライブラリから直接JMTEBタスクを実行できます。

```python
# run_jmteb.py
# JMTEBでの日本語Embedding評価
# 動作確認: Python 3.11, mteb 2.8.4

import mteb

# 日本語対応モデルの読み込み
model = mteb.get_model("intfloat/multilingual-e5-large")

# JMTEBタスクを指定して評価
# ※タスク名はMTEBに登録されている正式名を使用
tasks = mteb.get_tasks(
    languages=["jpn"],  # 日本語タスクのみフィルタ
    tasks=["MrTyDiRetrieval"],  # 日本語検索タスク
)

results = mteb.evaluate(
    model,
    tasks=tasks,
    output_folder="results/jmteb",
)

for task_result in results:
    print(f"Task: {task_result.task_name}")
    print(f"  Score: {task_result.get_score():.4f}")
```

### モデル選定のディシジョンツリー

日本語Embeddingモデルの選定において、以下の判断フローが実用的です。

1. **APIサービスで良いか？**
   - はい → text-embedding-3-large（安定性・エコシステム重視）またはVoyage 4（精度重視）
   - いいえ → 次へ

2. **日本語が主対象か？**
   - はい → Sarashina-Embedding-v1-1B（JMTEB最高スコア）またはruri-v3（軽量優先）
   - いいえ（多言語） → Qwen3-Embedding-8B（MTEB多言語1位）

3. **推論環境の制約は？**
   - GPU潤沢 → 8Bモデルも選択肢
   - GPU制限あり → bge-m3（MIT、軽量）またはruri-v3-130m

**最初はruri-v3-310mとtext-embedding-3-largeの2モデルで始め、自社データで比較してから最終判断することをおすすめします。** この2モデルは日本語性能とコスト・運用性のバランスが良く、ベースラインとして適しています。

## よくある問題と解決方法

| 問題 | 原因 | 解決方法 |
|------|------|----------|
| ベンチマーク高スコアだが本番で精度が出ない | ベンチマークのドメインと自社データの乖離 | 自社ドメインデータで追加評価する |
| 日本語検索の精度が低い | 多言語モデルの日本語学習不足 | JMTEB上位の日本語特化モデルに切り替える |
| エンコード速度が遅い | モデルサイズが大き過ぎる | Matryoshka Representation Learningで次元削減、またはlite/nanoモデルに変更 |
| NDCG@10は高いがユーザー満足度が低い | 検索タスク以外の品質問題（生成品質等） | RAG全体のパイプラインで評価する（検索だけでなく回答品質も） |
| 異なるモデルのスコアが比較できない | 評価データセットや指標が統一されていない | 同一データセット・同一指標で再評価する |

## まとめと次のステップ

**まとめ:**

- **MTEBは8タスクの総合評価**だが、ユースケースに応じてタスク別スコアを確認すべき
- **NDCG@10**がRetrievalタスクの標準指標。段階的関連度を扱え、順序も考慮できる
- **2026年のトップモデル**はVoyage 4-large（MoE＋共有埋め込み空間）、日本語ではSarashina-Embedding-v1-1BがOpenAIを上回る
- **万能モデルは存在しない**。多言語ベンチマーク上位でも日本語では専用モデルに劣ることがある
- **自社データでの実測が最終判断材料**。ベンチマークはフィルタリング、本番データは意思決定に使う

**次にやるべきこと:**

1. 自社ドメインの評価データセット（100クエリ以上、段階的関連度ラベル付き）を作成する
2. 候補モデル2-3個をMTEBライブラリ＋自社データで評価し、NDCG@10とRecall@10を比較する
3. コスト・レイテンシ・ライセンスも含めた総合判断でモデルを決定する

## 関連する深掘り記事

この記事で取り上げたトピックについて、1次情報（論文・企業ブログ）を詳細に解説した記事を公開しています。

- [論文解説: MTEB — 8タスク・56データセットによるテキスト埋め込みベンチマーク (arXiv 2210.07316)](https://0h-n0.github.io/posts/paper-2210-07316/)
- [論文解説: MMTEB — 250+言語・500+データセットの多言語ベンチマーク (arXiv 2502.13595)](https://0h-n0.github.io/posts/paper-2502-13595/)
- [Voyage AI解説: Voyage 4 — MoEアーキテクチャと共有埋め込み空間](https://0h-n0.github.io/posts/techblog-voyage-4-moe-shared-embedding/)
- [論文解説: Ruri — 日本語汎用テキスト埋め込みモデル (arXiv 2409.07737)](https://0h-n0.github.io/posts/paper-2409-07737/)
- [論文解説: Matryoshka Representation Learning — 可変次元埋め込み (arXiv 2212.09741)](https://0h-n0.github.io/posts/paper-2212-09741/)

## 参考

- [MTEB: Massive Text Embedding Benchmark（GitHub）](https://github.com/embeddings-benchmark/mteb) - MTEBライブラリ公式リポジトリ（v2.8.4、Apache 2.0）
- [MMTEB: Massive Multilingual Text Embedding Benchmark（arxiv:2502.13595）](https://arxiv.org/abs/2502.13595) - 250+言語対応の多言語ベンチマーク論文（ICLR 2025）
- [JMTEBの構築 - SB Intuitions TECH BLOG](https://www.sbintuitions.co.jp/blog/entry/2024/05/16/130848) - 日本語テキスト埋め込みベンチマークJMTEBの技術解説
- [Sarashina-Embedding-v1-1B - SB Intuitions TECH BLOG](https://www.sbintuitions.co.jp/blog/entry/2025/02/03/180848) - JMTEB最高スコアの日本語Embeddingモデル技術詳細
- [Voyage 4 Model Family - Voyage AI Blog](https://blog.voyageai.com/2026/01/15/voyage-4/) - MoEアーキテクチャ採用のEmbeddingモデル発表
- [Retrieval Evaluation Metrics - Weaviate](https://weaviate.io/blog/retrieval-evaluation-metrics) - 検索評価指標の解説ガイド
- [Top Embedding Models on MTEB Leaderboard - Modal](https://modal.com/blog/mteb-leaderboard-article) - MTEBリーダーボード上位モデルの分析
- [Qwen3-Embedding JMTEB評価 - secon.dev](https://secon.dev/entry/2025/06/11/100000-qwen3-embedding-jmteb/) - Qwen3-Embeddingの日本語性能実測

---

:::message
この記事はAI（Claude Code）により自動生成されました。内容の正確性については複数の情報源で検証していますが、実際の利用時は公式ドキュメントもご確認ください。
:::
